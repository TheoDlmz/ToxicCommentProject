{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cufyRV3uSoVi",
        "Ctay4c5Rv0dl",
        "dTVCfeomv6Dv",
        "Dexgbb0evEqP",
        "SPgyv5a3v-fI",
        "jsxSmb53wC_A",
        "s0gK7u2XxDLM",
        "E9KPM-qi6OuD",
        "Mwngiqd8kocK",
        "hQJv4-62_Ixn",
        "wY7yN7--eao6",
        "zbwyYlS7JEAq",
        "H6PELxXfoKJv",
        "A9-1sadLxALq"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30fec703e61144859f654749ddcc4c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b15e5b0f733b4d35b2facfcf6b619156",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6962ac9a36db4d9ea00075da1a125238",
              "IPY_MODEL_e786eea093be49e591cdbb1f54eb4917"
            ]
          }
        },
        "b15e5b0f733b4d35b2facfcf6b619156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6962ac9a36db4d9ea00075da1a125238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2598972b96a44e83a941fc5f153dabb7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e785c3059054815ba5764c70061a50a"
          }
        },
        "e786eea093be49e591cdbb1f54eb4917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c80a1a59f87462a9788b2162e600cca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 2.09kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aaa8682b67824850bb875b545aaa8a47"
          }
        },
        "2598972b96a44e83a941fc5f153dabb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e785c3059054815ba5764c70061a50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c80a1a59f87462a9788b2162e600cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aaa8682b67824850bb875b545aaa8a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0db7815de22d4dff8c26e4bfc3794b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6b7ca43e8bf741e7aa9f5e6a5b08af44",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_484ef53eb0d0475499b3b2f998790906",
              "IPY_MODEL_794280c984124fc3a1db9e3722291328"
            ]
          }
        },
        "6b7ca43e8bf741e7aa9f5e6a5b08af44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "484ef53eb0d0475499b3b2f998790906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a2aa728c29ab4f9faa6e1da97f39186d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3133a8b4e7cc42c89ee5609f0c7d4ea0"
          }
        },
        "794280c984124fc3a1db9e3722291328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32777a2f78274669993200a77b040741",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:26&lt;00:00, 19.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f49cdcccca443be9b7026e0e6c8967a"
          }
        },
        "a2aa728c29ab4f9faa6e1da97f39186d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3133a8b4e7cc42c89ee5609f0c7d4ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32777a2f78274669993200a77b040741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f49cdcccca443be9b7026e0e6c8967a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c1255123c8f4870aa0debb80bc70479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ff63d28a49494880adeb6a16f7342261",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bffbc96b21a14d22989fc74e7eb1aab3",
              "IPY_MODEL_2912346373524603bb7ca7c23fa1c64b"
            ]
          }
        },
        "ff63d28a49494880adeb6a16f7342261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bffbc96b21a14d22989fc74e7eb1aab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0c256c8a865499093924a2d04bb6965",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f08b65271a5c452aa985cfdc39a37e81"
          }
        },
        "2912346373524603bb7ca7c23fa1c64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5996b7413ded4f6d9062d22654c1dc51",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.40MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f78cbced2ec34931a024ed5b9ea87e73"
          }
        },
        "d0c256c8a865499093924a2d04bb6965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f08b65271a5c452aa985cfdc39a37e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5996b7413ded4f6d9062d22654c1dc51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f78cbced2ec34931a024ed5b9ea87e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8525cd3bcf74ff1a823620bd413c280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_362e486d2ac74364bf94b86b4aeb111e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d101245f02f4e25bd295a222415e9a6",
              "IPY_MODEL_d729e182617b4cfc9de14475b499d0de"
            ]
          }
        },
        "362e486d2ac74364bf94b86b4aeb111e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d101245f02f4e25bd295a222415e9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a2c15bd35254f87b379d1782e8d645f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c3e4ded203c4acf83f71fc293710f41"
          }
        },
        "d729e182617b4cfc9de14475b499d0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de4603ffd8884219adb1829ff31b1388",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 3.67MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce29901d1e7b48c59833565e5473a4d0"
          }
        },
        "7a2c15bd35254f87b379d1782e8d645f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c3e4ded203c4acf83f71fc293710f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de4603ffd8884219adb1829ff31b1388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce29901d1e7b48c59833565e5473a4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de63465a1f3946efb0c15327df4c8259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_973395fffcd7476ea0042404e6135177",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_16095192b7eb446ba8f52fe2dc615676",
              "IPY_MODEL_261009b7f49e46d286a1476b960eb86b"
            ]
          }
        },
        "973395fffcd7476ea0042404e6135177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16095192b7eb446ba8f52fe2dc615676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce7f3d27e4db41719740dead54c5266d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 111693,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 111693,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e096a6232e2243439433275eba465992"
          }
        },
        "261009b7f49e46d286a1476b960eb86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed991a31556848648e260f2c4adec9a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 111693/111693 [16:18&lt;00:00, 114.18it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a69fd58a752d41e28b30da0b4bd65632"
          }
        },
        "ce7f3d27e4db41719740dead54c5266d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e096a6232e2243439433275eba465992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed991a31556848648e260f2c4adec9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a69fd58a752d41e28b30da0b4bd65632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e863d88138b24749bebe0a86c895d207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d64f217d2fc649f1b93535b4101eeccc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1408647bf6eb4dfeb8b4f801d38957b3",
              "IPY_MODEL_f22c5303917b4cd7889faa33d6748d79"
            ]
          }
        },
        "d64f217d2fc649f1b93535b4101eeccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1408647bf6eb4dfeb8b4f801d38957b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_beba27d3ff184c619cf8934563f81427",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23a21bd87eae44148d184e236e7e365d"
          }
        },
        "f22c5303917b4cd7889faa33d6748d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f4ea27f2b865402382a7af054827c8dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [1:06:24&lt;00:00, 1992.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49f77211812b4b2f8232611a426b4c8e"
          }
        },
        "beba27d3ff184c619cf8934563f81427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23a21bd87eae44148d184e236e7e365d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4ea27f2b865402382a7af054827c8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49f77211812b4b2f8232611a426b4c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f47e86548581402ab45fcdb731de7de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14d81e67ee234c5688b39ac39ccf355a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5bb135e8f314484cb33cbd96268f545a",
              "IPY_MODEL_bcaf2bccbe31463e9800c3b49c8e9a77"
            ]
          }
        },
        "14d81e67ee234c5688b39ac39ccf355a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bb135e8f314484cb33cbd96268f545a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bddf82c85b4046b091c7c30bfb7ae6ef",
            "_dom_classes": [],
            "description": "Current iteration: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1746,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1746,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_366162efcfeb4d7aa9eeeeecff5b584d"
          }
        },
        "bcaf2bccbe31463e9800c3b49c8e9a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1d8ef3c45014642ad29ec4b86c4b994",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1746/1746 [45:39&lt;00:00,  1.57s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3835607b10642fc8ae9353da920f89d"
          }
        },
        "bddf82c85b4046b091c7c30bfb7ae6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "366162efcfeb4d7aa9eeeeecff5b584d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1d8ef3c45014642ad29ec4b86c4b994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3835607b10642fc8ae9353da920f89d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "811be12291b84b29b2d930dbeb39a9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_886a6e05a888415787e7a332705e35f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c8c14dc74cb14ffbb8f8aa25dad0dc7a",
              "IPY_MODEL_51ad4499b056437c83428676bdd16e6a"
            ]
          }
        },
        "886a6e05a888415787e7a332705e35f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8c14dc74cb14ffbb8f8aa25dad0dc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1c7650139ab845cda9efacd215417267",
            "_dom_classes": [],
            "description": "Current iteration: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1746,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1746,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3a560b7434d48628da61a3a6bcb3ecd"
          }
        },
        "51ad4499b056437c83428676bdd16e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51a01ace02a74702b5b8ff5eba921412",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1746/1746 [33:16&lt;00:00,  1.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46ffb60078e745c2a562016928ca5f54"
          }
        },
        "1c7650139ab845cda9efacd215417267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3a560b7434d48628da61a3a6bcb3ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51a01ace02a74702b5b8ff5eba921412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46ffb60078e745c2a562016928ca5f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc0e198907bf4e0b9b121a8b1de3b90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d02eb10f6aa437ab16d6a2690c72fd6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_712acb06c6a143b3aee1156f4c8863bb",
              "IPY_MODEL_74dd22172f324884825b2cebe492cd8d"
            ]
          }
        },
        "7d02eb10f6aa437ab16d6a2690c72fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "712acb06c6a143b3aee1156f4c8863bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_064906b68d004efb8294447821e17897",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15955,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15955,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1820c5c7f7dd43aea4060d91f52fc4e9"
          }
        },
        "74dd22172f324884825b2cebe492cd8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdb79665d27a4c9abdaa827f86d2f288",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15955/15955 [00:09&lt;00:00, 1630.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9dbd87b817554d6bb09cbcb92f784ffe"
          }
        },
        "064906b68d004efb8294447821e17897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1820c5c7f7dd43aea4060d91f52fc4e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdb79665d27a4c9abdaa827f86d2f288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9dbd87b817554d6bb09cbcb92f784ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "027df15538d3418b9a89fe88fc2b59cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3a05fcefa884bcc99ca4824984af81a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b4e9c639e790498ab229a35862b9972b",
              "IPY_MODEL_c9e4ab98cc23477a993019603144c08d"
            ]
          }
        },
        "e3a05fcefa884bcc99ca4824984af81a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4e9c639e790498ab229a35862b9972b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9eaed76c991c4f1ab7715ed4b37c2535",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1995,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1995,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3f105d7dabc498a8df31245bd8d61e1"
          }
        },
        "c9e4ab98cc23477a993019603144c08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_07426e0df8664bc0ad84a5e72a486c84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1995/1995 [01:56&lt;00:00, 17.09it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_638744826de24608a65e65a7111b52eb"
          }
        },
        "9eaed76c991c4f1ab7715ed4b37c2535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3f105d7dabc498a8df31245bd8d61e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07426e0df8664bc0ad84a5e72a486c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "638744826de24608a65e65a7111b52eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b2c8648e2ca4787b8d651e407b100c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a65f1947f274444b5c95c5284209b1d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3946e1ac30da49b58f4af2d92f25b335",
              "IPY_MODEL_b70cfab9d7214d36aa540a6b3cb2d301"
            ]
          }
        },
        "6a65f1947f274444b5c95c5284209b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3946e1ac30da49b58f4af2d92f25b335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d4616c0248df436f80aaf6b01a3d6580",
            "_dom_classes": [],
            "description": "training routine: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30bdadc3bf6741a99f4fa75080113150"
          }
        },
        "b70cfab9d7214d36aa540a6b3cb2d301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5644a3fe6830451e9a9093a35e923048",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [06:49&lt;00:00, 82.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f703fd0f20d14be6a31bff104de8fd64"
          }
        },
        "d4616c0248df436f80aaf6b01a3d6580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30bdadc3bf6741a99f4fa75080113150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5644a3fe6830451e9a9093a35e923048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f703fd0f20d14be6a31bff104de8fd64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d55e7907ea944c44bca656f76a441e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9bd5849b92cf4d1e921fcfefc1b304f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fbe5002300a54aaab9ce72d6bb3550f0",
              "IPY_MODEL_37b0b358cdf7484bbf0bee91448a9a44"
            ]
          }
        },
        "9bd5849b92cf4d1e921fcfefc1b304f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbe5002300a54aaab9ce72d6bb3550f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_00b299ce581e41ef89ed36fcb02ba014",
            "_dom_classes": [],
            "description": "split=train: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1745,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1744,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6c268b9a1cf4bfe97988c43d3376b12"
          }
        },
        "37b0b358cdf7484bbf0bee91448a9a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90c38f75d56f476695b3b3b87dbfd108",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1744/1745 [06:33&lt;00:00, 26.82it/s, acc=95.4, epoch=4, loss=0.135]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6cf1d0e0b25f47de8b29226beaa1829b"
          }
        },
        "00b299ce581e41ef89ed36fcb02ba014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6c268b9a1cf4bfe97988c43d3376b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90c38f75d56f476695b3b3b87dbfd108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6cf1d0e0b25f47de8b29226beaa1829b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3940f5ecd094121bef4645b9470a97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32c9a19f3ca1489fbcc53632e8081bd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_061cb04b066d4f5598004174fdf3aa5f",
              "IPY_MODEL_19fc61a6a031421c816f7bf7a6840ce3"
            ]
          }
        },
        "32c9a19f3ca1489fbcc53632e8081bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "061cb04b066d4f5598004174fdf3aa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_042e32ae11c54983bf6c86efc643a749",
            "_dom_classes": [],
            "description": "split=val: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 498,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 497,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc130ca7358741dbbc40542652dda1d0"
          }
        },
        "19fc61a6a031421c816f7bf7a6840ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7aae411207b74d6bbd72d32f5da4cccd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 497/498 [06:49&lt;00:00, 30.67it/s, acc=94.6, epoch=4, loss=0.178]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ef9fe89529c4a24b07521603f4eca46"
          }
        },
        "042e32ae11c54983bf6c86efc643a749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc130ca7358741dbbc40542652dda1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7aae411207b74d6bbd72d32f5da4cccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ef9fe89529c4a24b07521603f4eca46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBloDLsVovS7",
        "colab_type": "text"
      },
      "source": [
        "Fait :\n",
        "- Baselines: Naive Bayes, Logistic Regression, SVM, Boosting, Random Forest\n",
        "- Classifieur binaire MLP et Conv1D\n",
        "- Classifieur multilabel Conv1D\n",
        "- Classifier GRU + Conv1D\n",
        "- Classifier Fast Text\n",
        "- Classifier LSTM et LSTM + Attention\n",
        "- Classifier RCNN\n",
        "- Petit test de Transformers\n",
        "- Tenter une segmentation au niveau des caractères, car beaucoup de commentaire contiennent des mots du style \"fu cky ou\" ou \"dickfuck\" ou je ne sais quel autre poésie (ça marche moins bien)\n",
        "- J'ai mis une longueur maximum de 1000 mots par commentaires, pour accélerer le processus, car de toute façon quand y a + de 1000 mots en général c'est juste un copié collé à la suite avec des insultes\n",
        "- Ajouter la possibilité d'utiliser les embeddings préentrainés de Glove Twitter (attention ça prends du temps à download)\n",
        "- Détecteur de biais sur les minorités\n",
        "- Léger pre-processing des commentaires (qui ne semble pas apporter grand chose)\n",
        "- Modifié le balancing au niveau de la génération des batches \n",
        "- Essayé les modèles bi-directionnels\n",
        "- Essayé d'ajouter quelques métriques + significatives que l'accuracy (AUC, F1 Score)\n",
        "\n",
        "A faire :\n",
        "\n",
        "\n",
        "- Ajouter 'seq_length' dans les x_data\n",
        "- Se mettre à utiliser des LSTM ou GRU à un moment puis bi LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0iBFE3ykw5F",
        "colab_type": "text"
      },
      "source": [
        "# 1. Some imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sne6MmP98n44",
        "colab_type": "code",
        "outputId": "62c7f70e-eb71-4dc1-9eeb-ccc91ac76f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import nltk \n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import rcParams\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiEmokyq3rG4",
        "colab_type": "code",
        "outputId": "ce138ffe-ace6-44ac-9321-26986de198bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "sys.setrecursionlimit(10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CPC90lB9OR9",
        "colab_type": "code",
        "outputId": "2f59b79f-0ae4-4166-8244-22a6e898cda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardx\n",
        "!pip install simpletransformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=90c9c607c719af1a7774d184716b23d532fd0bede9dac6cbf9d52a0f6bece47e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.0\n",
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx) (46.1.3)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.0\n",
            "Collecting simpletransformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/bf/92b49029605dc8d3d00c4579007b83c723d3407c9d85ef7e676fa471eb99/simpletransformers-0.28.2-py3-none-any.whl (184kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers>=2.9.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.9.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.3)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (0.1.90)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (0.0.43)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.0->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.1.3)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=b1940b0d9d3620667a8d954a43b3598922b7dd1e115dde100fc2ee4bdfc87b76\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval, simpletransformers\n",
            "Successfully installed seqeval-0.0.12 simpletransformers-0.28.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1sEXR9b0lTo",
        "colab_type": "code",
        "outputId": "7449c12f-fe2b-4a40-e2a6-3ac36565f532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 6750 (delta 0), reused 1 (delta 0), pack-reused 6742\u001b[K\n",
            "Receiving objects: 100% (6750/6750), 13.74 MiB | 27.59 MiB/s, done.\n",
            "Resolving deltas: 100% (4505/4505), done.\n",
            "/content/apex\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-j3uug_ny\n",
            "Created temporary directory: /tmp/pip-req-tracker-kfqavovz\n",
            "Created requirements tracker '/tmp/pip-req-tracker-kfqavovz'\n",
            "Created temporary directory: /tmp/pip-install-o57_qj5a\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-fj9wbr5v\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-kfqavovz'\n",
            "    Running setup.py (path:/tmp/pip-req-build-fj9wbr5v/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "    torch.__version__  =  1.5.0+cu101\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-fj9wbr5v/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-fj9wbr5v/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-fj9wbr5v/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-fj9wbr5v/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-fj9wbr5v/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-fj9wbr5v/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-fj9wbr5v/setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-fj9wbr5v has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-kfqavovz'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-t5ppz8_j\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-fj9wbr5v/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-fj9wbr5v/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-t5ppz8_j/install-record.txt --single-version-externally-managed --compile\n",
            "    torch.__version__  =  1.5.0+cu101\n",
            "    /tmp/pip-req-build-fj9wbr5v/setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-t5ppz8_j/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-fj9wbr5v\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-kfqavovz'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdX050178bk9",
        "colab_type": "code",
        "outputId": "95f7d498-aadb-4c7d-b837-d42bf4b452a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "!wget http://theo.delemazure.fr/perso/toxic_comments.csv\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-13 13:00:41--  http://theo.delemazure.fr/perso/toxic_comments.csv\n",
            "Resolving theo.delemazure.fr (theo.delemazure.fr)... 217.160.0.80, 2001:8d8:100f:f000::267\n",
            "Connecting to theo.delemazure.fr (theo.delemazure.fr)|217.160.0.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65817304 (63M) [text/csv]\n",
            "Saving to: ‘toxic_comments.csv’\n",
            "\n",
            "toxic_comments.csv  100%[===================>]  62.77M  6.72MB/s    in 9.9s    \n",
            "\n",
            "2020-05-13 13:00:51 (6.35 MB/s) - ‘toxic_comments.csv’ saved [65817304/65817304]\n",
            "\n",
            "--2020-05-13 13:00:52--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2020-05-13 13:00:52--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2020-05-13 13:00:52--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  2.17MB/s    in 11m 41s \n",
            "\n",
            "2020-05-13 13:12:33 (2.07 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmsWFYMdaGzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "from simpletransformers.classification import MultiLabelClassificationModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0IEeKudvQAd",
        "colab_type": "text"
      },
      "source": [
        "## 1.a. Playing with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMXqM37ajlZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set()\n",
        "data = pd.read_csv('toxic_comments.csv')\n",
        "for index, row in data.iterrows():\n",
        "  if row.clean and (row.toxic == 1 or row.identity_hate == 1 or row.insult == 1 or row.obscene == 1 or row.severe_toxic == 1 or row.threat == 1):\n",
        "    print(row)\n",
        "#print(data[data.clean == True and data.toxic == 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyZ0GZIejzRE",
        "colab_type": "code",
        "outputId": "302109dd-6300-49b6-b197-c2eccd1ddeb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>insult</th>\n",
              "      <th>obscene</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>split</th>\n",
              "      <th>threat</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>i am only going to fucking say this once those...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>2010 's arthur rubin i want to thank but i lik...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>anonymous you gay afambro fuck you</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>putting stupid comments in articles will resul...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>boobies tatas taters hooters funbags tits hoot...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   clean                                       comment_text  ...  threat  toxic\n",
              "0  False  i am only going to fucking say this once those...  ...       0      1\n",
              "1  False  2010 's arthur rubin i want to thank but i lik...  ...       0      1\n",
              "2  False                 anonymous you gay afambro fuck you  ...       0      1\n",
              "3  False  putting stupid comments in articles will resul...  ...       0      1\n",
              "4  False  boobies tatas taters hooters funbags tits hoot...  ...       0      1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wRoWYhp9m1A",
        "colab_type": "code",
        "outputId": "f8274153-706e-486f-95ca-dc0a7024bf2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        }
      },
      "source": [
        "lcom = []\n",
        "for com in data[\"comment_text\"]:\n",
        "  lcom.append((len(com.split(\" \")), com))\n",
        "\n",
        "lcom = sorted(lcom)[::-1]\n",
        "\n",
        "for i in range(50):\n",
        "  print(lcom[i][0], lcom[i][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4949 you ! you blocked me worm why ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "4850 why do n't you suck my ass until your lips bleed ? congrats ! now you 've lost a good article as well as a reader ! i will never come back to wikishit again ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "4619 i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! i 'm gonna kill you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "3470 lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! 1lol imma donkey lunchables ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "3043 wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! wikipedia teh free enyclopedia ! ! \n",
            "2676 is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? is weezer okay ? answer my question ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "2498 jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wales must die ! ! ! ! ! ! ! ! ! ! ! ! jim wale\n",
            "1626 i own this page you fools ! ! you should fear me ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , geor\n",
            "1624 you should all rue the day you were born ! ! ! ! i have taken over this page ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , i want my cheese , george bush should go to hell ! ! ! ! ! ! ! ! ! i am the god of chocobos , \n",
            "1562 fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you all ! ! fuck you\n",
            "1552 yo fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck you bitch ! ! fuck\n",
            "1538 ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahhahahah yaaa bitch yaaaa ! ! ! ! ! ! ! ah haahh\n",
            "1537 u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! 'u suck ! ! ! ! u suck ! ! ! ! '\n",
            "1495 '''fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuck you ! fuc\n",
            "1429 in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in the name of ytmnd ! in\n",
            "1429 hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi\n",
            "1428 '''hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! ! hi wikipedia ! \n",
            "1412 fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking cocksucking admins , eat my shit cunts ! ! ! ! ! ! ! fucking coc\n",
            "1405 do go fuck off bastard do yyou have a life ? go fuck off bastard and yank your cock through your ass i hate you and hope you go away forever lame is you fuck your mom die die die and all that crap this is for mahy mahonerz ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass i ass \n",
            "1391 fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! fuck off you ass ! \n",
            "1356 just to let you know you have no life i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i\n",
            "1355 take that ! in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the ass in the\n",
            "1344 what is rong with you u pervert i ahte u just leave me alone and for ur info i am 10 turning a 11 in one week on oct 5 so go away i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate you i hate yo\n",
            "1334 i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuck niggas ! i fuc\n",
            "1325 access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! access deneid ! ! a\n",
            "1277 wikipedia , i have one itty bitty question leave my edits alone ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! leave them alone ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! leave them alone nnnnnnnnnnnnooooooooooooooooowwwwwwwwwwwwwwwwww ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! claymort , 7 6 7\n",
            "1254 ''' this site is stupid do n't trust it ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ' '\n",
            "1251 fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you atheist cunt ! fuck you , you at\n",
            "1250 suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my cock d suck my\n",
            "1250 pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig pig\n",
            "1250 oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes oh noes\n",
            "1250 lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol lol\n",
            "1250 fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you ! ! 'fuck you\n",
            "1250 die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag die fag\n",
            "1250 damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you u cunt damn you\n",
            "1249 do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because you are a fggt ! do i know you ? because\n",
            "1247 hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like jews hey i like\n",
            "1247 fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat\n",
            "1246 05 35 , 25 july 2006 ( hist ) ( diff ) glen jacobs ( early career ) 05 22 , 25 july 2006 ( hist ) ( diff ) jonathan coachman ( career ) 05 21 , 25 july 2006 ( hist ) ( diff ) shawn michaels ( return of dx ) 05 21 , 25 july 2006 ( hist ) ( diff ) shawn michaels ( return of dx ) 05 20 , 25 july 2006 ( hist ) ( diff ) shawn michaels ( return of dx ) 05 19 , 25 july 2006 ( hist ) ( diff ) shawn michaels ( return ) 05 18 , 25 july 2006 ( hist ) ( diff ) shawn michaels 05 17 , 25 july 2006 ( hist ) ( diff ) d generation x ( return ) 18 29 , 24 july 2006 ( hist ) ( diff ) hank ( disambiguation ) ( top ) 15 09 , 24 july 2006 ( hist ) ( diff ) you ca n't see me ( track listing ) ( top ) 14 56 , 24 july 2006 ( hist ) ( diff ) stone cold steve austin ( finishing and signature moves ) 14 54 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( extreme championship wrestling ) 10 15 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 10 14 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 10 11 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 10 10 , 24 july 2006 ( hist ) ( diff ) fishman ( wrestler ) ( trivia ) 10 09 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr ( smackdown ! ) 10 09 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr ( smackdown ! ) 10 08 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( world heavyweight champion ) 10 07 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( world heavyweight champion ) 10 07 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( world heavyweight champion ) 10 06 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( world heavyweight champion ) 10 05 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( world heavyweight champion ) 10 05 , 24 july 2006 ( hist ) ( diff ) oscar gutierrez ( world heavyweight champion ) 10 03 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr ( smackdown ! ) 10 03 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr ( smackdown ! ) 10 01 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr ( smackdown ! ) 10 00 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr ( smackdown ! ) 09 59 , 24 july 2006 ( hist ) ( diff ) chavo guerrero , jr 09 57 , 24 july 2006 ( hist ) ( diff ) image chavo hitting rey jpg ( chavo 's heel turn on rey ) ( top ) 06 28 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 06 28 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 06 27 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 06 27 , 24 july 2006 ( hist ) ( diff ) the great american bash ( 2006 ) 06 25 , 24 july 2006 ( hist ) ( diff ) cross canadian ragweed ( top ) 06 24 , 24 july 2006 ( hist ) ( diff ) cross canadian ragweed ( album ) ( top ) 06 24 , 24 july 2006 ( hist ) ( diff ) live and loud at the wormy dog saloon ( top ) 06 23 , 24 july 2006 ( hist ) ( diff ) cross canadian ragweed ( album ) 06 22 , 24 july 2006 ( hist ) ( diff ) garage ( album ) ( top ) 06 20 , 24 july 2006 ( hist ) ( diff ) highway 377 ( top ) 06 19 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 1 ( top ) 06 18 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 2 ( top ) 06 18 , 24 july 2006 ( hist ) ( diff ) live and loud at billy bob 's texas ( top ) 06 18 , 24 july 2006 ( hist ) ( diff ) live and loud at billy bob 's texas 06 17 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 2 06 16 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 1 06 15 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 1 06 13 , 24 july 2006 ( hist ) ( diff ) live and loud at the wormy dog saloon 06 13 , 24 july 2006 ( hist ) ( diff ) live and loud at the wormy dog saloon 06 12 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 1 06 12 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 2 06 11 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 2 06 10 , 24 july 2006 ( hist ) ( diff ) image liveloud2 ( jpg ( i dustind made this image ) ( top ) 06 10 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 2 06 09 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg ( top ) 06 09 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 2 06 08 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg 06 07 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 1 06 06 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg 06 05 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg disc 1 06 05 , 24 july 2006 ( hist ) ( diff ) live loud at the wormy dog saloon bootleg 06 05 , 24 july 2006 ( hist ) ( diff ) live\n",
            "1243 teabag ! teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , teabag , \n",
            "1240 you are 22 years older than your husband ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! you are an old cougar ! \n",
            "1240 caltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i will ban you , and i will kill youcaltlas if you continue to rvv it , i will blank your talk page , i will block you , i wi\n",
            "1238 that was really milleseconds wasnt it bitch fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jewfat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat jew fat\n",
            "1202 this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this person is an asshole ! ! ! this pers\n",
            "1200 dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will fuckin kill you ! i will block you , i will blank your talk page , i will steal your password , i will take over you ! i will block you ! dust filter , if you rvv it again ! i will f\n",
            "1198 i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going to murder you i am going to kill you i am going t\n",
            "1192 you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit where it 's due ! you fucking kike ! do n't edit things you have no idea about ! give credit\n",
            "1188 i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a homo sexual i am a\n",
            "1177 so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than i realized i need to be banned so i was all like who farted , than\n",
            "1167 hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chicken rice ) hey guys i love chic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIN8-K0LOEU8",
        "colab_type": "code",
        "outputId": "a7a296ee-4bc1-4be5-84c2-79c88ce8a162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "train = data[data.split == \"train\"].copy()\n",
        "train['length'] = train['comment_text'].apply(len)\n",
        "toxic_data = train[train['toxic'] == 1]\n",
        "sevtoxic_data = train[train['severe_toxic'] == 1]\n",
        "nontoxic=train[train.iloc[:,2:7].sum(axis=1)==0]\n",
        "\n",
        "count=pd.Series(' '.join(toxic_data['comment_text']).split()).value_counts()\n",
        "count=count.sort_values(ascending=False)\n",
        "count_head=count.head(10)\n",
        "count_head.plot(kind = 'bar', figsize=(5,10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc3807f7b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj3DZ2X4Objn",
        "colab_type": "code",
        "outputId": "b0c9500c-28f4-45c5-d1dc-2a6e3816f0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "bins = 1000\n",
        "plt.hist(toxic_data['length'], alpha = 0.4, bins=bins, label='toxic',density=True)\n",
        "plt.hist(sevtoxic_data['length'], alpha = 0.4, bins=bins, label='severe_toxic',density=True)\n",
        "plt.hist(nontoxic['length'], alpha = 0.4, bins=bins, label='non-toxic',density=True)\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('number_ratio')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(0,200)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SrbmovvO_nw",
        "colab_type": "code",
        "outputId": "56833da9-17c5-4a81-f153-f4c09263a6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "#x=train.iloc[:,2:7].sum()\n",
        "nb_occ = train[[\"clean\", \"toxic\", \"insult\", \"obscene\", \"severe_toxic\", \"identity_hate\", \"threat\"]].sum()\n",
        "#plot\n",
        "plt.figure(figsize=(8,4))\n",
        "print(nb_occ)\n",
        "ax= sns.barplot(nb_occ.index, nb_occ.values, alpha=0.8)\n",
        "plt.title(\"WIKI comment # per class\")\n",
        "plt.ylabel('# of Occurrences', fontsize=12)\n",
        "plt.xlabel('Type ', fontsize=12)\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = nb_occ.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n",
        "print('ratio of wiki toxic comment', nb_occ[[\"toxic\", \"insult\", \"obscene\", \"severe_toxic\", \"identity_hate\", \"threat\"]].sum()/len(train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean            100336.0\n",
            "toxic             10719.0\n",
            "insult             5521.0\n",
            "obscene            5904.0\n",
            "severe_toxic       1112.0\n",
            "identity_hate       996.0\n",
            "threat              340.0\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAEdCAYAAACytDX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd1hUR9sG8HsXpIgiooBIVBCjscQGiF3BQpGi2HgVjaImGo1dQU2wRmOJxqixx1hjL7FrYjeKYo0tKiAi0gQRaQu7O98ffG7cALpsWEC9f9eVK+zMKc85i8uzM3NmJEIIASIiIqJCkpZ0AERERPRuYhJBREREWmESQURERFphEkFERERaYRJBREREWmESQURERFphEkFEVAqEhoaibdu2JR0GUaEwiSAqJitXrsTgwYPVyjp37pxv2cGDBwEAderUQVRUFABgyZIlGD9+vGq7+Ph4uLu7Y9asWRBCoF+/ftixY4eOr6J0ePLkCerUqQO5XK7R9ps3b8bChQsBAH379sW9e/d0GR7RB4NJBFExcXR0xLVr16BQKAAACQkJkMvluHv3rlpZVFQUHB0d33ismJgY9O3bF66urvj6668hkUh0Hv+77Pbt22jQoAGUSiUiIiJQq1atYjnvq/eV6H3FJIKomHz66aeqpAEAwsLC4OzsDDs7O7Wy6tWrw8rKqsDjPH78GAEBAfD29sbEiRM1Pv/27dvh4eGBJk2awNPTE7dv3wYAhIeHo1+/fnB0dESXLl3wxx9/qPYJDg7GtGnTMHjwYDRp0gT+/v5ITEzEt99+CycnJ7i7u+POnTuq7V1dXbFmzRp4e3ujcePGmDx5Mp49e6baf8CAAXjx4oVq++vXr8Pf3x+Ojo7w8fFBaGioqq5fv3744Ycf4O/vjyZNmiAwMBDJyckAgICAAACAk5MTmjRpgmvXrr3x2m/duoX69esjMjISNjY20NfXL3Db4OBghISEYODAgWjSpAkCAgIQExOjqg8PD8fAgQPRrFkzuLm54dChQ2r7Tp06FUOGDEHjxo3VrueVlJQUTJo0Ca1bt4aTkxO+/PLLfONYtWoVOnbsqHq/jh8/rqqLiopCQEAAHBwc4OzsjNGjRwMAhBCYPXs2WrRogaZNm8Lb2xv3799/470h+k8EERWbgIAAsW7dOiGEENOnTxc7duwQCxcuVCsLDg5WbV+7dm3x6NEjIYQQP/74o+jdu7do3bq1WL58eb7H3r59e77nPXTokGjdurW4ceOGUCqV4tGjR+LJkyciOztbdOzYUSxfvlzIZDLx559/isaNG4vw8HAhhBBBQUGiWbNm4q+//hJZWVmiX79+wsXFRezZs0fI5XKxcOFCERAQoDqPi4uL6Nmzp0hMTBRxcXGiefPmomvXruL27duq/ZcsWSKEECIuLk40a9ZMnDp1SigUCnHu3DnRrFkzkZSUpLqeDh06iIiICJGZmSkCAgLE/PnzhRBCREdHi9q1a4ucnJwC77VMJhMODg6iadOm4pNPPhEODg6iYcOGon79+sLBwUH89NNP+e4XFBQkGjduLC5duiRkMpmYOXOm8Pf3F0IIkZ6eLtq2bSt27twpcnJyxO3bt0WzZs3EgwcPVPs2bdpUhIWFCYVCIbKysvIcf8iQIWLUqFEiJSVFZGdni9DQUCGEEBcvXhRt2rRRe8/i4uKEQqEQBw8eFI0aNRLx8fFCCCHGjBkjfvrpJ9U5Ll++LIQQ4syZM6Jbt27ixYsXQqlUiocPH6r2IdIFtkQQFaNmzZrh8uXLAHJbHRwdHeHg4KBW1qxZswL3v3//PjIzM+Hp6Vmo8+7cuRODBw9Gw4YNIZFIUKNGDdjY2ODGjRvIyMjA559/DgMDA7Ro0QIuLi6qMRkA0KlTJzRo0ACGhobo1KkTDA0N0bVrV+jp6cHT01PVivJKQEAAKleuDCsrKzg6OqJhw4aoV6+eav9XLRf79u1D27Zt0a5dO0ilUrRq1QoNGjTA6dOnVcfy8/ODnZ0djIyM4O7unudcb2JgYICwsDAEBwejX79+CAsLg4ODA7Zs2YKwsDAMGzaswH3bt28PJycnGBgYYMyYMbh+/TpiY2Nx6tQp2NjYoHv37tDX10e9evXg5uaGI0eOqPbt0KEDHBwcIJVKYWhoqHbchIQEnDlzBtOnT0eFChVQpkyZAt9vDw8PWFlZQSqVwtPTEzVq1MDNmzcBAPr6+nj69CkSEhJgaGio6v7S19dHeno6IiIiIISAvb09LC0tNb5nRIVVcJseERU5R0dHbN68GSkpKUhOToatrS0qV66M4OBgpKSk4MGDB28cD+Hq6opKlSrhs88+w6ZNm2BjY6PReWNjY1G9evU85QkJCahSpQqk0n++T1StWhXx8fGq15UqVVL9bGRkhMqVK6u9zsjIUDvm6/WGhoYFbv/06VMcOXIEJ0+eVNXL5XI4OzurXltYWKh+NjY2znOuNxkzZgzOnj2LzMxMGBgYYNeuXcjIyMDNmzdha2uLnTt3FrhvlSpVVD+bmJigQoUKSEhIQExMDG7evKn2HikUCvj4+KheW1tbF3jcuLg4VKhQARUqVHhr/Hv37sW6detUXSkZGRl4/vw5AGDChAlYvHgxevTogQoVKmDgwIHo0aMHWrRogb59+2LGjBmIiYlB586dERQUhHLlyr31fETaYBJBVIyaNGmCtLQ0bN++HU2bNgUAlCtXDpaWlti+fTssLS1RrVq1Nx5j0qRJyM7OxmeffYbNmze/cfzEK9bW1nj8+HGecktLS8TFxUGpVKoSidjYWNja2hb+4grJ2toavr6+mDVrVqH31WQg6aJFi6BQKNC6dWucO3cOJ0+exJEjR7BgwYK37hsXF6f6OT09HS9evIClpSWsra3h5OSEdevWFTpmIDc5efHiBVJTU2FqalrgdjExMfj666/xyy+/oEmTJtDT04Ovr6+q3sLCQnXfwsLCMHDgQDg5OaFGjRro378/+vfvj6SkJIwePRpr1qxRjZkgKmrsziAqRkZGRmjQoAF++eUXtW+zDg4OecreJCQkBM7Ozvjss8/w7Nmzt27fo0cP/Pzzz7h16xaEEIiKikJMTAwaNmwIIyMjrFmzBjk5OQgNDcWJEycK3V2iDR8fH5w8eRJnz56FQqGATCZDaGio2h/wgpibm0MqlSI6OvqN20VERKB69erQ09PDnTt30KBBA41iO336NMLCwpCdnY3FixejUaNGsLa2Rvv27fHo0SPs3bsXOTk5yMnJwc2bNxEeHq7RcS0tLdG2bVtMnz4dL168QE5Ojqor63WZmZmQSCQwNzcHAOzatQsPHjxQ1R8+fFh1nypUqACJRAKpVIqbN2/ixo0byMnJgbGxMQwMDNRamYiKGn+7iIqZk5MTkpKS4ODgoCpzcHBAUlISnJycNDqGRCLBzJkz0bBhQwwcOFD11EJBPDw8MHToUIwbNw5NmzbF8OHD8eLFCxgYGGDFihU4c+YMmjdvjunTp2PevHmwt7f/T9eoCWtra/z0009YuXIlWrRogXbt2mHt2rVQKpVv3dfY2BhDhw7F//73Pzg6OuL69ev5bnf79m3Uq1cPAAqVRHh5eWHZsmVwdnbG7du3MX/+fAC5rUZr167FoUOH0KZNG7Ru3RoLFixAdna2hlcNzJs3D/r6+vDw8EDLli2xfv36PNvUqlULgYGB8Pf3R8uWLXH//n1VyxUA/PXXX+jZsyeaNGmCYcOGYcqUKahWrRrS09Px9ddfo1mzZnBxcYGZmRkGDRqkcWxEhSURQoiSDoKIqLQIDg6GlZUVxowZU9KhEJV6bIkgIiIirTCJICIiIq2wO4OIiIi0wpYIIiIi0gqTiCKwadMm+Pn5oUGDBggODlaru3DhAtzd3dGoUSP069dPbQ7+7OxsTJo0CU2bNkWrVq3Unj1/+PAh/Pz84OTkBCcnJwwYMAAPHz5U1f/yyy/o0KEDmjZtitatW2P27Nl5VjRcv349XF1d0bhxY3h4eCAyMjLf+IUQmD9/PpydneHs7Iz58+eDDVRERPQ2TCKKgKWlJb788kt0795drTw5ORkjRozAqFGjcOnSJTRo0EBtxPeSJUsQFRWFkydPYsOGDVizZg3OnDmjOuaPP/6IS5cu4eLFi3B1dVXb19XVFXv27MHVq1dx4MAB3Lt3Dxs3blTV79ixAzt37sSqVatw7do1rFy5EhUrVsw3/m3btuH333/Hvn378Ntvv+HkyZPYunVrUd4iIiJ6D3HGSi08f54OpfKfb+oODi0BAJcuXUVqahqSktIAAHv3HoCtbU00a9YGaWk56Ns3EJs2bcaVK3/B1tYOu3fvxpQp0yCX68HMzApeXr7YunUH6tdvCkAKY2MzJCenQy6XIytLjqioKNWxTUzMkZMDJCWl4cWLNCgUAn///RBJSWlQKpX48ccl+PrraahYsQqSk9NhYmIOhQKq/V+3Y8cu9OrVF2XK5E6N26tXH+zYsROdO3vr+E4SEVFpJpVKULGiSYH1TCK0oFQKtSTiFSEEhICqLiIiHLVqfax6bWhoBBsbG0REhMPMzBzPnj2Dvf0/9fb2H+PMmVNqx3Z3b4/MzEwolUoMGvSFWt2xY0ewYMEcZGSkw8zMDMOHj4ZSKRAXF4+EhHg8fPgQs2ZNg56eHtzdu2DgwCH5zl4XGRmeJ46IiIh8r5GIiOgVJhE6lJmZATMz9S6EcuXKISMjA5mZuQsJmZiUy1P3uiNHTiEzMxOHDx9AlSrqC/t07uyOzp3dER39GEeOHFRNkZuYmLt40uXLF7F+/Vakpb3EmDEjYGFhCR+fbvnEmam2QI+JSTlkZmZACKHRGgVERPRh4pgIHTI2Lov09HS1svT0dJQtWxbGxmUBABkZ6Xnq8h7HGF27dsesWVPx/Hne6Y2rVasOO7ua+P77uQCgWn64T5/+KF++PKytq8LX1w8XLpwvIE5jpKf/082Rnp4OY+OyTCCIiOiNmETokJ1dTYSH31e9zszMREzME9jZ2cPU1BSVKlXGw4f/1D98+AB2djXzPZZSqURWVhYSExPyrVcoFIiJeQIAqF7dFmXKlFFLAt6UD9jZ2ePhw38W93n48H6BcRAREb3CJKIIyOVyyGQyKJVKKJW5qxHK5XK0beuCiIhwnDr1B2QyGdatWw17+49Ro4YtAMDdvQvWr/8ZqampiIp6hP3798DDI3cw4+XLF3H//j0oFAqkp6dh6dJFKF++PGrUsAMA7N+/V9UqERkZgY0bf4GjY+7iTUZGRnB17YQtWzYgIyMdCQnx+O23PWjVqnW+8bu7e2Lbts1ITEzAs2eJ2Lp1Mzw9vXR814iI6F1XLDNWzp07F0ePHkVMTAz279+P2rVrAwAiIyMRHByMlJQUmJmZYe7cubC1tS2RusLIfQLin9u2du1KrFu3Wm2bgQOHYNCgL3D5cigWLZqHuLg41KtXH1OmTIO1dVUAufNELFgwB6dOnYChoSH69u0Pf/8AAMCJE79jzZrlSExMgKGhIerWrY8vvhiBWrU+BgDMnj0dFy6cV427cHHpiMGDh6q6MtLT0zBv3rf488/zKF++PLy9u2LAgMGQSCS4ceMaxo8fiePHzwLIHRC6fPmP2L9/HwDA29sXw4aNZHcGEdEHTiqVoFKlcgXWF0sSERYWBhsbG/Tt2xcrVqxQJRH9+/dH9+7d4evri3379mHXrl3YsGFDidQVxr+TCCIiovfR25KIYunOcHR0hLW1+pMFSUlJuHPnDry8cpvNvby8cOfOHSQnJxd7HRERERVeiT3iGRsbCysrK+jp6QEA9PT0YGlpidjYWAghirXu1aOR/4WBoT4E3o/WCQkkyJbJ374hERF90DhPhBbya9pJTcvCvLWnij8YHQga5AILi/IlHQYREZVyJZZEWFtbIz4+HgqFAnp6elAoFEhISIC1tTWEEMVaV1j5jYkoY6gHuVxRVLenRCmVSiQmvizpMIiIqISVijER+alUqRLq1q2LAwcOAAAOHDiAunXrwtzcvNjriIiIqPCK5emMWbNm4dixY3j27BkqVqwIMzMzHDx4EOHh4QgODkZqaipMTU0xd+5c1KyZO8lRcdcVRkEtEXNWn/gvt6nUmDTEFTmy96NVhYiItFcqHvF83zCJICKiD0Gp7c4gIiKidxuTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSCpMIIiIi0gqTCCIiItIKkwgiIiLSSqlIIk6ePImuXbvC19cXPj4+OHbsGAAgMjISvXv3hpubG3r37o1Hjx6p9tFFHREREWmuxJMIIQQmTpyIefPmYd++fZg3bx6CgoKgVCoxdepU9OnTB0ePHkWfPn0QEhKi2k8XdURERKS5Ek8iAEAqleLly5cAgJcvX8LS0hLPnz/HnTt34OXlBQDw8vLCnTt3kJycjKSkpCKvIyIiosLRL+kAJBIJfvjhB3z55ZcoW7Ys0tPTsWrVKsTGxsLKygp6enoAAD09PVhaWiI2NhZCiCKvMzc31zjmSpXK5SlLTcuCvr7ef70dpYJUKoWFRdmSDoOIiEq5Ek8i5HI5Vq5ciZ9++gkODg64cuUKRo8ejXnz5pV0aAVKSkqDUinUysoY6kEuV5RQREVLqVQiMfFlSYdBREQlTCqV5PvF+ZUSTyLu3r2LhIQEODg4AAAcHBxgbGwMQ0NDxMfHQ6FQQE9PDwqFAgkJCbC2toYQosjriIiIqHBKfExElSpVEBcXh4iICABAeHg4kpKSUKNGDdStWxcHDhwAABw4cAB169aFubk5KlWqVOR1REREVDgSIYR4+2a69dtvv2H16tWQSCQAgJEjR6Jjx44IDw9HcHAwUlNTYWpqirlz56JmzZoAoJM6TRXUnTFn9Yn/eitKhUlDXJEjez+6ZoiISHtv684oFUnEu4ZJBBERfQjelkRo1Z1x8eJFXLp0SeugiIiI6N2nURIREBCAK1euAABWrVqFsWPHYty4cVixYoVOgyMiIqLSS6Mk4sGDB2jcuDEAYMeOHdiwYQO2b9+OrVu36jQ4IiIiKr00esRTqVRCIpHg8ePHEEKgVq1aAIAXL17oNDgiIiIqvTRKIhwcHDBjxgwkJiaiU6dOAIDHjx+jYsWKOg2OiIiISi+NujPmzJkDU1NT1KlTByNGjAAAREREoH///joNjoiIiEovjVoiKlasiLFjx6qVtW/fXhfxEBER0TtCo5aI7OxsLFq0CB06dFBNT33u3Dls2rRJp8ERERFR6aVREjF79mzcv38fCxYsUM0q+fHHH+PXX3/VaXBERERUemnUnfH777/j2LFjKFu2LKTS3LzDysoK8fHxOg2OiIiISi+NWiLKlCkDhUJ9GuTk5GSYmZnpJCgiIiIq/TRKItzd3REUFITo6GgAQEJCAmbMmIEuXbroNDgiIiIqvTRKIsaMGYOPPvoIPj4+SE1NhZubGywtLTF8+HBdx0dERESlVKFX8UxOTkbFihVVAyw/RFzFk4iIPgRFsorn3r17ce/ePQCAubk5JBIJ7t27h7179xZNlERERPTO0SiJWLx4MaytrdXKqlSpgsWLF+skKCIiIir9NEoi0tLSUK6cenNG+fLlkZqaqpOgiIiIqPTTKImwt7fH0aNH1cqOHz8Oe3t7nQRFREREpZ9Gk02NHz8en3/+OQ4fPoxq1arh8ePHuHDhAlatWqXr+IiIiKiU0qglwtHREfv378enn36KzMxMNGzYEAcOHFCto0FEREQfHo1aIgDAxsYGn3/+uS5jISIioneIRklESkoKfv75Z9y9excZGRlqdZs3b9ZJYERERFS6aZREjBs3DtnZ2fDw8ICxsbGuYyIiIqJ3gEZJxLVr13Dx4kUYGBjoOh4iIiJ6R2g0sLJOnTqIi4vTdSxERET0DtGoJaJ58+YYPHgw/Pz8ULlyZbW6Hj166CQwIiIiKt00SiLCwsJgZWWF8+fPq5VLJBImEURERB8ojZKIjRs36joOIiIiesdoNCYCAJ4/f469e/dizZo1AID4+HiOkyAiIvqAaZREXLp0Ce7u7ti/fz+WLVsGAIiKisK0adN0GRsRERGVYholEbNnz8YPP/yAtWvXQl8/twekUaNGuHnzpk6DIyIiotJLoyQiJiYGLVq0AJA7mBIAypQpA4VCobvIiIiIqFTTeCnws2fPqpX9+eefqF27dpEEIZPJMHXqVHTu3Bne3t745ptvAACRkZHo3bs33Nzc0Lt3bzx69Ei1jy7qiIiISHMaJRHBwcEYP348goKCkJWVhZCQEAQHB2PChAlFEsT8+fNhaGiIo0ePYv/+/Rg1ahQAYOrUqejTpw+OHj2KPn36ICQkRLWPLuqIiIhIcxIhhHjbRkqlEomJifjtt9/w9OlTWFtbw8fHB1WqVPnPAaSnp6Ndu3Y4ffo0TExMVOVJSUlwc3NDaGgo9PT0oFAo4OzsjGPHjkEIUeR15ubmGseclJQGpVL9tpUx1MOc1Sf+8/0oDSYNcUWOjF1VREQfOqlUgkqVyhVY/9Z5IhQKBZo0aYKwsDAMGTKkSIMDgOjoaJiZmWHp0qUIDQ2FiYkJRo0aBSMjI1hZWUFPTw8AoKenB0tLS8TGxkIIUeR1hUkiiIiISIMkQk9PD7a2tnj+/DmsrKyKPACFQoHo6GjUq1cPQUFBuHHjBoYOHYrFixcX+bmKSn5ZWWpaFvT19UogmqInlUphYVG2pMMgIqJSTqMZK729vTF06FD0798/TxfGq6c2tGVtbQ19fX14eXkByH10tGLFijAyMkJ8fDwUCoWq6yEhIQHW1tYQQhR5XWEU1J0hl78fXQC53VcvSzoMIiIqYf+5OwMAfv31VwDAkiVL1MolEgn++OOP/xAeYG5uDmdnZ5w/fx6tW7dGZGQkkpKSYGtri7p16+LAgQPw9fXFgQMHULduXVW3gy7qiIiISHNvHVipVCoRGhoKBwcHGBgY6CSI6OhoTJ48GSkpKdDX18fo0aPRrl07hIeHIzg4GKmpqTA1NcXcuXNRs2ZNANBJnaY4sJKIiD4Eb2uJ0OjpjCZNmuDatWtFGti7jEkEERF9CN6WRGg0T4STkxOuX79eZEERERHRu0+jMRFVq1bFkCFD0KFDB1SpUkU19TUA1cRQRERE9GHRKImQyWTo2LEjgNwlwImIiIg0SiLmzJmj6ziIiIjoHaNREhEdHV1gXbVq1YosGCIiInp3aJREdOrUCRKJBK8/yPFqXMTdu3d1ExkRERGVaholEffu3VN7nZiYiKVLl8LR0VEnQREREVHpp9Ejnv9mYWGBKVOmYOHChUUdDxEREb0jtEoiACAiIgKZmZlFGQsRERG9QzTqzujTp4/a3BCZmZl4+PAhhg8frrPAiIiIqHTTKIno2bOn2mtjY2N88sknsLW11UVMRERE9A7QKIno1q2bruMgIiKid4xGYyJGjBiBsLAwtbKwsDCMHDlSJ0ERERFR6adREnH58mU0adJEraxx48YIDQ3VSVBERERU+mmURBgYGOR5EiMjIwP6+hr1hhAREdF7SKMkonXr1ggJCUFaWhoAIC0tDTNmzECbNm10GhwRERGVXholEcHBwUhLS0OzZs3QokULNGvWDGlpaZg8ebKu4yMiIqJSSqP+iAoVKmDVqlVITExEbGwsrK2tYWFhoevYiIiIqBTTKIk4d+4cbGxsYGdnp0oeIiIiEBsbi1atWuk0QCIiIiqdNOrOmDFjBkxMTNTKTExMMGPGDJ0ERURERKWfRklEUlISLC0t1cosLS2RmJiok6CIiIio9NMoiahWrRouXLigVhYaGoqPPvpIJ0ERERFR6afRmIgRI0bgq6++Qo8ePVCtWjVER0dj9+7dmD17tq7jIyIiolJKo5aIjh074ueff0ZGRgZOnz6NjIwMrFmzBh07dtR1fERERFRKaTzlZMOGDdGwYUNdxkJERETvkLcmEU+ePMHSpUtx/vx5PH/+HBUrVkTLli0xYsQIVKtWrThiJCIiolLojd0Z4eHh8PPzQ1JSEsaMGYPly5djzJgxSE5ORvfu3REeHl5ccRIREVEp88aWiAULFqBPnz4YPXq0Wrmfnx8WLVqE+fPnY8WKFToNkIiIiEqnN7ZEhIWFITAwMN+6wMBAhIWF6SQoIiIiKv3emEQoFIoCl/vW19eHQqHQSVBERERU+r0xifj000+xe/fufOv27NmDBg0a6CQoIiIiKv3eOCZi1KhRGDRoECIjI+Hm5gYLCwskJibiyJEj2LNnD9auXVtccRIREVEp88aWiKZNm+Lnn3/GvXv3MGDAAHh4eGDAgAG4d+8e1qxZg6ZNmxZpMEuXLkWdOnVw//59AMD169fh4+MDNzc3BAYGIikpSbWtLuqIiIhIc2+dsbJJkybYvHkzrl69itOnT+PKlSvYsmULHBwcijSQ27dv4/r167CxsQEAKJVKTJgwASEhITh69CgcHR2xYMECndURERFR4Wg07TUAGBkZwcrKCsbGxkUeRHZ2NmbMmIFp06apym7dugVDQ0M4OjoCAPz9/XHkyBGd1REREVHhaJxE6NLixYvh4+OjtipobGwsqlatqnptbm4OpVKJlJQUndQRERFR4Wi8doauXLt2Dbdu3cL48eNLOhSNVapULk9ZaloW9PX1SiCaoieVSmFhUbakwyAiolKuwCRi7ty5CAoKAgBcuHABLVq00EkAly9fRnh4ODp06AAAiIuLw6BBg9CvXz88ffpUtV1ycjKkUinMzMxgbW1d5HWFkZSUBqVSqJWVMdSDXP5+zJuhVCqRmPiypMMgIqISJpVK8v3irKovqGL79u2qn4cPH160Ub3m888/x7lz53DixAmcOHECVapUwdq1azF48GBkZWWpZsXcunUr3N3dAQANGjQo8joiIiIqnAJbIj755BOMHMzOn5AAACAASURBVDkS9vb2yM7OxuLFi/PdbtSoUToJTCqVYt68eZg6dSpkMhlsbGwwf/58ndURERFR4UiEECK/iqSkJGzbtg1Pnz7F3r174e3tne8B5syZo9MAS6OCujPmrD5RQhEVrUlDXJEjez+6ZoiISHtv684osCWiUqVK+PLLLwHkrqHxISYLREREVDCNns6YM2cOXrx4gZMnTyI+Ph5WVlZo3759oQckEhER0ftDo3kirl27hk6dOmHr1q34+++/sXXrVnTu3BnXrl3TdXxERERUSmnUEjF79mxMnToVXbp0UZUdOnQIs2bNwq5du3QWHBEREZVeGrVEPHr0CB4eHmplbm5uePz4sU6CIiIiotJPoySiRo0aOHjwoFrZkSNHUK1aNZ0ERURERKWfRt0ZkydPxtChQ7Fx40ZUrVoVMTExiIqKwooVK3QdHxEREZVSGiURTZs2xfHjx3Hq1CkkJCTAxcUF7dq149MZREREHzCNF+CqUKECfH19dRkLERERvUNKxVLgRERE9O5hEkFERERaYRJBREREWtE4iYiJidFlHERERPSO0TiJ6NatGwBgw4YNOguGiIiI3h1vfDrDz88P9evXR926daFQ5C4NvXTpUvTv379YgiMiIqLS640tEYsXL0arVq3w9OlTZGVloVu3bsjOzsbFixfx8uXL4oqRiIiISqE3JhFKpRLu7u4YP348TExM8NNPP0EIgU2bNsHX1xedO3curjiJiIiolHljd8b48eMRGxsLe3t7yGQyvHjxAoaGhli6dCkAICUlpViCJCIiotLnjUnEjh07IJfLcf/+ffTp0wczZ85Eeno6pk6divr166NevXqc+pqIiOgD9danM/T19VGvXj2UKVMGmzdvhrGxMZydnfHo0SMsWLCgOGIkIiKiUkjjtTMmTZoEAJBIJPD09ISnp6fOgiIiIqLST+N5Ivz8/AAAv//+u86CISIiondHoae9rlChgi7iICIioncM184gIiIirTCJICIiIq0wiSAiIiKtMIkgIiIirTCJICIiIq0wiSAiIiKtMIkgIiIirTCJICIiIq0wiSAiIiKtlHgS8fz5cwwZMgRubm7w9vbGiBEjkJycDAC4fv06fHx84ObmhsDAQCQlJan200UdERERaa7EkwiJRILBgwfj6NGj2L9/P6pVq4YFCxZAqVRiwoQJCAkJwdGjR+Ho6KhaNVQXdURERFQ4JZ5EmJmZwdnZWfW6cePGePr0KW7dugVDQ0M4OjoCAPz9/XHkyBEA0EkdERERFU6JJxGvUyqV+PXXX+Hq6orY2FhUrVpVVWdubg6lUomUlBSd1BEREVHh6Jd0AK+bOXMmypYti4CAABw/frykwylQpUrl8pSlpmVBX1+vBKIpelKpFBYWZUs6DCIiKuVKTRIxd+5cREVFYcWKFZBKpbC2tsbTp09V9cnJyZBKpTAzM9NJXWEkJaVBqRRqZWUM9SCXKwp72aWSUqlEYuLLkg6DiIhKmFQqyfeLs6q+GGMp0MKFC3Hr1i0sW7YMBgYGAIAGDRogKysLYWFhAICtW7fC3d1dZ3VERERUOBIhhHj7Zrrz4MEDeHl5wdbWFkZGRgCAjz76CMuWLcPVq1cxdepUyGQy2NjYYP78+ahcuTIA6KROUwW1RMxZfeK/3o5SYdIQV+TI3o9WFSIi0t7bWiJKPIl4FzGJICKiD8E70Z1BRERE7x4mEURERKQVJhFERESklVLziCe9H3bt2oZDhw4gIuIhOnZ0w5Qp01R1YWGXsHDhXMTHx6FevQaYMmUaqlSxBgAEBPRCfHysatvs7Gw4O7fEvHmLAABz536L69ev4MmTaEyaFAJPT2+1bVesWII//jgOmUyGjh3dMHr0eOjr5//r/eDB35gzZyaioiJRo4YdJk36Bh9/XEcHd4OI6P3GlggqUpUrW+CzzwahSxcftfKUlBRMmTIBgwcPw6FDJ/DJJ/UQEjJJVb9p03YcP34Wx4+fxbFjZ2BpaQUXlw6q+lq1Psa4ccGoXfuTPOfctOkX3Lt3Fxs3bsOvv+7G/fv3sH792nzjy8nJQXDwOLi5eeDw4ZPw8PBCcPA45OTkFNEdICL6cDCJoCLVrp0r2rZtD1PTCmrlp0+fgJ2dPVxdO8LQ0BCBgZ/j4cMHiIp6lOcY169fRUpKCtq3/yeJ6N69Fxwdm6nmEXnd+fNn0aOHP0xNK6BixYro0cMfBw/+lm98165dgUKhQK9efWBgYICePf0hhMCVK5f/24UTEX2AmERQsYiMjECtWh+rXhsbG8PGxgaRkeF5tj18+ADat3eFsbGxxsdXf1JZICEhHmlpafnEEQ57+48hkUhUZbVqfYzIyAiNz0VERLmYRFCxyMzMgImJ+rPG5cqVQ0ZGhlpZVlYWTp06AQ8PL42P7ezcAjt3/ornz58jKekZduzYqjpW3jgyUa6ciVqZiUk5ZGSka3w+IiLKxYGVVCyMjcsiPV39D3V6ejrKllVf6Ov06RMwNTVFkyYOGh/7s88CkZb2EgMH9kGZMgbw8emKBw/+hrm5eT5xGBcQh0mebYmI6M3YEkHFws6uJsLD76teZ2ZmIibmCezs7NW2O3z4ANzdu6h1N7yNoaERxo4Nwt69h7Fjxz6YmlZAnTp1IZXm/fW2s7NHePhDte6P8PAHsLOrqcVVERF92JhEUJGSy+WQyWRQKpVQKhWQyWSQy+Vo29YFERHhOHXqD8hkMqxbtxr29h+jRg1b1b4JCfG4du1Kvl0ZOTk5kMlkAITaOQAgMTEBz54lQgiBW7f+wvr1azFo0Bf5xtekiQOkUil27NiK7Oxs7Nq1DQDg4OBU5PeCiOh9x7UztMC1Mwq2du1KrFu3Wq1s4MAhGDToC1y+HIpFi+YhLi4O9erVx5Qp02BtXVW13caN63Dhwnn89NOaPMcdMeJzXL9+Va3sxx9XoGlTR1y/fhWzZk3F8+fJsLS0wsCBQ9C5s4dqu3HjRqJRo8bo3z8QAHD//j18990sPHoUCVtbWwQHf5Pvo6NERB86LsClA0wiiDQzYsTnuHPnFvT09ADkziPy66+7IYTAhg0/Y9++3UhLS0OLFi0xceIU1eDb7OxsLFgwB6dOnYCRkRH69OkHf/+APMdft2411q5diUWLlsHJyTnfGGJjn2L27Om4c+cWrKyqYMyYiQVuS0Tq3pZEcGAlkQ4V9Ef06tUwjBo1DEZGRqptx44NgoeHF7Kzs/H9998hLOwSUlNTYWPzEb74YjhatGgFILdrZ/r0Kbh37y7i4mJVLTIFSU19gTlzZuLy5YuoUMEMX3wxAp07u+v2wl8zZsxEeHt3VSs7cuQgjh49hOXL16J8eVPMmPE1Fi2aj6+/ng4A+PnnVXjyJBo7d+5HcnISRo4cClvbmmjevKXqGDExT3Dy5O+oVKnyG88/bdoUNGjwKRYsWIwLF87jm2+C8Ouve1CxYsWiv1iiDwyTCCoS5YwAKZQlHUaRUEKKtLxPh2otvz+iQG5CsWfPoTzlCoUClpZWWLp0FaysquDChfMICZmEDRu2qrp/GjZsjJ49+yAkJOit5//++7koU6YMfvvtGB48uI+JE0ehVq2PUbOm/Vv31ZXz58/Ay8sXVlZVAAB9+36GUaOGYfz4STAyMsLhwwcwZco0mJqawtTUFN7eXXH48H61JOL77+di2LCv8P33cws8z+PHUbh//x4WLVoKQ0MjtG/fAdu3/4rTp/9A1649dH6dRO87JhFUJKRQ4v72RSUdRpGo3WsMSnLMsbGxsdrA0Fat2qBq1ar4+++7sLauijJlyqBXrz4AAKlU743HyszMxOnTJ7BhwzaULVsWjRo1RuvWbXH06CEMG/aVTq/jlZUrl2LFiiWoXr0Ghgz5UtVq8npPqhAC2dnZiI5+DCurKkhKeqY2OVmtWh/j7NlTqtcnTvwOA4MyaNGiNYCCk4jIyAhUrWqj9ggvJxcjKjp8OoNIx1auXIouXTpg2LBAXL0apip//jwZ3t6d0bOnD3788XtkZmbmu39ychKiox/neRxWE9HRUdDT00P16jVUZfb2tYvtj+iwYSOxffs+7NlzGD4+fggKGouYmCdwdm6J/fv3ITb2KdLS0rB583oAgEyWhczM3AnIXp+c7PWJyTIy0rFq1TKMGjX+refPb5Kz3MnFMgrYg4gKgy0RRDo0bNhI2NnZQV+/DP744xiCgsbil1+2oEYNW6xbl/v/uLhYfPvtNCxZshATJ05R218ul2P69G/g7t5F7XFYTWVmZhYwU2jxzNBZv34D1c8eHl44fvwoLlw4Bz+/XkhIiMdXX30BhUIBf/++OH/+LCwsLGFsnDsBWUZGOgwNDQGoT0y2du0quLl5qj3ZUxBj47J5rjUjI+8kZ0SkHbZEEOlQ/foNULasCQwMDODh4YVPP22ECxfOoVKlyrCzqwmpVIqqVW0wbNhInD6t/nSPUqnEzJnfoEwZfYwd+/axD/nJnaFTfQ2RkpyhUyKRQAhAKpVi0KAvsHPnfuzZcwh2dvawsLCEhYUlTE1NUalSZTx8+M/kZA8f/jMh2JUrl7Fz51b4+LjBx8cNCQnxCAmZhE2bfslzPju7mnj6NEYtkXj9WET03zCJICpGr/6I5lf++mPDQgh8991MJCcn49tv50FfX7tGw2rVakChUCA6+rGq7OHD+8XyR/Tly5cIDb2gmnDs2LHDuHHjKpydWyA19QViYp5ACIHIyAgsWbIQAwYMVs0y6u7eBevX/4zU1FRERT3C/v174OHhDQBYvPgnbNiwDevWbca6dZtRubIFJkyYDD+/XnliqF69BmrVqo2ff14NmUyG06dPIjz8Adq165BnWyIqPHZnEOnIy5cvcefOLTRu3BR6eno4ceI4bty4ilGjxuHq1TBUrWoDK6sqSEiIx4oVS9CmTTvVvgsWzMGjR5H44YefYGholOfY2dnZqoGJr2bwNDAwyDNduLGxMdq1c8GaNSsQHPwNHjz4G+fOncby5T/r9uL/P67Vq5cjKuoR9PSkqF7dFnPmLED16jXw+HEUgoLGICEhHmZmFdGzpz98ff1U+w4a9AUWLJiDHj28YWhoiL59+6uezKhQwUztPFKpFOXLl1d1UcyfPxsAMGHCZADA9Omz8e230+Dh4QorKyvMnDmXj3cSFRFONqUFTjaVl6nR+/V0RmrWf2+ke/78OSZMGKX2R3TIkKFwcmqOrVs3YevWzXj5MhUVKpihbdv2+PzzL1G2rAni4mLRo4c3DAwMVPNLALl/FF/NxNmjhzfi4mLVzrdjx2+wtq6KDRt+xo0b1/H99z8CeDVPxAxcvhwKU9MKGDr0q2KdJ4KI3l2csVIHmETkxSSCSN2uXdtw6NABREQ8RMeObpgyZRqAt08WdvVqGNatW4379++hfHlT7Ny5X1X3/HkyfvhhAa5fv4qsrEzUrGmPESPGqg1gfZ0QAsuXL8GBA/sAAF5evhg27KtCLXBHH7a3JRH8pCQi0oHKlS3w2WeD0KWLT566hg0b45tvZqJSpUp56oyMjNCliw++/HJUnrqMjAzUrVsPa9duwqFDJ+Du7oWJE0cV+Mjqvn27cfbsKfzyyxasX/8r/vzzLPbt2/XfL04Ljx5FYuTIoXBza4fevbvi9OmTqrr9+/eid++u6NSpDcaO/QrPniWq7fv33/cwfPgQdOrUBt7enbF9+68Fnics7BL69OmODh1a4auvvsjTYkdFi0kEEf0nJgZAuTLK9+I/E4Oiuy/t2rmibdv2MDWtoFb+arKwRo0a5ztZWL16DeDu3gVVq9rkqbOx+Qj+/gGoXLky9PT04Ovrh5wcOR4/jso3hiNHDsLfPwCWllawsLCEv39fHDp0oGgusBDkcjmCg8ehZcvWOHToBCZOnIKZM7/B48dRuHo1DCtXLsOcOd/j0KETqFq1KqZN++dR55SUFIwb9xV8ff1w8OAf2LZtD5o1a57veVJSUjBlygQMHjwMhw6dwCef1ENIyKTiusw3mjHjG/j6uqFz53bw9/fD/v1782yzbt1qtG7tiMuXQ1Vl2dnZmD17Ojp3bgcfHzds3brpjefZtm0zfHxyzzN79nRkZ2cX+bW8jgMriYqAgbEEQvJ+TPstEVJkZ2reyykRSoTO+16HERUf54nj8C59t3rw4G/I5Tn46KNq+dZHRoajVq3aqte1ahXfRGOve/z4EZKSEtG7d19IJBI4ODjh008b4ejRQ5DJZHBx6aiahn3AgMHo2tUDMTFPYGPzEbZt2wxn5+aq8UAGBgawtbXL9zynT5+AnZ09XF07AgACAz9Hly4dERX1SKt5VopSQMAABAd/AwMDA0RFPcJXX32Bjz+ug08+qQug4LVgNFlH5pXQ0AvYtGk9Fi9ejsqVLTB58nisXbtSp7PTMokgKgJCosSCo8tLOowiMd5tGAD2mZd26elpmDkzBAMHDkG5cvn3WWdmZqrVmZiUQ2ZmBoQQJT4uQgiBiIhw2Nh8lGcKdACIiHgIG5uPcPv2X6hZsxaGDg3EkyfRqFevAcaODUKVKlXyHDMyMkJtunRjY2PY2NggMjK8xJOI19eqkUhy/4uJeaJKIgpaC0aTdWRe39bLy1ctIZsx4xudJhHvTspNREQAcqcHDwoai/r1P0W/fgML3O7fk42lp6fD2LhssScQ1avbwszMHFu2bIBcLselSxdx/fpVyGRZcHZugZMnj+PhwweQybKwbt1qSCQSZGXlroKXkJCAI0cOYNSocdi16wCqVq2K6dMn53ue/KY5f33K9JK2YMF36NChFfr06YFKlSqrVuZVXwvmH6mpqfmuI1NQa9K/k6hatWojOTkJL16k6OBqcjGJICJ6h2RnZ2PSpPGwsLBUzYVREDs7ezx8+ED1urgmGvs3fX19zJmzABcunFf167u6doKFhSWcnJwRGPgFvv56Inr08IG1dVWULVsWlpZWAABDQ0O0beuCunXrw9DQEAMHDsFff91EWlpanvMYG5dFerr6NOevT5le0saPD8axY2ewbNkatG3rAgMDgzeuBfO2dWTy2/7f2wLQaRLF7gwiIh2Qy+VQKBRQKpVQKhWQyWTQ09ODvr7+GycLUyqVyMnJgVwuhxACMpkMUqkUZcqUgVwux9dfB8HQ0BBTpkxTzfBZEHd3T2zbthktWrSCRCLB1q2b0aNH3pk9i0OtWh9j6dJVqtdDhwbC3b0LAKB7917o3j03rsePo7B+/VrVgnO1atVSO86bWlHs7GriyJF/Bo5mZmYiJuaJVovX6Yqenh4aNWqMY8cOYc+enYiLiy1wLZi3rSOT3/avJ1GvWqF0mUSxJYKISAfWr1+LDh1aYdOmX3D06GF06NAK69evBQDVI4iJiQkYO3YEOnRopXoU8fr1q+jQoRUmTBiF+Pg4dOjQCmPGDAcA/PXXDfz551lcunQRHh4u6NSpDTp1aoMbN64BAG7cuIZOndqoYvD17Y5Wrdqgf39/9OvXGy1btoKvb/divhO5crsrZMjKysKWLRuRlPQMnp7ekMlkiIh4CCEE4uLiMG/et+jZ838wNTUFAHh6+uDMmVP/P4hUjl9+WYOGDRvnOw6kbVsXRESE49SpPyCTybBu3WrY239c4uMh8qNQKBAT8+SNa8G8bR2Zf7Ozq5lnW3PzSnlmeS1KbIkgItKBQYO+wKBBX+Rb9/oEUv/WtKkjzp0Ly7euSROHAusAoFGjJjh+/KzqtUQiwZdfjsp3zonidvToIezfvxcKhRwNGzbBokXLYGBggJcvX2L69K8RE/MEZcuawNPTG4MHD1Xt5+DghC++GI4JE0YjKysLDRs2xtSps1T1AQG90L//QHTu7IGKFSti1qx5WLRoHmbMCEG9evUxffrskrhcNc+fJ+PKlcto2bINDA0NERZ2Cb//fhTTpn2LgQMHQy6Xq7YdMuQzjBgxRjVw8tU6MnXq1MPz58nYv38PJk2amu953N27/P/joB6oXNkC69evhYeHl06vjUkEERHp3PDhozB8eN5kpnz58li/fusb9+3WrQe6deuRb92mTdvVXjs5OWPLlpKZUKtgEuzduwsLFsyBUilQpUoVjBw5Dq1bt8uz5b/XgnnTOjJxcXHo168nNm7cgSpVqqB585bo06cfRo4cCplMhvbtXQtMZIvKB5lEREZGIjg4GCkpKTAzM8PcuXNha2tb0mEREdF7qGLFimrjQd7k361UBgYGmDx5KiZPztv6UKVKFbWWJwDw9w+Av3+A9sEW0geZREydOhV9+vSBr68v9u3bh5CQEGzYsKGkwyKid5CR4fv1MZolk799o9cYGwLA+7IEkwSZspKO4d3yfv32ayApKQl37tzBunXrAABeXl6YOXMmkpOTYW5urtExpNK8o4OlEgkqmhoXaawlRSqR5HuNbySRoEw53Q3eKVZaXL9UApiVNdVRQMWrsO+/RCqBkdn78d5LpIV/7yUSCX7bcUlHERUvn57NtLh+gcM7ftFNQMXMo+fAwn/2vefedj8+uFU8b926haCgIBw8eFBV5unpifnz56N+/folGBkREdG7hY94EhERkVY+uCTC2toa8fHxUCgUAHKf1U1ISIC1tXUJR0ZERPRu+eCSiEqVKqFu3bo4cCB3VrMDBw6gbt26Go+HICIiolwf3JgIAAgPD0dwcDBSU1NhamqKuXPnombN4p9PnoiI6F32QSYRRERE9N99cN0ZREREVDSYRBAREZFWmEQQERGRVphEEBERkVaYRJQirq6uuH///ts3fE8sWbIE2dnZWu8/ZcoUhIUVvCxyaeXr64usrKwiP25oaCj8/PwAAKmpqVi9enWRn0Nbr8dG9CF6/fMuODgYmzZt0un5fv/9d9y8eVOn5wCYRFAJWrp0KXJycrTe/9tvv4Wjo2MRRlQ89u3bByMjI52eIzU1FWvWrNHpOah0efLkCbZt2/afj/MqyX1TslsUX3j+/SVi8eLFOHToEIDcpPPcuXNaH/vJkydwdnYu9H53795VxVDUtPm8k8sLtxja64orifjgFuAqLa5du4Z58+YhPT0dADBx4kS1+oSEBMyaNQtPnz6FTCZDly5dMHToUADA3LlzcenSJeTk5KBixYqYPXs2bGxs8OTJE3Tv3h3+/v44ffo0MjMzS+0f2unTpwMA/P39IZVKsXbtWkydOhWPHz8GAAwaNAhdu3bFvn37sGnTJmzZsgV6enoIDAyEm5sb/ve//6Ffv34IDAyEi4sLXr58idmzZ+PWrVuQSCRwdHRESEhISV5igerUqYOrV6/CxMQErq6u8PX1xZ9//onExEQEBgYiICAASqUSM2bMwMWLF2FgYICyZcti69atCA0Nxdy5c7F7924AyPP6lRkzZuDly5fw9fWFsbExtm7dWmzXd+bMGSxcuBAKhQLm5uaYMWMGgNwPxIkTJ+L27dswNjbGd999h1q1aiEiIgKTJk1CZmYmlEolunXrhkGDBiE7OxuLFi3C2bNnIZVKUa1aNSxbtgwAsGrVKhw7dgwKhQJWVlaYOXMmLCwssGTJEkRGRuLly5eIjo5G9erVsXjxYhgbG6uOd/nyZWRnZ6NOnTqYNm0aTExMiu3eaEoul0Nfv3AfzzExMdi2bRt69+79n869b98+tf/rytKlSxEYGAgDAwMAwKhRo1R1ly5dQkZGBlq3bq3TGP7t7t27OHXqFDw9PYv0uP/+vLOxscH9+/fRv39/xMXFoXHjxpg7dy4kEgmCg4Ohp6eHyMhIpKenY9++fdizZw+2bNkChUKBcuXKYdq0aahZsyb+/vtvTJ8+HZmZmZDJZOjVqxcGDBiAs2fP4sSJE/jzzz+xY8cODBw4EF27di3Sa1IRVOyeP38uWrZsKa5cuSKEEEIul4uUlBTh4uIi/v77byGEEAMGDBCXLl0SQgghk8nE//73P3Hu3DkhhBBJSUmqY23fvl2MHj1aCCFEdHS0qF27tjhx4oQQQoh9+/aJ3r17F9t1FVbt2rVFWlqaEEKIUaNGiUWLFgkhhIiPjxetWrVS3YtJkyaJOXPmiCVLloiRI0eq9g8ICFBda3BwsJgxY4ZQKBRCCPV7VNq8ft0uLi7iu+++E0Lkvn+NGzcWaWlp4vbt28Ld3V11PSkpKUIIIS5evCi6deumOtbrr1//OTo6WjRr1qzYrumVZ8+eCWdnZ/HgwQMhRO7vZ48ePcTFixdF7dq1RWhoqBBCiN27d6tinTlzplixYoXqGK+udcmSJWL48OFCJpMJIf55T/fu3Su+/vpr1b3ZvHmzGDt2rBBCiB9//FF06tRJvHjxQiiVSjFw4ECxbds2IYQQy5YtE8uWLVOdZ968eWLhwoVvvaaMjAzx1VdfCQ8PD+Ht7a36Hdy9e7fo0aOH6Natm+jXr58IDw8XQgjRqVMncffuXdX+GzduFMHBwUIIIcLDw8WgQYOEn5+f8Pb2Fjt37lRtV7t2bfHjjz8KPz8/sWjRIvHy5UsxefJk0b17d+Hl5SVmzpwp5HJ5gXF6enqKhg0bCh8fH/HVV18JIYS4ceOG6NWrl/Dy8hK9evUSN27cEEIIMXnyZPHtt98KIYRITEwULi4u4s6dO6o40tLSRO3atcVff/0lBg4cKFxdXUWDBg1EmzZtxPTp00X79u1V/z7fdk3Lly8Xfn5+wtXVVRw5ckQIIcS0adNE7dq1hZeXl/Dx8REvXrwQQUFBYuPGjeLevXuiZcuWonnz5sLHx0esXLlSTJs2TaxevVp13Nu3b4vOnTsLpVKZ77149fu/cOFC4evrKzp37iwuX74shBAiJydHBAYGim7duglPT08RHBwsZDKZSE5OFu3atRMODg7Cx8dHzJw5UwghxPXr10VAQIDo1q2b6Natmzh58mTBvyxv8Pq/+6CgIOHv7y+ysrKETCYTnp6eqs/3oKAg0a1bN5Geni6EEOLy5ctiyJAhqn8Hp06dUn2uv3z5UlWelpYmPDw8xMOHD1XH+ckddQAADapJREFU2bhxo1axFgZbIkrA9evXYW9vj6ZNmwIA9PT0UKFCBVV9RkYGLl26hOTkZFVZeno6wsPD0apVK5w5cwZbtmxBRkZGnuausmXLwsXFBQBU2e274MKFCwgODgYAWFpaol27dggNDUXt2rUREhICPz8/yOXyPN+4Xzl58iR2794NqTS3h+5dmsb81beejz76CKampoiLi0O1atUgl8sxZcoUODs7q97T0u7GjRv45JNPUKtWLQBA9+7dMX36dKSnp6NGjRpo1qwZgNwm82+++QZpaWlwcnLC/PnzkZmZCWdnZzRv3hxA7nsaHBys+qb66j09ceIEbt26hW7dugGA6tvZK61b/1979x4UVfk/cPy9ck1WAg1Nc3MMBWQ0F1MaKC+tFqTsyj0iK9GRdCySBoaBDHVGkYSKUTOyixaYjkgol6HMTLFMGsiJatKaFRQdFBMDVkQuu78/+HFGVLyg49bXz+svOLf9POfsPudznuec8zyJs3PXsOyPPvqo0rq1d+9eTCYTX3/9NQBtbW14eXndsEzff/89Fy5cUJq5GxsbqaiooLS0lC1btmBvb8/+/ftJSUlh27ZtBAcHU1BQQHJyMgBffvklycnJdHR0kJCQQEZGBu7u7phMJsLCwtBqtbi7uwPg4OBAfn4+0HXPz6RJk1i1ahVms5mEhATy8/OJjIy8Zpypqak9WqXa2tqIi4tj9erV+Pn5cfDgQeLi4ti9ezdvvfUWERER7Nmzh9zcXObPn8+YMWOu2mZ8fDxxcXGsWbOGjz/+GA8PD3788Ue2bNkCcFNlUqvV5OfnU1lZyZIlSwgICGDZsmV88cUXbNu27aqWIE9PT6KiomhpaSEpKQnoesvwwoULmT9/PiqVitzcXKKjo1Gpeh+m+p9//kGr1RIfH09hYSGZmZls27YNGxsbMjMzcXV1xWKxkJSURH5+Ps8//zxxcXHs27ePtWvXAl3dgsuWLWPjxo0MHjyY+vp6wsPDKS4uVr5jfTVjxgwcHBwA8Pb25sSJEzzxxBMABAYG0r9/f6Dre3vkyBEiIiIAsFgsNDU1AdDa2sry5cs5evQoKpWK+vp6jhw5ouz7u0GSiH8hs9mMSqVix44d2NnZ9Zh36tQpVq9ezY4dO9BoNPz8888kJCQo87srXIB+/frdVp/av8XZs2dpaWlBpVJhMpl6nDD+F3RXJNCVUHZ2djJgwABKSkooLy/n4MGDZGZmUlBQgI2NDZbLXjJ76dIla4R8RwUEBKDVavnhhx/46KOPyM/PJzMzs9flLRYLixYtIjw8/Jrzr9yf3fvIYrGwbNky/Pz8bik+Ly8vjEYjK1aswNfXl2nTpl23Yg8ODiYyMpLExESMRiNNTU1MnDgRo9GI0WjkjTfeULbd3t7OsWPHlEq/OzGCrpNHVVUVmzZtArpOGEOGDLnpuKurq7Gzs1PK6+/vj52dHdXV1Xh6epKVlUV4eDiTJ0/mhRdeuOY2Ojo6GD16NPfdd59yj8HMmTOVrsKampoblqk7SdZqtdTX13Pp0qUex+hmuLu7o9FoKCsrQ6vVsnfvXiVJ601vF1Rms5lPP/2UsrIyzGYzjY2Nvd6jdPjwYU6ePMmCBQuUaSqViuPHjzNu3LhbKsOVrvW7vzz2bhaLhbCwsB7dPd3effdd3NzcSE9Px9bWlnnz5t31OkGSCCvQarUYjUYOHz6Mj48PnZ2dmEwmZb5areaxxx5j48aNLF68GIC6ujpsbW0xmUzY2dnh5uaG2Wy+q33dd5qTkxMmkwknJyf8/PzYvn07cXFxnD17lv379zN37lza2tqIj48nMTGR1tZW4uPjycnJuaq/+KmnnuKTTz5h6dKlqFQqGhoa/lOtEVdqaGjAxsaGyZMn4+/vz759+6itrUWj0VBbW0tjYyPOzs6UlJRcc321Wk1ra2uf+tZvh1arJSUlBaPRiLu7OwUFBXh7e+Pk5MSJEyeoqKhg4sSJFBUV4eHhgVqt5vjx42g0GkJDQxkxYgQpKSlA1zH97LPPGD9+PPb29sox1el0fP755zz99NPcf//9tLW1cezYsRu2Kuh0OjZv3oyPjw+Ojo6YTCbOnDlzw6s2jUZDcXExhw4doqysjPfee4/p06f3WrEPGzaMUaNGUVZWxk8//URISAgqlQqLxYKrq+t17zW48uSxYcMGNBrNdePrK6PRiJOTE2fPnr3l70l3C8DNlKn7ZGljYwN0JSa3mkQAvPjii2zduhWj0cgzzzzDgAEDrrt8bxdURUVFVFZWsmXLFtRqNdnZ2dTU1FxzGxaLBU9PT6Xl5XZcXt/dCp1OR1JSEs899xwPPvggnZ2d/PHHH4wdO5bm5mY8PT2xtbXlzz//pKKigqCgIKCrDmhubr7tuG9Ens6wAhcXF9atW0d6ejp6vZ7Q0FB+//33HstkZmZiNBrR6/Xo9Xri4+NpamrC09OTwMBAZs6cSUREBMOHD7dSKW7fvHnzeOmll5g9ezZLly7lyJEj6PV65s2bR0JCAqNHjyYjI4MxY8Ywa9YswsLCGD58OFlZWVdtKzk5mQsXLhAUFITBYGDDhg1WKNGdU1dXR0xMDAaDAYPBwJQpU9BqtQwZMoSYmBhCQ0OJiorCzc3tmuu7uLgo352oqKi7FvfAgQNZs2YNCQkJ6PV6CgsLycjIAMDDw4O8vDyCgoLIyclhzZo1AJSWlqLX6wkODmblypVKEhEbG8tDDz1EcHAws2fPZvny5UDXlb7BYGDOnDnK76eysvKGscXGxuLl5UV4eDh6vZ7o6GiMRuMN1zt9+jQ2NjbMmDGD5ORkGhoa0Ol07Nq1i9OnTwNdXSq//fabsk5ISAh5eXkUFxcrrQsjR47E0dGRnTt3KssZjcYeFxCX0+l0bNy4UblCbWhooLa2ttc41Wp1j22NHDmS9vZ2Dh06BHR1GXZ0dDBy5Ehqa2tJS0sjNzeXhx9++Jq/KQBbW1v++usvWltbqaio4Pz583z11VdKq8utluly3SfV3spy5Qlw6tSpVFdXs2nTJqKjo2+4/d40Nzfj6uqqfEb3iM7X+lwfHx+OHz+u7EOAqqqqHq2BN+vy+q57/92MSZMmsWTJEhYtWoTBYCAoKIhvv/0WgEWLFpGXl4der2f9+vVMmjRJWc9gMFBcXMzs2bN7HJ87TQbgEkKI69i/fz/vvPMO0NUUbjAYiI2NpbCwkE2bNtHZ2Ul7ezuBgYFKy8TFixd58sknGTduHJs3b1a2VVNTQ1paGnV1dZjNZgYNGkRWVhYDBw7s8dQOgMlkIiMjg8rKSlQqFXZ2dqSkpPT6tFVHRweLFy/m1KlTPPLII6xdu5aqqipWrVpFS0sL/fv3580338TLy4vo6Ghefvll9Ho9LS0thIeHk5SUxNSpU5U4JkyYQH5+PhkZGZw8eZL6+noGDhzI9OnT+e677/jwww/x8PC4pTJd/v/69espKirC0dGRnJwc0tLSGDt2LHPmzKG2tpZXX30VgFmzZhEbGwvABx98QFlZGVu3br3uMet+Uq28vPyq/5ubm3nttdc4ffo0gwYNYtSoUVy6dIn09HSam5tZsGABLS0t+Pr6snTpUqqqqsjIyKCxsZH29nY0Gg3Z2dnK/Vf3OkkihBBC/CfExMQQGRnJs88+a+1QxP+TVEoIIcS/2q+//sqMGTMYMGAAAQEB1g5HXEZaIoQQ4j9k4cKF1NXV9Zg2dOhQsrOzrRSR9aSmpvLLL7/0mGZjY9Pro+DizpMkQgghhBB9It0ZQgghhOgTSSKEEEII0SeSRAghhBCiT+SNlUKIu8LHx0f5++LFi9jb2ytvMVyxYgUGg8FaoQkh+khurBRC3HU6nY6VK1fi7+9v7VCEELdBujOEEFbT1taGr68vR48eVaadO3eO8ePH09DQQHl5OVOmTCE7O5vHH38cnU5HYWFhj/Xffvttpk2bhr+/P6mpqbS2tlqjKELckySJEEJYjb29PTNnzuyRGBQXF+Pn56cMoPb3339z/vx5Dhw4QHp6OqmpqRw7dgzoGmOmurqanTt3snv3burr63n//fetUhYh7kWSRAghrCokJISSkhJlUKNdu3ZddX/E66+/jr29Pb6+vkydOpXS0lIsFgvbt28nJSUFFxcX1Go1r7zySq8jmwoh7jy5sVIIYVXjx4/H0dGR8vJy3NzcOHHiBNOnT1fmOzs79xgie9iwYdTX19PQ0MDFixcJDQ1V5lksFsxm812NX4h7mSQRQgirCwkJobCwEDc3NwICAnBwcFDmNTU1KaNQQtcw6aNHj8bV1RVHR0dKSkoYMmSItUIX4p4m3RlCCKszGAzs2bOHwsJCgoODr5q/bt062traqKioYN++fQQGBtKvXz8iIiJIS0vj3LlzAJw5c4YDBw7c7fCFuGdJEiGEsLqhQ4fi7e2NSqVi4sSJPeY98MADODs7M3nyZBISEli+fDnu7u4AJCYmMmLECCIjI5kwYQJz586lurraGkUQ4p4k74kQQvwrJCcnM3jwYOLj45Vp5eXlJCYmUlZWZsXIhBC9kXsihBBWd/LkSb755hsKCgqsHYoQ4hZId4YQwqqysrLQ6/XMnz8fjUZj7XCEELdAujOEEEII0SfSEiGEEEKIPpEkQgghhBB9IkmEEEIIIfpEkgghhBBC9IkkEUIIIYToE0kihBBCCNEn/wdxaS+gmybUJwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ratio of wiki toxic comment 0.22017494381921876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edxq25t88wO_",
        "colab_type": "text"
      },
      "source": [
        "# 2. Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWh_mOkARQ_",
        "colab_type": "text"
      },
      "source": [
        "La classe \"Vocabulary\" créé un dictionnaire qui associe à chaque mot du vocabulaire un entier (son index). Elle contient des fonctions pour aller du mot vers son index et inversement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TinpzN8zqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token \n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "        \n",
        "    def to_serializable(self):\n",
        "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "            \n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\n",
        "        \n",
        "        Args:\n",
        "            tokens (list): a list of string tokens\n",
        "        Returns:\n",
        "            indices (list): a list of indices corresponding to the tokens\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "        \n",
        "        Args: \n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "            KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qFvUhwAcFF",
        "colab_type": "text"
      },
      "source": [
        "La classe \"SequenceVocabulary\" hérite de la classe vocabulary et permets de définir des token pour les débuts de phrase, fin de phrase, mot inconnu et \"mask\" pour après la fin de la phrase (si les phrase d'un minibatch sont de longueur différente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcoJ-BZU_8Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = self.add_token(self._unk_token)\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "          or the UNK index if token isn't present.\n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
        "              for the UNK functionality \n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaL5cI_k_j8V",
        "colab_type": "text"
      },
      "source": [
        "# 3. The Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9hA0fgcB4p_",
        "colab_type": "text"
      },
      "source": [
        "Le vectorizer permet de transformer un commentaire en un suite de token (leur index  + les tokens spéciaux)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-4LQypFvhB4",
        "colab_type": "text"
      },
      "source": [
        "## 3.a. Word level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb-AY7JC_Wwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CommentVectorizer(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
        "    def __init__(self, comment_vocab, bigrams=False):\n",
        "        self.comment_vocab = comment_vocab\n",
        "        self.bigrams = bigrams\n",
        "    def vectorize(self, comment, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            comment (str): the string of words separated by a space\n",
        "            vector_length (int): an argument for forcing the length of index vector\n",
        "        Returns:\n",
        "            the vetorized comment (numpy.array)\n",
        "        \"\"\"\n",
        "\n",
        "        # Added bigrams\n",
        "        indices = [self.comment_vocab.begin_seq_index]\n",
        "        tokens = comment.split(\" \")\n",
        "        if args.bigrams:\n",
        "            tokens = generate_bigrams(tokens)\n",
        "        indices.extend(self.comment_vocab.lookup_token(token) \n",
        "                       for token in tokens)\n",
        "        indices.append(self.comment_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices[:vector_length]\n",
        "        out_vector[len(indices):] = self.comment_vocab.mask_index\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, comments_df, cutoff=5, bigrams=False):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            comments_df (pandas.DataFrame): the target dataset\n",
        "            cutoff (int): frequency threshold for including in Vocabulary \n",
        "        Returns:\n",
        "            an instance of the NewsVectorizer\n",
        "        \"\"\"\n",
        "        \n",
        "        # Bigrams\n",
        "        word_counts = Counter()\n",
        "        for comment in comments_df.comment_text:\n",
        "            tokens = comment.split(\" \")\n",
        "            if args.bigrams:\n",
        "              tokens = generate_bigrams(tokens)\n",
        "            for token in tokens:\n",
        "                if token not in string.punctuation:\n",
        "                    word_counts[token] += 1\n",
        "        \n",
        "        comment_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                comment_vocab.add_token(word)\n",
        "        \n",
        "        return cls(comment_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        comment_vocab = \\\n",
        "            SequenceVocabulary.from_serializable(contents['comment_vocab'])\n",
        "\n",
        "        return cls(comment_vocab=comment_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'comment_vocab': self.comment_vocab.to_serializable()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANB6chGdvkxc",
        "colab_type": "text"
      },
      "source": [
        "## 3.b. char level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg0snxG-t00U",
        "colab_type": "text"
      },
      "source": [
        "Voici une deuxième classe de vectorizer, pour séparer plus finement, au niveau des caractères"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX4VXqSEt6UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CommentVectorizerChar(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
        "    def __init__(self, comment_vocab, max_comment_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            surname_vocab (Vocabulary): maps characters to integers\n",
        "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
        "            max_surname_length (int): the length of the longest surname\n",
        "        \"\"\"\n",
        "        self.comment_vocab = comment_vocab\n",
        "        self._max_comment_length = max_comment_length\n",
        "\n",
        "    def vectorize(self, comment, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            comment (str): the comment\n",
        "        Returns:\n",
        "            one_hot_matrix (np.ndarray): a matrix of one-hot vectors\n",
        "        \"\"\"\n",
        "        indices = [self.comment_vocab.begin_seq_index]\n",
        "        indices.extend(self.comment_vocab.lookup_token(token) \n",
        "                       for token in comment)\n",
        "        indices.append(self.comment_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices[:vector_length]\n",
        "        out_vector[len(indices):] = self.comment_vocab.mask_index\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, comments_df,cutoff=5):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            comments_df (pandas.DataFrame): the surnames dataset\n",
        "        Returns:\n",
        "            an instance of the SurnameVectorizer\n",
        "        \"\"\"\n",
        "        max_comment_length = 0\n",
        "\n",
        "        char_count = Counter()\n",
        "        for comment in comments_df.comment_text:\n",
        "          max_comment_length = max(max_comment_length,len(comment))\n",
        "          for token in comment:\n",
        "            # Modified this\n",
        "            if token not in string.punctuation and token not in stopword:\n",
        "              char_count[token] += 1\n",
        "        \n",
        "        comment_vocab = SequenceVocabulary()\n",
        "        for char, char_c in char_count.items():\n",
        "            if char_c >= cutoff:\n",
        "                comment_vocab.add_token(char)\n",
        "\n",
        "        return cls(comment_vocab,  max_comment_length)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        comment_vocab = Vocabulary.from_serializable(contents['comment_vocab'])\n",
        "        return cls(comment_vocab=comment_vocab,  \n",
        "                   max_comment_length=contents['max_comment_length'])\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'comment_vocab': self.comment_vocab.to_serializable(),\n",
        "                'max_comment_length': self._max_comment_length}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f34L0xEuBd28",
        "colab_type": "text"
      },
      "source": [
        "# 4. The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z60cgo9hSoVd"
      },
      "source": [
        "Cette classe permet d'accéder facilement aux commentaires du dataset\n",
        "\n",
        "J'ai rajouté un paramètre \"balanced\". Ce paramètre permets de choisir si on veut un dataset avec 50% Good et 50% Bad (True) ou 90% Good et 10% Bad (False). Les deux ont autant de commentaires labelisé \"Good\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKjbIZRqSoVe",
        "colab": {}
      },
      "source": [
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, comments_df, vectorizer,\n",
        "                 balanced=False,binary=True,fine_grained=False,\n",
        "                 max_length=-1,length_out=False, preprocess=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            comments_df (pandas.DataFrame): the dataset\n",
        "            vectorizer (CommentsVectorizer): vectorizer instatiated from dataset\n",
        "        \"\"\"\n",
        "        self.comments_df = comments_df.copy()   # Je le modifie plus tard. Juste pour eviter un effet de bord.\n",
        "        if preprocess:\n",
        "          self.comments_df['comment_text'] = self.comments_df['comment_text'].apply(lambda txt: clean_text(txt, tokenize=False)) \n",
        "        self._vectorizer = vectorizer\n",
        "        self._binary = binary\n",
        "        self._length_out = length_out\n",
        "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
        "        if binary:\n",
        "          self._n_classes = 2\n",
        "        else:\n",
        "          self._n_classes = 7  # 6 or 7 ?\n",
        "        if fine_grained:\n",
        "          self.measure_len = lambda context: len(context)\n",
        "        else:\n",
        "          self.measure_len = lambda context: len(context.split(\" \"))\n",
        "\n",
        "        if max_length == -1:\n",
        "          self._max_seq_length = max(map(self.measure_len, comments_df.comment_text)) + 2\n",
        "        else:\n",
        "          self._max_seq_length = max_length +2\n",
        "        \n",
        "\n",
        "        self.train_df = self.comments_df[self.comments_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        if balanced:\n",
        "          self.samples_weights = np.zeros(self.train_size) \n",
        "          if binary:\n",
        "            self.samples_weights[self.train_df.clean == True] = 1. / 6. # 1. / self.train_df.loc[self.train_df.clean == True].shape[0] pour 50-50\n",
        "            self.samples_weights[self.train_df.clean == False] = 1. # 1. / self.train_df.loc[self.train_df.clean == False].shape[0] pour 50-50\n",
        "          else:\n",
        "            # This one is tricky. How to balance the multi-class case ?\n",
        "            self.samples_weights[self.train_df.clean == True] = 1. / 6. \n",
        "            self.samples_weights[self.train_df.clean == False] = 1. \n",
        "\n",
        "        \n",
        "        self.val_df = self.comments_df[self.comments_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.comments_df[self.comments_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.validation_size),\n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "        \"\"\"\n",
        "        class_counts = comments_df.clean.value_counts().to_dict()\n",
        "\n",
        "        def sort_key(item):\n",
        "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
        "        \n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, comments_csv,balanced=False,\n",
        "                                         binary=True,fine_grained=False,\n",
        "                                         max_length=-1,length_out=False, preprocess=False):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        comments_df = pd.read_csv(comments_csv)\n",
        "        train_comments_df = comments_df[comments_df.split=='train']\n",
        "        if fine_grained:\n",
        "          comment_vectorizer = CommentVectorizerChar.from_dataframe(train_comments_df)\n",
        "        else:\n",
        "          comment_vectorizer = CommentVectorizer.from_dataframe(train_comments_df)\n",
        "        return cls(comments_df,comment_vectorizer ,\n",
        "                   balanced,binary,fine_grained,\n",
        "                   max_length, length_out, preprocess)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \n",
        "        Used in the case in the vectorizer has been cached for re-use\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        comments_df = pd.read_csv(comments_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(comments_csv, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"a static method for loading the vectorizer from file\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameVectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return NameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"saves the vectorizer to disk using json\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        vect_length = min(self.measure_len(row.comment_text)+1,self._max_seq_length-1)\n",
        "\n",
        "        comments_vector = \\\n",
        "            self._vectorizer.vectorize(row.comment_text, self._max_seq_length)\n",
        "\n",
        "        if self._length_out:\n",
        "          comments_vector = np.concatenate([[vect_length], comments_vector])\n",
        "        \n",
        "        if row.clean:\n",
        "          category_index = 0.0\n",
        "        else:\n",
        "          category_index = 1.0\n",
        "\n",
        "        if self._binary:\n",
        "          target = category_index\n",
        "        else:\n",
        "          target = torch.tensor([category_index,\n",
        "                                 row.identity_hate,\n",
        "                                 row.insult,\n",
        "                                 row.obscene,\n",
        "                                 row.severe_toxic,\n",
        "                                 row.threat,\n",
        "                                 row.toxic])\n",
        "        return {'x_data': comments_vector,\n",
        "                'y_target': target}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cufyRV3uSoVi"
      },
      "source": [
        "## 4.a. batch generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cx4tkoOOSoVi"
      },
      "source": [
        "Ci-dessous une fonction pour génerer automatiquement des batchs lors de la phase d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LeVnPI5_SoVj",
        "colab": {}
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\", balanced=False): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    if balanced:\n",
        "      sampler = WeightedRandomSampler(\n",
        "      weights=dataset.samples_weights,\n",
        "      num_samples=dataset.train_size,\n",
        "      replacement=True)\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=sampler,\n",
        "                            drop_last=drop_last)\n",
        "    else:\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TvCkvAKsSoVl",
        "colab": {}
      },
      "source": [
        "def generate_batches_sorted(dataset, batch_size, shuffle=True, \n",
        "                            drop_last=True, device=\"cpu\"):\n",
        "    \"\"\"A generator function which wraps the PyTorch DataLoader.  The NMT Version \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        lengths = data_dict['x_data'][:,0].numpy()\n",
        "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
        "        \n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwaJYEdbksUN",
        "colab_type": "text"
      },
      "source": [
        "# 5. Helping functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q_AXzYj-C3E",
        "colab_type": "text"
      },
      "source": [
        "Ces fonctions permettent de mettre à jour l'état d'entrainement qui sauvegarde les loss et accuracy à chaque epoch pour le train et le val dataset (entre autre)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q37k5mV6kuZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"Handle the training state updates.\n",
        "\n",
        "    Components:\n",
        "     - Early Stopping: Prevent overfitting.\n",
        "     - Model Checkpoint: Model is saved if the model is better\n",
        "\n",
        "    :param args: main arguments\n",
        "    :param model: model to train\n",
        "    :param train_state: a dictionary representing the training state values\n",
        "    :returns:\n",
        "        a new train_state\n",
        "    \"\"\"\n",
        "\n",
        "    # Save one model at least\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "\n",
        "    # Save model if performance improved\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "\n",
        "        # If loss worsened\n",
        "        if loss_t >= train_state['early_stopping_best_val']:\n",
        "            # Update step\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # Loss decreased\n",
        "        else:\n",
        "            # Save the best model\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "\n",
        "            # Reset early stopping step\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # Stop early ?\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxacMTR0Uo5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "def clean_text(text, tokenize=True, lemmatize=False):\n",
        "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
        "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
        "    if tokenize:\n",
        "      tokens = re.split('\\W+', text_rc)    # tokenization\n",
        "      if lemmatize:\n",
        "        text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
        "      else:\n",
        "        text = [word for word in tokens if word not in stopword] # remove stopwords\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy39Z3SKlAm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSvo7qFp_Tbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_embedding_matrix(glove_filepath, words):\n",
        "    \"\"\"\n",
        "    Create embedding matrix for a specific set of words.\n",
        "    \n",
        "    Args:\n",
        "        glove_filepath (str): file path to the glove embeddigns\n",
        "        words (list): list of words in the dataset\n",
        "    \"\"\"\n",
        "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
        "    embedding_size = glove_embeddings.shape[1]\n",
        "    \n",
        "    final_embeddings = np.zeros((len(words), embedding_size))\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word in word_to_idx:\n",
        "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
        "        else:\n",
        "            embedding_i = torch.ones(1, embedding_size)\n",
        "            torch.nn.init.xavier_uniform_(embedding_i)\n",
        "            final_embeddings[i, :] = embedding_i\n",
        "\n",
        "    return final_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l6I-AlU_w3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove_from_file(glove_filepath):\n",
        "    \"\"\"\n",
        "    Load the GloVe embeddings \n",
        "    \n",
        "    Args:\n",
        "        glove_filepath (str): path to the glove embeddings file \n",
        "    Returns:\n",
        "        word_to_index (dict), embeddings (numpy.ndarary)\n",
        "    \"\"\"\n",
        "\n",
        "    word_to_index = {}\n",
        "    embeddings = []\n",
        "    with open(glove_filepath, \"r\") as fp:\n",
        "        for index, line in enumerate(fp):\n",
        "            line = line.split(\" \") # each line: word num1 num2 ...\n",
        "            word_to_index[line[0]] = index # word = line[0] \n",
        "            embedding_i = np.array([float(val) for val in line[1:]])\n",
        "            embeddings.append(embedding_i)\n",
        "    return word_to_index, np.stack(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hs-GrFg-NPz",
        "colab_type": "text"
      },
      "source": [
        "La fonction ci-dessous permets de calculer une accuracy, dans le cas ou il y a une seule output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCS9rGJ1DP0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(y_pred, y_target):\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w95r0Z5YvY4Y",
        "colab_type": "text"
      },
      "source": [
        "Fonction pour la generation des bi-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymqah48svfSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_bigrams(x):\n",
        "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "    for n_gram in n_grams:\n",
        "        x.append(' '.join(n_gram))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8qL5KJ1kc88",
        "colab_type": "text"
      },
      "source": [
        "# 6. Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctay4c5Rv0dl",
        "colab_type": "text"
      },
      "source": [
        "## 6.a. Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7REBTHJW-ULu",
        "colab_type": "text"
      },
      "source": [
        "Ci-dessous un premier modèle (MultiLayerPerceptron)\n",
        "\n",
        "+ 10 epochs, emb 50,hid 50, bs128,do0.15,lr-3 : 87%\n",
        "+ idem + glove : 89%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRc1mNW7kcRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None,length_out=False,\n",
        "                 padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(MLPClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "            \n",
        "        \n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self._length_out = length_out\n",
        "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, num_features)\n",
        "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,)\n",
        "        \"\"\"\n",
        "        if self._length_out:\n",
        "          x_in = x_in[:,1:]\n",
        "          \n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        features = x_embedded.sum(dim=1)\n",
        "        #features = F.dropout(features, p=self._dropout_p)\n",
        "        \n",
        "        # mlp classifier\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
        "        prediction = self.fc2(intermediate_vector).squeeze()\n",
        "\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVCfeomv6Dv",
        "colab_type": "text"
      },
      "source": [
        "## 6.b. Conv 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3tEPOdqxYO",
        "colab_type": "text"
      },
      "source": [
        "Le classifieur CNN suivant à l'air d'overfitté au fil des époques, cependant, il atteint 95% d'accuracy mais surtout 81% sur les commentaires \"méchants\"\n",
        "\n",
        "+ 20 epochs, emb 50,hid 50, 128chqnm bs128,do0.15,lr-3 : 90%\n",
        "+ idem + glove : 92%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOd26_gMMTV5",
        "colab": {}
      },
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 num_channels, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None,length_out=False,\n",
        "                 padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(CNNClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=embedding_size, \n",
        "                   out_channels=num_channels, kernel_size=3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self._length_out = length_out\n",
        "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        if self._length_out:\n",
        "          x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
        "        \n",
        "        features = self.convnet(x_embedded)\n",
        "\n",
        "        # average and remove the extra dimension\n",
        "        remaining_size = features.size(dim=2)\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
        "        features = F.dropout(features, p=self._dropout_p)\n",
        "        \n",
        "        # mlp classifier\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
        "        prediction = self.fc2(intermediate_vector).squeeze()\n",
        "\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dexgbb0evEqP",
        "colab_type": "text"
      },
      "source": [
        "## 6.c. Fast Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGe93CAyrSWM",
        "colab_type": "text"
      },
      "source": [
        "Modèle inspiré du papier https://arxiv.org/abs/1607.01759 \n",
        "En gros, c'est juste un MLP avec une phase de pre-processing du texte où on ajoute à la fin de chaque phrase les n-grams qui y apparaissent (bigrams ici)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjgjpnbEvKxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastText(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_size, hidden_dim, out_dim, padding_idx, length_out=False, pretrained_embeddings=None, dropout_p=0.0):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self._length_out = length_out\n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "        \n",
        "    def forward(self, x, apply_sigmoid=False):\n",
        "        \n",
        "        x_in = x.t()\n",
        "        if self._length_out:\n",
        "          x_in = x_in[1:, :]\n",
        "          \n",
        "        embedded = self.emb(x_in)                \n",
        "        \n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "                \n",
        "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "        prediction = F.relu(F.dropout(self.fc1(pooled), p=self._dropout_p))\n",
        "        prediction = self.fc2(prediction).squeeze()\n",
        "\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPgyv5a3v-fI",
        "colab_type": "text"
      },
      "source": [
        "## 6.d. GRU + Conv1D sur les variables d'état"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hfF8BYB9AL8",
        "colab_type": "text"
      },
      "source": [
        "90% Pour le classifieur suivant avec une courbe de loss bizarre mais 90% sur les mauvais commentaires !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yztvZBDWM2Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size,num_channels, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, length_out=False,\n",
        "                 padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(GRUClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=rnn_hidden_size*2, \n",
        "                   out_channels=num_channels, kernel_size=3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size),\n",
        "            nn.ELU()\n",
        "        )\n",
        "        self._dropout_p = dropout_p\n",
        "        self._length_out = length_out\n",
        "        self.fc1 = nn.Linear(num_channels, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        if self._length_out:\n",
        "          x_in = x_in[:,1:]\n",
        "          \n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "        y_out = y_out.permute(0, 2, 1)\n",
        "        features = self.convnet(y_out)\n",
        "        # average and remove the extra dimension\n",
        "        remaining_size = features.size(dim=2)\n",
        "\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
        "        features = F.dropout(features, p=self._dropout_p)\n",
        "\n",
        "        prediction = self.fc1(F.dropout(features, p=self._dropout_p)).squeeze()\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsxSmb53wC_A",
        "colab_type": "text"
      },
      "source": [
        "## 6.e. GRU + MLP sur les variable d'état finales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQu1nh1iMjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUClassifier2(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(GRUClassifier2, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=False)\n",
        "        \n",
        "        self._dropout_p = dropout_p\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:, 0]\n",
        "\n",
        "        x_in = x_in[:, 1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        features = []\n",
        "        for i in range(batchsize):\n",
        "          features.append(y_out[i,x_length[i],:])\n",
        "\n",
        "        features = torch.stack(features)\n",
        "\n",
        "        prediction = self.fc1(F.dropout(features, p=self._dropout_p)).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s0gK7u2XxDLM"
      },
      "source": [
        "## 6.f. LSTM + MLP sur les variable d'état finales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P-euScCnxDLO",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, padding_idx=0, \n",
        "                 kernel_size=3, dropout_rec=0.1, bidirectional=True, n_layers=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.LSTM(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          num_layers=n_layers, \n",
        "                          dropout=dropout_rec,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=bidirectional)\n",
        "        \n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "        if bidirectional:\n",
        "          self.fc1 = nn.Linear(2 * rnn_hidden_size, hidden_dim)\n",
        "        else:\n",
        "          self.fc1 = nn.Linear(rnn_hidden_size, hidden_dim)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:,0]\n",
        "\n",
        "        x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        features = []\n",
        "        for i in range(batchsize):\n",
        "          features.append(y_out[i,x_length[i],:])\n",
        "\n",
        "        features = torch.stack(features)\n",
        "\n",
        "        intermediate_vector = self.fc1(F.dropout(features, p=self._dropout_p))\n",
        "        prediction = self.fc2(F.dropout(intermediate_vector, p=self._dropout_p)).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9KPM-qi6OuD",
        "colab_type": "text"
      },
      "source": [
        "##6.g. LSTM with Attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkkA4bQL6Trg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMAttentionClassifier(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, num_embeddings, out_dim, rnn_hidden_size, pretrained_embeddings=None, padding_idx=0, dropout_rec=0.1, bidirectional=False, n_layers=1):\n",
        "    super(LSTMAttentionClassifier, self).__init__()\n",
        "    if pretrained_embeddings is None:\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "    else:\n",
        "      pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\t\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size=embedding_size, \n",
        "                        hidden_size=rnn_hidden_size,\n",
        "                        num_layers=n_layers, \n",
        "                        dropout=dropout_rec,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=bidirectional)\n",
        "  \n",
        "\n",
        "    if bidirectional:\n",
        "        self.fc = nn.Linear(2 * rnn_hidden_size, out_dim)\n",
        "    else:\n",
        "        self.fc = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\t\t\n",
        "  def attention_net(self, lstm_output, final_state):\n",
        "    \"\"\" \n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\t  \n",
        "\t\t\"\"\"\n",
        "\n",
        "\n",
        "    hidden = final_state.squeeze(0)\n",
        "    attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "    soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "    new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "    return new_hidden_state\n",
        "\n",
        "  def forward(self, input_sentences, apply_sigmoid=False):\n",
        "    \"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "    batchsize, seq_length = input_sentences.size()\n",
        "    x_length = input_sentences[:,0]\n",
        "\n",
        "    input_sentences = input_sentences[:,1:]\n",
        "    input_sentences = input_sentences.t()\n",
        "    x_embedded = self.emb(input_sentences)\n",
        "    x_embedded = x_embedded.permute(1, 0, 2)\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(x_embedded)  \n",
        "    attn_output = self.attention_net(output, final_hidden_state)\n",
        "    prediction = self.fc(attn_output).squeeze()\n",
        "\n",
        "    if apply_sigmoid:\n",
        "      prediction = F.sigmoid(prediction)\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwngiqd8kocK",
        "colab_type": "text"
      },
      "source": [
        "##6.h RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_73YV_pbktRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RCNNClassifier(nn.Module):\n",
        "  def __init__(self, embedding_size, num_embeddings, out_dim, rnn_hidden_size, pretrained_embeddings=None, padding_idx=0, dropout_rec=0.1, bidirectional=False, n_layers=1, dropout_p=0.5):\n",
        "    super(RCNNClassifier, self).__init__()\n",
        "    if pretrained_embeddings is None:\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                              num_embeddings=num_embeddings,\n",
        "                              padding_idx=padding_idx)        \n",
        "    else:\n",
        "      pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "\n",
        "    self._dropout_p = dropout_p\n",
        "    self.lstm = nn.LSTM(input_size=embedding_size, \n",
        "                        hidden_size=rnn_hidden_size,\n",
        "                        num_layers=n_layers, \n",
        "                        dropout=dropout_rec,\n",
        "                        batch_first=False, \n",
        "                        bidirectional=bidirectional)\n",
        "  \n",
        "    if bidirectional:\n",
        "      self.fc1 = nn.Linear(2 * rnn_hidden_size + embedding_size, rnn_hidden_size) \n",
        "    else:\n",
        "      self.fc1 = nn.Linear(rnn_hidden_size + embedding_size, rnn_hidden_size) \n",
        "    self.fc2 = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\t\t\n",
        "  def forward(self, input_sentence, apply_sigmoid=False):\n",
        "    \"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\t\n",
        "\t\t\n",
        "\t\tThe idea of the paper \"Recurrent Convolutional Neural Networks for Text Classification\" is that we pass the embedding vector\n",
        "\t\tof the text sequences through a bidirectional LSTM and then for each sequence, our final embedding vector is the concatenation of \n",
        "\t\tits own GloVe embedding and the left and right contextual embedding which in bidirectional LSTM is same as the corresponding hidden\n",
        "\t\tstate. This final embedding is passed through a linear layer which maps this long concatenated encoding vector back to the hidden_size\n",
        "\t\tvector. After this step, we use a max pooling layer across all sequences of texts. This converts any varying length text into a fixed\n",
        "\t\tdimension tensor of size (batch_size, hidden_size) and finally we map this to the output layer.\n",
        "\t\t\"\"\"\n",
        "    \n",
        "    x_embedded = self.emb(input_sentence) \n",
        "    x_embedded = x_embedded.permute(1, 0, 2) \n",
        "\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(x_embedded)\n",
        "    final_encoding = torch.cat((output, x_embedded), 2).permute(1, 0, 2)\n",
        "    y = self.fc1(final_encoding) \n",
        "    y = y.permute(0, 2, 1) \n",
        "    y = F.max_pool1d(y, y.size()[2]) \n",
        "    y = y.squeeze(2)\n",
        "    prediction = self.fc2(y).squeeze()\n",
        "    if apply_sigmoid:\n",
        "      prediction = F.sigmoid(prediction)\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQJv4-62_Ixn",
        "colab_type": "text"
      },
      "source": [
        "## 6.i. Encoder + Generator (Rationales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxHnE4fE_NpE",
        "colab_type": "text"
      },
      "source": [
        "In the paper Rationalizing Neural Predictions (https://arxiv.org/pdf/1606.04155.pdf), the authors use the encoder described in cell 6.d. of this notebook and a generator to generate rationales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJS-7zbQuMtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myEncoder(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(myEncoder, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=False)\n",
        "        \n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, z=None,apply_sigmoid=False,train=True):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:,0]\n",
        "\n",
        "        x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "        \n",
        "        if (z is not None):\n",
        "          x_in = x_in * z\n",
        "        \n",
        "\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        features = []\n",
        "        for i in range(batchsize):\n",
        "          features.append(y_out[i,x_length[i],:])\n",
        "\n",
        "        features = torch.stack(features)\n",
        "        if train:\n",
        "          features = F.dropout(features, p=self._dropout_p)\n",
        "        prediction = self.fc1(features).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfGqgMEp_NL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myGenerator(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, num_layers,\n",
        "                 size_s, pretrained_embeddings=None, \n",
        "                 padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(myGenerator, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.LSTM(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.s_size = size_s\n",
        "        self.rnn_cond = nn.LSTM(2 * rnn_hidden_size + 1, size_s, 1)\n",
        "        self.fc1 = nn.Linear(2 * rnn_hidden_size+ size_s, 1)\n",
        "\n",
        "    def logProb(self, x_in, z, init_hidden, use_cuda=True):\n",
        "        lstm_i2h_h0, lstm_i2h_c0, lstm_h2s_h0, lstm_h2s_c0 = init_hidden\n",
        "\n",
        "\n",
        "        x_length = x_in[:,0] - 1\n",
        "        x_in = x_in[:,1:]\n",
        "        batch_size, seq_len = x_in.size()\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        x_embedded = torch.transpose(x_embedded, 1, 0) # (seqlength,batchsize,embsize)\n",
        "\n",
        "        hidden_features, _ = self.rnn(x_embedded, (lstm_i2h_h0, lstm_i2h_c0))\n",
        "\n",
        "\n",
        "        z_transformed = torch.transpose(z, 1, 0)\n",
        "        z_transformed_unsqueezed = torch.unsqueeze(z_transformed, 2)  \n",
        " \n",
        "        s_h2s, (_, _) = self.rnn_cond(torch.cat((hidden_features, z_transformed_unsqueezed), dim=2), (lstm_h2s_h0, lstm_h2s_c0))\n",
        "\n",
        "        if use_cuda:\n",
        "          log_p_z = Variable(torch.zeros((batch_size)).cuda())\n",
        "        else:\n",
        "          log_p_z = Variable(torch.zeros((batch_size)))\n",
        "\n",
        "        for i in range(seq_len):\n",
        "          if (i == 0):\n",
        "            cur_p_z = self.fc1(torch.cat((hidden_i2h[i], lstm_h2s_h0[0]), dim=1))\n",
        "          else:\n",
        "            cur_p_z = self.fc1(torch.cat((hidden_i2h[i], s_h2s[i - 1]), dim=1))\n",
        "\n",
        "          cur_p_z = F.sigmoid(torch.squeeze(cur_p_z, 1))\n",
        "          cur_p_z = z_transformed[i] * cur_p_z + (1 - z_transformed[i]) * (1 - cur_p_z)\n",
        "          cur_log_p_z = torch.log(cur_p_z)\n",
        "\n",
        "          log_p_z = log_p_z + cur_log_p_z\n",
        "\n",
        "        return log_p_z\n",
        "\n",
        "\n",
        "    def sample(self, x_in, init_hidden, use_cuda=True):\n",
        "        lstm_i2h_h0, lstm_i2h_c0, lstm_h2s_h0, lstm_h2s_c0 = init_hidden\n",
        "\n",
        "        x_length = x_in[:,0] - 1\n",
        "        x_in = x_in[:,1:]\n",
        "        batch_size, seq_len = x_in.size()\n",
        "        x = self.emb(x_in)\n",
        "        #x = torch.transpose(self.emb(x_in), 1, 0)\n",
        "\n",
        "        hidden_i2h , (_, _) = self.rnn(x, (lstm_i2h_h0, lstm_i2h_c0))\n",
        "\n",
        "        hidden_i2h = torch.transpose(hidden_i2h, 0,1)  \n",
        "        if use_cuda:\n",
        "          z = torch.zeros((seq_len, batch_size)).cuda()\n",
        "        else:\n",
        "          z = torch.zeros((seq_len, batch_size))\n",
        "\n",
        "        z = Variable(z)\n",
        "\n",
        "        s_h2s_h = lstm_h2s_h0\n",
        "        s_h2s_c = lstm_h2s_c0\n",
        "\n",
        "        for i in range(seq_len):\n",
        "          cur_p_z = self.fc1(torch.cat((hidden_i2h[i], s_h2s_h[0]), dim=1))\n",
        "\n",
        "          cur_p_z = F.sigmoid(torch.squeeze(cur_p_z, 1))\n",
        "          m = Bernoulli(cur_p_z)\n",
        "          z[i] = m.sample()\n",
        "\n",
        "\n",
        "          cat_hidden_z = torch.unsqueeze(torch.cat((hidden_i2h[i], torch.unsqueeze(z[i], 1)), dim=1), 0)\n",
        "          _, (s_h2s_h, s_h2s_c) = self.rnn_cond(cat_hidden_z, (s_h2s_h, s_h2s_c))\n",
        "\n",
        "\n",
        "        return torch.transpose(z, 1, 0)\n",
        "    \"\"\"\n",
        "     def forward(self, x_in):\n",
        "        The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "       \n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:,0] - 1\n",
        "        x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        y_out = y_out[:,1:] # On enlève le charactere de début\n",
        "\n",
        "        features = pack_padded_sequence(y_out,x_length.detach().cpu().numpy(),batch_first=True)\n",
        "        #Utiliser le packpading\n",
        "\n",
        "        intermediate_vector = self.fc1(features.data).squeeze()\n",
        "        intermediate_vector = F.sigmoid(intermediate_vector)\n",
        "        features = PackedSequence(intermediate_vector, features.batch_sizes, \n",
        "                 features.sorted_indices, features.unsorted_indices)\n",
        "        prediction, _ = pad_packed_sequence(features, batch_first=True)\n",
        "\n",
        "\n",
        "        return prediction\n",
        "\"\"\"\n",
        "    def initHidden(self, batch_size, use_cuda=True):\n",
        "      if use_cuda:\n",
        "        lstm_i2h_h0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size).cuda())\n",
        "        lstm_i2h_c0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size).cuda())\n",
        "\n",
        "        lstm_h2s_h0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size).cuda())\n",
        "        lstm_h2s_c0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size).cuda())\n",
        "\n",
        "      else:\n",
        "        lstm_i2h_h0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size))\n",
        "        lstm_i2h_c0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size))\n",
        "\n",
        "        lstm_h2s_h0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size))\n",
        "        lstm_h2s_c0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size))\n",
        "\n",
        "      return lstm_i2h_h0, lstm_i2h_c0, lstm_h2s_h0, lstm_h2s_c0\n",
        "\n",
        "    def loss(self, z):\n",
        "\n",
        "      length_cost = torch.sum(z, dim=1)\n",
        "      l_padded_mask =  torch.cat([z[:,0].unsqueeze(1), z] , dim=1)\n",
        "      r_padded_mask =  torch.cat([z, z[:,-1].unsqueeze(1)] , dim=1)\n",
        "      continuity_cost = torch.sum(torch.abs(r_padded_mask - l_padded_mask), dim=1)\n",
        "      return length_cost, continuity_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIT36j5NqHSv",
        "colab_type": "text"
      },
      "source": [
        "# 7. Préparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE9fsZC5-fTR",
        "colab_type": "text"
      },
      "source": [
        "les différents paramètres du modèle, du dataset, et d'entrainements à un seul endroit !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbjBLz6k98G",
        "colab_type": "code",
        "outputId": "53711932-12ef-4977-e151-061944aa9339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    frequency_cutoff=5,\n",
        "    model_state_file='model.pth',\n",
        "    comments_csv='toxic_comments.csv',\n",
        "    save_dir='model_storage',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    glove_filepath='glove.twitter.27B.50d.txt',\n",
        "    balanced=False,\n",
        "    binary=True, #Est-ce que l'output est binaire (good/bad) ou multilabel ?\n",
        "    fine_grained = False,\n",
        "    max_length=800,\n",
        "    length_out=True,\n",
        "    preprocess=True,\n",
        "    # Model hyper parameters\n",
        "    model=\"MLP1\",\n",
        "    bigrams=False, # A mettre a True seulement pour FastText\n",
        "    use_glove=False,\n",
        "    embedding_size=128, \n",
        "    hidden_dim=100,\n",
        "    num_channels=128,\n",
        "    rnn_hidden_size=100,\n",
        "    kernel_size=3,\n",
        "    out_dim=1, #Nombre de labels = 1 ou 7 (6 particulier + 1 general)\n",
        "    # For the generator\n",
        "    num_samples = 20,\n",
        "    gen_num_layers = 1,\n",
        "    gen_s_size = 30,\n",
        "    # Training hyper parameters\n",
        "    batch_size=64,\n",
        "    early_stopping_criteria=3,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=5,\n",
        "    seed=1337, \n",
        "    dropout_p=0.3, \n",
        "    dropout_rec=0.1,  # Droupout LSTM\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.binary:\n",
        "  assert(args.out_dim == 1)\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tmodel_storage/vectorizer.json\n",
            "\tmodel_storage/model.pth\n",
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4K1pWvH-qUO",
        "colab_type": "text"
      },
      "source": [
        "Cette cellulle créé tout ce qu'on a défini au dessus !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvjCGCAclife",
        "colab_type": "code",
        "outputId": "173f2f4d-c4af-4421-fa75-d4f74ec80def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Loading dataset and creating vectorizer\")\n",
        "# create dataset and vectorizer\n",
        "dataset = CommentDataset.load_dataset_and_make_vectorizer(args.comments_csv,\n",
        "                                                          args.balanced,\n",
        "                                                          args.binary,\n",
        "                                                          args.fine_grained,\n",
        "                                                          args.max_length,\n",
        "                                                          args.length_out, \n",
        "                                                          args.preprocess)\n",
        "dataset.save_vectorizer(args.vectorizer_file)    \n",
        "vectorizer = dataset.get_vectorizer()\n",
        "# Use GloVe or randomly initialized embeddings\n",
        "if args.use_glove:\n",
        "    words = vectorizer.comment_vocab._token_to_idx.keys()\n",
        "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
        "                                       words=words)\n",
        "    print(\"Using pre-trained embeddings\")\n",
        "else:\n",
        "    print(\"Not using pre-trained embeddings\")\n",
        "    embeddings = None\n",
        "\n",
        "if args.model == \"MLP1\":\n",
        "  classifier = MLPClassifier(embedding_size=args.embedding_size, \n",
        "                              num_embeddings=len(vectorizer.comment_vocab),\n",
        "                              hidden_dim=args.hidden_dim, \n",
        "                              dropout_p=args.dropout_p,\n",
        "                              out_dim =args.out_dim,\n",
        "                              pretrained_embeddings=embeddings,\n",
        "                              length_out=args.length_out,\n",
        "                              padding_idx=0)\n",
        "  \n",
        "elif args.model == \"FastText\":\n",
        "  classifier = FastText(embedding_size=args.embedding_size, \n",
        "                        num_embeddings=len(vectorizer.comment_vocab),\n",
        "                        hidden_dim=args.hidden_dim, \n",
        "                        dropout_p=args.dropout_p,\n",
        "                        out_dim =args.out_dim,\n",
        "                        length_out=args.length_out,\n",
        "                        padding_idx=0)\n",
        "\n",
        "elif args.model == \"CNN1\":\n",
        "\n",
        "  classifier = CNNClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            num_channels=args.num_channels, \n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            length_out=args.length_out,\n",
        "                            padding_idx=0)\n",
        "\n",
        "\n",
        "elif args.model == \"GRU1\":\n",
        "\n",
        "  classifier = GRUClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            num_channels=args.num_channels,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            length_out=args.length_out,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"GRU2\":\n",
        "  assert(args.length_out)\n",
        "  classifier = GRUClassifier2(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"LSTM\":\n",
        "\n",
        "  classifier = LSTMClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"LSTMAttention\":\n",
        "  classifier = LSTMAttentionClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            dropout_rec=args.dropout_rec,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "  \n",
        "elif args.model == \"RCNN\":\n",
        "  classifier = RCNNClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            dropout_rec=args.dropout_rec,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"RAT\":\n",
        "  assert(args.length_out)\n",
        "  classifier = myEncoder(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "  \n",
        "  generator = myGenerator(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            num_layers = args.gen_num_layers,\n",
        "                            size_s = args.gen_s_size,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            padding_idx=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset and creating vectorizer\n",
            "Not using pre-trained embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myQKG6pQvEKT",
        "colab_type": "code",
        "outputId": "a436d4b7-4489-4be4-8467-0adba74439f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model): \n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
        "\n",
        "count_parameters(classifier)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4471369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCvk7zeWJdDT",
        "colab_type": "code",
        "outputId": "93f0df36-9b69-4908-e9c0-e45f72eb6c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "dataset.train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>insult</th>\n",
              "      <th>obscene</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>split</th>\n",
              "      <th>threat</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>i am only going to fucking say this once those...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>2010 's arthur rubin i want to thank but i lik...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>anonymous you gay afambro fuck you</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>putting stupid comments in articles will resul...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>boobies tatas taters hooters funbags tits hoot...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   clean                                       comment_text  ...  threat  toxic\n",
              "0  False  i am only going to fucking say this once those...  ...       0      1\n",
              "1  False  2010 's arthur rubin i want to thank but i lik...  ...       0      1\n",
              "2  False                 anonymous you gay afambro fuck you  ...       0      1\n",
              "3  False  putting stupid comments in articles will resul...  ...       0      1\n",
              "4  False  boobies tatas taters hooters funbags tits hoot...  ...       0      1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMgQOdc1cKIe",
        "colab_type": "code",
        "outputId": "d19e1966-c0e5-4edd-8afd-e66186e89180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "dataset.set_split('train')\n",
        "batch_generator = generate_batches_sorted(dataset, \n",
        "                                  batch_size=args.batch_size, \n",
        "                                  device=args.device)\n",
        "batch_index, batch_dict = next(enumerate(batch_generator))\n",
        "\n",
        "batch_dict['x_data'].size()\n",
        "\n",
        "print(batch_dict['x_data'][-5:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  11,    2,    1,  ...,    0,    0,    0],\n",
            "        [  11,    2, 1029,  ...,    0,    0,    0],\n",
            "        [  10,    2,  844,  ...,    0,    0,    0],\n",
            "        [   8,    2,  541,  ...,    0,    0,    0],\n",
            "        [   6,    2, 1823,  ...,    0,    0,    0]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO1rzhQkoVUO",
        "colab_type": "text"
      },
      "source": [
        "# 8. Simple Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-t4Y9C2m-5D",
        "colab_type": "text"
      },
      "source": [
        "## 8.a. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMN0KZgzoYT3",
        "colab_type": "text"
      },
      "source": [
        "Un peu de pre-processing pour les methodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHjoGF9olz8",
        "colab_type": "code",
        "outputId": "c39a5da3-f491-45dd-b142-2fab464d8df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "X_train = dataset.train_df['comment_text']\n",
        "X_val = dataset.val_df['comment_text']\n",
        "X_test = dataset.test_df['comment_text']\n",
        "\n",
        "if args.binary:\n",
        "  y_train = dataset.train_df['clean']\n",
        "  y_val = dataset.val_df['clean']\n",
        "  y_test = dataset.test_df['clean']\n",
        "else:\n",
        "  # TODO: Complete the multi-class case\n",
        "  y_train = dataset.train_df['clean', 'identity_hate', 'insult', 'obscene', 'severe_toxic', 'severe_toxic', 'toxic']\n",
        "\n",
        "print(\"Number of clean comments: \", len(y_train[y_train == True]))\n",
        "print(\"Number of toxic comments: \", len(y_train[y_train == False]))\n",
        "\n",
        "print(\"Before cleaning:\", X_train[0])\n",
        "print(\"After training:\", clean_text(X_train[0]))\n",
        "\n",
        "if args.binary:\n",
        "  tmp_y_train = y_train[:]\n",
        "  tmp_y_train[y_train == False] = 1.0\n",
        "  tmp_y_train[y_train == True] = 0.0\n",
        "  y_train = tmp_y_train[:]\n",
        "\n",
        "  tmp_y_val = y_val[:]\n",
        "  tmp_y_val[y_val == False] = 1.0\n",
        "  tmp_y_val[y_val == True] = 0.0\n",
        "  y_val = tmp_y_val[:]\n",
        "\n",
        "  tmp_y_test = y_test[:]\n",
        "  tmp_y_test[y_test == False] = 1.0\n",
        "  tmp_y_test[y_test == True] = 0.0\n",
        "  y_test = tmp_y_test[:]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of clean comments:  100336\n",
            "Number of toxic comments:  11357\n",
            "Before cleaning: i am only going to fucking say this once those bay murphy forums are biased if you do not believe me check them yourself one more time , and i delete the whole fucking entry \n",
            "After training: ['going', 'fucking', 'say', 'bay', 'murphy', 'forums', 'biased', 'believe', 'check', 'one', 'time', 'delete', 'whole', 'fucking', 'entry', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY7yN7--eao6",
        "colab_type": "text"
      },
      "source": [
        "## 8.b. Naive Bayes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cwGM1fbe860",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes en utilisant TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tnObz3aeebT",
        "colab_type": "code",
        "outputId": "4757ea48-f5b4-41f7-91c2-e4f36679993b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB()),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"Test ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96     14333\n",
            "         1.0       1.00      0.20      0.34      1622\n",
            "\n",
            "    accuracy                           0.92     15955\n",
            "   macro avg       0.96      0.60      0.65     15955\n",
            "weighted avg       0.93      0.92      0.89     15955\n",
            "\n",
            "Val accuracy:  0.9190586613186262\n",
            "Test accuracy:  0.9188342212472579\n",
            "Test ROC-AUC:  0.8826715538276075\n",
            "Test F1 score:  0.33623782675551006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbwyYlS7JEAq",
        "colab_type": "text"
      },
      "source": [
        "## 8.c. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnMdoSU2syuP",
        "colab_type": "text"
      },
      "source": [
        "Regression logistique avec TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdALaAnTJGyk",
        "colab_type": "code",
        "outputId": "11d02c67-e2f0-4a4f-c9fc-b93419e0a4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression()),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      1.00      0.97     14333\n",
            "         1.0       0.93      0.58      0.71      1622\n",
            "\n",
            "    accuracy                           0.95     15955\n",
            "   macro avg       0.94      0.79      0.84     15955\n",
            "weighted avg       0.95      0.95      0.95     15955\n",
            "\n",
            "Val accuracy:  0.9555339684131361\n",
            "Test accuracy:  0.9528047633970542\n",
            "ROC-AUC:  0.9734172337159562\n",
            "Test F1 score:  0.7131428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6PELxXfoKJv",
        "colab_type": "text"
      },
      "source": [
        "## 8.d. Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHrTl_nHs2dk",
        "colab_type": "text"
      },
      "source": [
        "Gradient Boosting avec TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUtM9UnWoNpO",
        "colab_type": "code",
        "outputId": "6438f121-4537-4f3a-c4d5-62a8d31c9233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', GradientBoostingClassifier(n_estimators=200)),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      1.00      0.97     14333\n",
            "         1.0       0.93      0.49      0.64      1622\n",
            "\n",
            "    accuracy                           0.94     15955\n",
            "   macro avg       0.94      0.74      0.81     15955\n",
            "weighted avg       0.94      0.94      0.94     15955\n",
            "\n",
            "Val accuracy:  0.9466031586863876\n",
            "Test accuracy:  0.9441554371670323\n",
            "ROC-AUC:  0.9290666912249186\n",
            "Test F1 score:  0.6405808793868495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hHIrRn7v_Jq",
        "colab_type": "text"
      },
      "source": [
        "## 8.e. SVM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dy6g968wFtN",
        "colab_type": "text"
      },
      "source": [
        "SVM avec TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMfDWQfQwHa8",
        "colab_type": "code",
        "outputId": "89c51468-d347-476d-f68d-45962897afec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LinearSVC()),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98     14333\n",
            "         1.0       0.87      0.70      0.77      1622\n",
            "\n",
            "    accuracy                           0.96     15955\n",
            "   macro avg       0.92      0.84      0.87     15955\n",
            "weighted avg       0.96      0.96      0.96     15955\n",
            "\n",
            "Val accuracy:  0.960767109551266\n",
            "Test accuracy:  0.9583829520526481\n",
            "ROC-AUC:  0.9290666912249186\n",
            "Test F1 score:  0.7727583846680356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9-1sadLxALq",
        "colab_type": "text"
      },
      "source": [
        "## 8.e. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmWRDNpnxJZ1",
        "colab_type": "text"
      },
      "source": [
        "Random Forest avec TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9IX-04NxDw-",
        "colab_type": "code",
        "outputId": "05cace89-2e1a-4247-e5e6-d4c26ca70768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', RandomForestClassifier(n_estimators=100)),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.99      0.97     14333\n",
            "         1.0       0.92      0.56      0.69      1622\n",
            "\n",
            "    accuracy                           0.95     15955\n",
            "   macro avg       0.94      0.78      0.83     15955\n",
            "weighted avg       0.95      0.95      0.94     15955\n",
            "\n",
            "Val accuracy:  0.9529957382802707\n",
            "Test accuracy:  0.9501723597618301\n",
            "ROC-AUC:  0.9673966194092376\n",
            "Test F1 score:  0.6945831732616212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IPi6U8B9q0w",
        "colab_type": "text"
      },
      "source": [
        "# 9. Transformers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSvTj1hMnRiq",
        "colab_type": "text"
      },
      "source": [
        "La bibliothèque Simple Transformers rend l'utilisation des transformers ridiculement simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2MiYJRI9slg",
        "colab_type": "code",
        "outputId": "5fb586fc-368d-4828-a73a-ec96cc377779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951,
          "referenced_widgets": [
            "30fec703e61144859f654749ddcc4c6b",
            "b15e5b0f733b4d35b2facfcf6b619156",
            "6962ac9a36db4d9ea00075da1a125238",
            "e786eea093be49e591cdbb1f54eb4917",
            "2598972b96a44e83a941fc5f153dabb7",
            "9e785c3059054815ba5764c70061a50a",
            "4c80a1a59f87462a9788b2162e600cca",
            "aaa8682b67824850bb875b545aaa8a47",
            "0db7815de22d4dff8c26e4bfc3794b72",
            "6b7ca43e8bf741e7aa9f5e6a5b08af44",
            "484ef53eb0d0475499b3b2f998790906",
            "794280c984124fc3a1db9e3722291328",
            "a2aa728c29ab4f9faa6e1da97f39186d",
            "3133a8b4e7cc42c89ee5609f0c7d4ea0",
            "32777a2f78274669993200a77b040741",
            "0f49cdcccca443be9b7026e0e6c8967a",
            "9c1255123c8f4870aa0debb80bc70479",
            "ff63d28a49494880adeb6a16f7342261",
            "bffbc96b21a14d22989fc74e7eb1aab3",
            "2912346373524603bb7ca7c23fa1c64b",
            "d0c256c8a865499093924a2d04bb6965",
            "f08b65271a5c452aa985cfdc39a37e81",
            "5996b7413ded4f6d9062d22654c1dc51",
            "f78cbced2ec34931a024ed5b9ea87e73",
            "e8525cd3bcf74ff1a823620bd413c280",
            "362e486d2ac74364bf94b86b4aeb111e",
            "9d101245f02f4e25bd295a222415e9a6",
            "d729e182617b4cfc9de14475b499d0de",
            "7a2c15bd35254f87b379d1782e8d645f",
            "4c3e4ded203c4acf83f71fc293710f41",
            "de4603ffd8884219adb1829ff31b1388",
            "ce29901d1e7b48c59833565e5473a4d0",
            "de63465a1f3946efb0c15327df4c8259",
            "973395fffcd7476ea0042404e6135177",
            "16095192b7eb446ba8f52fe2dc615676",
            "261009b7f49e46d286a1476b960eb86b",
            "ce7f3d27e4db41719740dead54c5266d",
            "e096a6232e2243439433275eba465992",
            "ed991a31556848648e260f2c4adec9a7",
            "a69fd58a752d41e28b30da0b4bd65632",
            "e863d88138b24749bebe0a86c895d207",
            "d64f217d2fc649f1b93535b4101eeccc",
            "1408647bf6eb4dfeb8b4f801d38957b3",
            "f22c5303917b4cd7889faa33d6748d79",
            "beba27d3ff184c619cf8934563f81427",
            "23a21bd87eae44148d184e236e7e365d",
            "f4ea27f2b865402382a7af054827c8dc",
            "49f77211812b4b2f8232611a426b4c8e",
            "f47e86548581402ab45fcdb731de7de3",
            "14d81e67ee234c5688b39ac39ccf355a",
            "5bb135e8f314484cb33cbd96268f545a",
            "bcaf2bccbe31463e9800c3b49c8e9a77",
            "bddf82c85b4046b091c7c30bfb7ae6ef",
            "366162efcfeb4d7aa9eeeeecff5b584d",
            "b1d8ef3c45014642ad29ec4b86c4b994",
            "c3835607b10642fc8ae9353da920f89d",
            "811be12291b84b29b2d930dbeb39a9a8",
            "886a6e05a888415787e7a332705e35f0",
            "c8c14dc74cb14ffbb8f8aa25dad0dc7a",
            "51ad4499b056437c83428676bdd16e6a",
            "1c7650139ab845cda9efacd215417267",
            "d3a560b7434d48628da61a3a6bcb3ecd",
            "51a01ace02a74702b5b8ff5eba921412",
            "46ffb60078e745c2a562016928ca5f54"
          ]
        }
      },
      "source": [
        "train_df = dataset.train_df.copy()\n",
        "test_df = dataset.test_df.copy()\n",
        "\n",
        "if args.binary:\n",
        "  train_df['labels'] = y_train\n",
        "  test_df['labels'] = y_test\n",
        "\n",
        "else:\n",
        "  train_df['labels'] = list(zip(train_df.toxic.tolist(), train_df.severe_toxic.tolist(), train_df.obscene.tolist(), train_df.threat.tolist(), train_df.insult.tolist(), train_df.identity_hate.tolist()))\n",
        "\n",
        "train_df['text'] = train_df['comment_text'].apply(lambda x: x.replace('\\n', ' '))\n",
        "test_df['text'] = test_df['comment_text'].apply(lambda x: x.replace('\\n', ' '))\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "if args.binary:\n",
        "  model = ClassificationModel('roberta', 'roberta-base', args={'train_batch_size':64, 'gradient_accumulation_steps':16, 'learning_rate': 1e-4, 'num_train_epochs': 2, 'max_seq_length': 200})\n",
        "else:\n",
        "  model = MultiLabelClassificationModel('roberta', 'roberta-base', num_labels=2, args={'train_batch_size':64, 'gradient_accumulation_steps':16, 'learning_rate': 1e-4, 'num_train_epochs': 2, 'max_seq_length': 200})\n",
        "\n",
        "model.train_model(train_df)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30fec703e61144859f654749ddcc4c6b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0db7815de22d4dff8c26e4bfc3794b72",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c1255123c8f4870aa0debb80bc70479",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8525cd3bcf74ff1a823620bd413c280",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de63465a1f3946efb0c15327df4c8259",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=111693.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e863d88138b24749bebe0a86c895d207",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f47e86548581402ab45fcdb731de7de3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1746.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.596093"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.066295"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.263926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "811be12291b84b29b2d930dbeb39a9a8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1746.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.084785\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gye4RJtTIO1",
        "colab_type": "code",
        "outputId": "ca2c727e-9ac8-4659-af4c-36fe2eca3380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "dc0e198907bf4e0b9b121a8b1de3b90e",
            "7d02eb10f6aa437ab16d6a2690c72fd6",
            "712acb06c6a143b3aee1156f4c8863bb",
            "74dd22172f324884825b2cebe492cd8d",
            "064906b68d004efb8294447821e17897",
            "1820c5c7f7dd43aea4060d91f52fc4e9",
            "cdb79665d27a4c9abdaa827f86d2f288",
            "9dbd87b817554d6bb09cbcb92f784ffe",
            "027df15538d3418b9a89fe88fc2b59cb",
            "e3a05fcefa884bcc99ca4824984af81a",
            "b4e9c639e790498ab229a35862b9972b",
            "c9e4ab98cc23477a993019603144c08d",
            "9eaed76c991c4f1ab7715ed4b37c2535",
            "b3f105d7dabc498a8df31245bd8d61e1",
            "07426e0df8664bc0ad84a5e72a486c84",
            "638744826de24608a65e65a7111b52eb"
          ]
        }
      },
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(test_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc0e198907bf4e0b9b121a8b1de3b90e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=15955.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "027df15538d3418b9a89fe88fc2b59cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1995.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqC86jI1XxlW",
        "colab_type": "code",
        "outputId": "480b8aef-e25e-46f4-ef52-8d5852c50fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'mcc': 0.823447765840835, 'tp': 1397, 'tn': 14032, 'fp': 301, 'fn': 225, 'eval_loss': 0.08132693760451816}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7WUUi6MqLJa",
        "colab_type": "text"
      },
      "source": [
        "# 10. Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiDI0qGJ-wc8",
        "colab_type": "text"
      },
      "source": [
        "On initialize tout ça et on créé des jolies barre de progression :D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FQSjlSemqoc",
        "colab_type": "code",
        "outputId": "75189d8b-ba24-4083-dd98-751e499942da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "1b2c8648e2ca4787b8d651e407b100c0",
            "6a65f1947f274444b5c95c5284209b1d",
            "3946e1ac30da49b58f4af2d92f25b335",
            "b70cfab9d7214d36aa540a6b3cb2d301",
            "d4616c0248df436f80aaf6b01a3d6580",
            "30bdadc3bf6741a99f4fa75080113150",
            "5644a3fe6830451e9a9093a35e923048",
            "f703fd0f20d14be6a31bff104de8fd64",
            "d55e7907ea944c44bca656f76a441e9c",
            "9bd5849b92cf4d1e921fcfefc1b304f6",
            "fbe5002300a54aaab9ce72d6bb3550f0",
            "37b0b358cdf7484bbf0bee91448a9a44",
            "00b299ce581e41ef89ed36fcb02ba014",
            "d6c268b9a1cf4bfe97988c43d3376b12",
            "90c38f75d56f476695b3b3b87dbfd108",
            "6cf1d0e0b25f47de8b29226beaa1829b",
            "e3940f5ecd094121bef4645b9470a97f",
            "32c9a19f3ca1489fbcc53632e8081bd9",
            "061cb04b066d4f5598004174fdf3aa5f",
            "19fc61a6a031421c816f7bf7a6840ce3",
            "042e32ae11c54983bf6c86efc643a749",
            "fc130ca7358741dbbc40542652dda1d0",
            "7aae411207b74d6bbd72d32f5da4cccd",
            "7ef9fe89529c4a24b07521603f4eca46"
          ]
        }
      },
      "source": [
        "classifier = classifier.to(args.device)\n",
        "    \n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=5)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b2c8648e2ca4787b8d651e407b100c0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='training routine', max=5.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d55e7907ea944c44bca656f76a441e9c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=train', max=1745.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3940f5ecd094121bef4645b9470a97f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=val', max=498.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJbE7oU-1Gu",
        "colab_type": "text"
      },
      "source": [
        "Et c'est partit pour l'entrainement !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3TDWBJSq5S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "\n",
        "            # --------------------------------------\n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = classifier(batch_dict['x_data'])\n",
        "            y_target = batch_dict['y_target']\n",
        "            if not(args.binary):\n",
        "              y_pred = y_pred.view(-1)\n",
        "              y_target = y_target.view(-1)\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, y_target.float())\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, y_target)\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "              # compute the output\n",
        "              y_pred = classifier(batch_dict['x_data'])\n",
        "              y_target = batch_dict['y_target']\n",
        "              if not(args.binary):\n",
        "                y_pred = y_pred.view(-1)\n",
        "                y_target = y_target.view(-1)\n",
        "\n",
        "              # step 3. compute the loss\n",
        "              loss = loss_func(y_pred, y_target.float())\n",
        "              loss_t = loss.item()\n",
        "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "              # compute the accuracy\n",
        "              acc_t = compute_accuracy(y_pred, y_target)\n",
        "              running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "              val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                              epoch=epoch_index)\n",
        "              val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier,\n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_7cGsBW6tFz",
        "colab_type": "text"
      },
      "source": [
        "# 11. Evolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBMXySg1-4cq",
        "colab_type": "text"
      },
      "source": [
        "Une petite étude de l'évolution de la loss et de l'accuracy au cours de epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KrvmFPt6uKf",
        "colab_type": "code",
        "outputId": "263d4723-616c-4750-96fb-5a34c332d141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        }
      },
      "source": [
        "train_loss = train_state['train_loss']\n",
        "train_acc = train_state['train_acc']\n",
        "val_loss = train_state['val_loss']\n",
        "val_acc = train_state['val_acc']\n",
        "\n",
        "plt.plot(train_loss,label=\"train\")\n",
        "plt.plot(val_loss,label=\"val\")\n",
        "plt.legend()\n",
        "plt.title(\"Evolution of the loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_acc,label=\"train\")\n",
        "plt.plot(val_acc,label=\"val\")\n",
        "plt.legend()\n",
        "plt.title(\"Evolution of the accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL9aM0HQo99",
        "colab_type": "text"
      },
      "source": [
        "On calcule l'AUC. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZ22QabQoQA",
        "colab_type": "code",
        "outputId": "b67084b3-c03b-49e8-9306-278624ddf0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "classifier.eval().cuda()\n",
        "n_chunks = 8\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                  batch_size=int(dataset.test_size / n_chunks), \n",
        "                                  device=args.device)\n",
        "y_score = np.array([])\n",
        "y_test = np.array([])\n",
        "\n",
        "for batch_dict in batch_generator:\n",
        "  y_score = np.concatenate((y_score, torch.sigmoid(classifier(batch_dict['x_data'])).cpu().detach().numpy()))\n",
        "  y_test = np.concatenate((y_test, batch_dict['y_target'].cpu().detach().numpy()))\n",
        "\n",
        "#y_test = batch_dict['y_target'].cpu().detach().numpy()\n",
        "print(y_score[:20])\n",
        "y_pred_test = np.zeros(len(y_score))\n",
        "y_pred_test[y_score > 0.5] = 1.0\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "auc = roc_auc_score(y_test, y_score)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.54511833e-05 3.30200195e-02 1.63940430e-01 1.16149902e-01\n",
            " 3.85742188e-01 6.84204102e-02 2.39729881e-04 1.65161133e-01\n",
            " 4.57382202e-03 8.52661133e-02 1.47857666e-02 1.15966797e-02\n",
            " 6.52465820e-02 1.08215332e-01 7.51876831e-03 3.15551758e-02\n",
            " 4.13208008e-02 1.30233765e-02 3.33862305e-02 1.54876709e-03]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.99      0.97     14330\n",
            "         1.0       0.82      0.56      0.67      1622\n",
            "\n",
            "    accuracy                           0.94     15952\n",
            "   macro avg       0.89      0.77      0.82     15952\n",
            "weighted avg       0.94      0.94      0.94     15952\n",
            "\n",
            "Test F1 score:  0.6669104204753198\n",
            "ROC-AUC:  0.9308815544807396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE4e89dN3atO",
        "colab_type": "text"
      },
      "source": [
        "## 12. Inference (qualitative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhJ3izE-8Mx",
        "colab_type": "text"
      },
      "source": [
        "Regardons quelques exemples de prédictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87DTPFy54X-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzf7zNxSrPI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_category(comment, classifier, vectorizer, max_length,i=-1):\n",
        "    \"\"\"Predict a News category for a new title\n",
        "    \n",
        "    Args:\n",
        "        title (str): a raw title string\n",
        "        classifier (NewsClassifier): an instance of the trained classifier\n",
        "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
        "        max_length (int): the max sequence length\n",
        "            Note: CNNs are sensitive to the input data tensor size. \n",
        "                  This ensures to keep it the same size as the training data\n",
        "    \"\"\"\n",
        "    comment = preprocess_text(comment)\n",
        "    vectorized_comment = \\\n",
        "        torch.tensor(vectorizer.vectorize(comment, vector_length=max_length))\n",
        "    result = classifier(vectorized_comment.unsqueeze(0), apply_sigmoid=True).squeeze()\n",
        "    \n",
        "    if i == -1:\n",
        "      out = result.item()\n",
        "    else:\n",
        "      out = result[i].item()\n",
        "    return {'probability': out}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuqzbZph4ixt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_samples_binary(max_s=5):\n",
        "    samples = {}\n",
        "    for cat in dataset.val_df.clean.unique():\n",
        "        samples[cat] = dataset.val_df.comment_text[dataset.val_df.clean==cat].sample(max_s).tolist()\n",
        "    return samples\n",
        "\n",
        "def get_samples_multi(max_s=5):\n",
        "    labels = []\n",
        "    samples = {}\n",
        "    samples[\"clean\"] = dataset.val_df.comment_text[dataset.val_df.clean==True].sample(max_s).tolist()\n",
        "    samples[\"identity_hate\"] = dataset.val_df.comment_text[dataset.val_df.identity_hate==1].sample(max_s).tolist()\n",
        "    samples[\"insult\"] = dataset.val_df.comment_text[dataset.val_df.insult==1].sample(max_s).tolist()\n",
        "    samples[\"obscene\"] = dataset.val_df.comment_text[dataset.val_df.obscene==1].sample(max_s).tolist()\n",
        "    samples[\"severe_toxic\"] = dataset.val_df.comment_text[dataset.val_df.severe_toxic==1].sample(max_s).tolist()\n",
        "    samples[\"threat\"] = dataset.val_df.comment_text[dataset.val_df.threat==1].sample(max_s).tolist()\n",
        "    samples[\"toxic\"] = dataset.val_df.comment_text[dataset.val_df.toxic==1].sample(max_s).tolist()\n",
        "    return samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAr7Tsp94yXQ",
        "colab_type": "code",
        "outputId": "5600839a-b880-4b91-bafa-7ad80593df73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "\n",
        "if args.binary:\n",
        "  val_samples = get_samples_binary(5)\n",
        "  classifier = classifier.to(\"cpu\")\n",
        "\n",
        "  for truth, sample_group in val_samples.items():\n",
        "      print(f\"True Category: {truth}\")\n",
        "      print(\"=\"*30)\n",
        "      for sample in sample_group:\n",
        "          prediction = predict_category(sample, classifier, \n",
        "                                        vectorizer, dataset._max_seq_length + 1)\n",
        "          print(\"Prediction:(p={:0.2f})\".format( prediction['probability']))\n",
        "          print(\"\\t + Sample: {}\".format(sample))\n",
        "      print(\"-\"*30 + \"\\n\")\n",
        "else:\n",
        "  val_samples = get_samples_multi(5)\n",
        "  classifier = classifier.to(\"cpu\")\n",
        "  i = 0\n",
        "  for truth, sample_group in val_samples.items():\n",
        "      print(f\"True Category: {truth}\")\n",
        "      print(\"=\"*30)\n",
        "      for sample in sample_group:\n",
        "          prediction = predict_category(sample, classifier, \n",
        "                                        vectorizer, dataset._max_seq_length + 1,i)\n",
        "          print(\"Prediction:(p={:0.2f})\".format( prediction['probability']))\n",
        "          print(\"\\t + Sample: {}\".format(sample))\n",
        "      i += 1\n",
        "      print(\"-\"*30 + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Category: False\n",
            "==============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-4d610da2fe71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           prediction = predict_category(sample, classifier, \n\u001b[0;32m---> 11\u001b[0;31m                                         vectorizer, dataset._max_seq_length + 1)\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction:(p={:0.2f})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t + Sample: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-9ca89bb94095>\u001b[0m in \u001b[0;36mpredict_category\u001b[0;34m(comment, classifier, vectorizer, max_length, i)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvectorized_comment\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_comment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_sigmoid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-41dfb9ebd3de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in, apply_sigmoid)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# mlp classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mintermediate_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/apex/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                      \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                      kwargs)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/apex/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                      \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                      kwargs)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlB_qUKHL9U",
        "colab_type": "text"
      },
      "source": [
        "Below is a biais detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Iu47ULFKrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comment_list = [\"white people\",\"black people\",\"I am heterosexual\",\"I am homosexual\",\"I am gay\",\"I am a man\",\"I am a woman\"]\n",
        "classifier = classifier.to(\"cpu\")\n",
        "\n",
        "if args.binary:\n",
        "      print(\"=\"*30)\n",
        "      for comment in comment_list:\n",
        "        prediction = predict_category(comment, classifier, \n",
        "                                      vectorizer, dataset._max_seq_length + 1)\n",
        "        print(\"Sample : {}\".format(comment))\n",
        "        print(\"Toxicity:(p={:0.2f})\".format( prediction['probability']))\n",
        "else:\n",
        "      print(\"=\"*30)\n",
        "      for comment in comment_list:\n",
        "        print(\"Sample : {}\".format(comment))\n",
        "        for i in range(7):\n",
        "          prediction = predict_category(comment, classifier, \n",
        "                                            vectorizer, dataset._max_seq_length + 1,i)\n",
        "          print(prediction['probability'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TP7D8on-6oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comment = input()\n",
        "classifier = classifier.to(\"cpu\")\n",
        "\n",
        "if args.binary:\n",
        "      print(\"=\"*30)\n",
        "      prediction = predict_category(comment, classifier, \n",
        "                                    vectorizer, dataset._max_seq_length + 1)\n",
        "      print(\"Prediction:(p={:0.2f})\".format( prediction['probability']))\n",
        "else:\n",
        "      print(\"=\"*30)\n",
        "      for i in range(7):\n",
        "        prediction = predict_category(comment, classifier, \n",
        "                                          vectorizer, dataset._max_seq_length + 1,i)\n",
        "        print(prediction['probability'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwz7x-lu5heB",
        "colab_type": "text"
      },
      "source": [
        "# 12. Confusion (quantitative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ujuk9g_EAc",
        "colab_type": "text"
      },
      "source": [
        "L'accuracy n'est pas une mesure suffisante dans notre cas, on veut regarder les cas commentaires toxique/non toxique séparemment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SteHWTAY44KU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_multi(y_pred, y_target):\n",
        "    out_dim = y_pred.size(-1)\n",
        "    M = torch.zeros((out_dim,2,2))\n",
        "    l = len(y_target)\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu()\n",
        "    for j in range(out_dim):\n",
        "      for i in range(l):\n",
        "        M[j,int(y_target[i][j]),int(y_pred_indices[i][j])] += 1\n",
        "    return M\n",
        "\n",
        "\n",
        "def confusion_binary(y_pred, y_target):\n",
        "    M = torch.zeros((1,2,2))\n",
        "    l = len(y_target)\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu()\n",
        "    for i in range(l):\n",
        "      M[0,int(y_target[i]),int(y_pred_indices[i])] += 1\n",
        "    return M\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YU33pPG7pvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.eval().cuda()\n",
        "dataset.set_split('val')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                  batch_size=args.batch_size, \n",
        "                                  device=args.device)\n",
        "\n",
        "M = torch.zeros((args.out_dim,2,2))\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "  y_pred =  classifier(batch_dict['x_data'])\n",
        "  if args.binary:\n",
        "    M_t = confusion_binary(y_pred, batch_dict['y_target'])\n",
        "  else:\n",
        "    M_t = confusion_multi(y_pred, batch_dict['y_target'])\n",
        "  M += M_t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEbtase3BmuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(args.out_dim):\n",
        "  print(M[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG_nVXiYV-W8",
        "colab_type": "text"
      },
      "source": [
        "# 13. Analyse Rationales "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_uPQVHpXAlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_rationale(comment, generator, vectorizer, max_length,i=-1):\n",
        "    \"\"\"Predict a News category for a new title\n",
        "    \n",
        "    Args:\n",
        "        title (str): a raw title string\n",
        "        classifier (NewsClassifier): an instance of the trained classifier\n",
        "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
        "        max_length (int): the max sequence length\n",
        "            Note: CNNs are sensitive to the input data tensor size. \n",
        "                  This ensures to keep it the same size as the training data\n",
        "    \"\"\"\n",
        "    comment = preprocess_text(comment)\n",
        "    vectorized_comment = \\\n",
        "        torch.tensor(vectorizer.vectorize(comment, vector_length=max_length))\n",
        "\n",
        "    init_hidden = generator.initHidden(1)\n",
        "    z = generator.sample(vectorized_comment.unsqueeze(0),init_hidden)\n",
        "    print(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs40eHjCV9d_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if args.binary:\n",
        "  val_samples = get_samples_binary(5)\n",
        "  classifier = classifier.to(\"cpu\")\n",
        "  generator = generator.to(\"cpu\")\n",
        "\n",
        "  for truth, sample_group in val_samples.items():\n",
        "      print(f\"True Category: {truth}\")\n",
        "      print(\"=\"*30)\n",
        "      for sample in sample_group:\n",
        "          #prediction = predict_category(sample, classifier, \n",
        "                                        #vectorizer, dataset._max_seq_length + 1)\n",
        "          prediction = generate_rationale(sample, generator, \n",
        "                                        vectorizer, dataset._max_seq_length + 1)\n",
        "          print(\"\\t + Sample: {}\".format(sample))\n",
        "      print(\"-\"*30 + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSRLJ6ygahqc",
        "colab_type": "text"
      },
      "source": [
        "# 14. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNYYodjLaj8n",
        "colab_type": "text"
      },
      "source": [
        "Binary output :\n",
        "\n",
        "id | Model used | Embeddings | epochs | dropout | batch size | learning rate | hidden_size | num_channels| kernel size | max length | level | accuracy | loss\n",
        "--- | --- | --- | ---| --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
        "1 | MLP1 | 50 | 20 | 0.15 | 128 | 0.001 |  50 | - | - | 800 | words | 87 % | 0.42 \n",
        "2 | MLP1 | G50 | 20 | 0.25 | 128 | 0.002 |  50 | - | - | 800 | words | 89 % | 0.3\n",
        "3 | CNN1 | 50 | 2 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 91% | 0.254 \n",
        "4 | CNN1 | G50 | 5 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 91% | 0.28\n",
        "5 | GRU1 | 50 | 4 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 92.4% | 0.204\n",
        "6 | GRU1 | G50 | 4 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 93% | 0.19\n",
        "7 | GRU2 | 50 | 5 | 0.25 | 128 | 0.002 |  50 | - | - | 800 | words | 90 % | 0.28\n",
        "8 | GRU2 | G50 | 5 | 0.25 | 128 | 0.002 |  50 | - | - | 800 | words | 92 % | 0.23\n",
        "9 | GRU2 | 25 | 8 | 0.25 | 128 | 0.0005 |  50 | - | - | 2000 | char | 90 % | 0.31\n",
        "\n",
        "\n",
        "le CNN overfit à balle\n",
        "\n",
        "On note également que les embeddings entrainé sur ce dataset sont TRES biaisé (black/white, gay, etc...) alors que Glove bcp moins !\n",
        "\n",
        "Multilabel :\n",
        "\n",
        "\n",
        "id | epochs | clean | unclean | hate | insult | obscene | severe toxic | threat  |  toxic\n",
        "--- | --- | --- | --- | --- | --- | --- | --- | --- | --- \n",
        "1 | 20 | 84 % |  86 % | 1.3 % | 60 % |  65 % | 12 % |0  % | 83%\n",
        "2 | 20 | 83 % | 87% | 0.6 % | 53 % | 55 % | 5% | 0% | 85 %\n",
        "3 | 2 | 89 % | 83 % | 0% | 67% | 78% | 8% | 0% | 81% \n",
        "3 | 4 | 86 % | 91 % | 0% | 67% | 77% | 10% | 0% | 90% \n",
        "4 | 5 | 89 % | 94 % | 0% | 73% | 81% | 7 % | 0% | 94 % \n",
        "5 | 5 | 88 % | 93 % | 0% | 58% | 71% | 9 % | 0% | 89 % \n",
        "5 | 6 | 91 % | 89 % | 0% | 72% | 73% | 19 % | 0% | 88 % \n",
        "6 | 5 | 90 % | 95 % | 0% | 75% | 87% | 50 % | 0% | 86 % \n",
        "7 | 5 | 90 % | 90 % | 1.7% | 67% | 77% | 20 % | 1% | 89 % \n",
        "8 | 4 | 92 % | 94 % | 10% | 77% | 84% | 28 % | 0% | 92 % \n"
      ]
    }
  ]
}