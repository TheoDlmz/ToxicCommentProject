{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_8qL5KJ1kc88",
        "Ctay4c5Rv0dl",
        "dTVCfeomv6Dv",
        "Dexgbb0evEqP",
        "SPgyv5a3v-fI",
        "jsxSmb53wC_A",
        "s0gK7u2XxDLM",
        "E9KPM-qi6OuD",
        "Mwngiqd8kocK",
        "hQJv4-62_Ixn",
        "mO1rzhQkoVUO",
        "wY7yN7--eao6",
        "zbwyYlS7JEAq",
        "H6PELxXfoKJv",
        "-hHIrRn7v_Jq",
        "A9-1sadLxALq",
        "-IPi6U8B9q0w"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a676acd7046a4e138eda9d98a230204b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b375e88eeb5e4812b38cecf24f36230c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8321bf0a5586488fb9f7126c2b4bae86",
              "IPY_MODEL_b310f3243fed4ef0b211300d653943f7"
            ]
          }
        },
        "b375e88eeb5e4812b38cecf24f36230c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8321bf0a5586488fb9f7126c2b4bae86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_00a45aac0432410ea397b6179b677b4b",
            "_dom_classes": [],
            "description": "training routine:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5858fc69b7684bca9fec3e3dbf186729"
          }
        },
        "b310f3243fed4ef0b211300d653943f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_184b344193e143218ef7c0214e0cb074",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/5 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a4695ec8a6c41bbb0fa0fc8d587ad8a"
          }
        },
        "00a45aac0432410ea397b6179b677b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5858fc69b7684bca9fec3e3dbf186729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "184b344193e143218ef7c0214e0cb074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a4695ec8a6c41bbb0fa0fc8d587ad8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f168f02133bc4ee5a3368739c6579b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_670891fa974c453b851bc8a4b7067479",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f879507f24c4c2d959e9ae4963e2d38",
              "IPY_MODEL_bb1db400dbb74a78b6cc9d7109bf4753"
            ]
          }
        },
        "670891fa974c453b851bc8a4b7067479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f879507f24c4c2d959e9ae4963e2d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_17500a66ee5548bebfe05a3d1c03b174",
            "_dom_classes": [],
            "description": "split=train:  59%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1745,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1029,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76f0541702e64deeafbab200f643e010"
          }
        },
        "bb1db400dbb74a78b6cc9d7109bf4753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d295d83662c64ddb80cca60bcae837f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1029/1745 [00:27&lt;00:19, 37.40it/s, acc=95.8, epoch=0, loss=0.138]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d0e9c08cd47421284942417cfa3b1e6"
          }
        },
        "17500a66ee5548bebfe05a3d1c03b174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76f0541702e64deeafbab200f643e010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d295d83662c64ddb80cca60bcae837f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d0e9c08cd47421284942417cfa3b1e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5759e55b569642bf8d70c25fa09490f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_70a4543637704a8693f9aefe17a19032",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68813df9d4864dd295cf544af63838f8",
              "IPY_MODEL_ac4e1a8d092c4ea493602c0764186100"
            ]
          }
        },
        "70a4543637704a8693f9aefe17a19032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68813df9d4864dd295cf544af63838f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c85f848957f948ac84ae21fbb31ef73e",
            "_dom_classes": [],
            "description": "split=val:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 498,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7cabd5cc96a41d2aca431380def2059"
          }
        },
        "ac4e1a8d092c4ea493602c0764186100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_933b7b5ceae0405f9316e6f5d037b23f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/498 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b04c7ed55516405fa0746298c8dd3b9b"
          }
        },
        "c85f848957f948ac84ae21fbb31ef73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7cabd5cc96a41d2aca431380def2059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "933b7b5ceae0405f9316e6f5d037b23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b04c7ed55516405fa0746298c8dd3b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBloDLsVovS7",
        "colab_type": "text"
      },
      "source": [
        "Fait :\n",
        "- Baselines: Naive Bayes, Logistic Regression, SVM, Boosting, Random Forest\n",
        "- Classifieur binaire MLP et Conv1D\n",
        "- Classifieur multilabel Conv1D\n",
        "- Classifier GRU + Conv1D\n",
        "- Classifier Fast Text\n",
        "- Classifier LSTM et LSTM + Attention\n",
        "- Classifier RCNN\n",
        "- Petit test de Transformers\n",
        "- Tenter une segmentation au niveau des caractÃ¨res, car beaucoup de commentaire contiennent des mots du style \"fu cky ou\" ou \"dickfuck\" ou je ne sais quel autre poÃ©sie (Ã§a marche moins bien)\n",
        "- J'ai mis une longueur maximum de 1000 mots par commentaires, pour accÃ©lerer le processus, car de toute faÃ§on quand y a + de 1000 mots en gÃ©nÃ©ral c'est juste un copiÃ© collÃ© Ã  la suite avec des insultes\n",
        "- Ajouter la possibilitÃ© d'utiliser les embeddings prÃ©entrainÃ©s de Glove Twitter (attention Ã§a prends du temps Ã  download)\n",
        "- DÃ©tecteur de biais sur les minoritÃ©s\n",
        "- LÃ©ger pre-processing des commentaires (qui ne semble pas apporter grand chose)\n",
        "- ModifiÃ© le balancing au niveau de la gÃ©nÃ©ration des batches \n",
        "- EssayÃ© les modÃ¨les bi-directionnels\n",
        "- EssayÃ© d'ajouter quelques mÃ©triques + significatives que l'accuracy (AUC, F1 Score)\n",
        "\n",
        "A faire :\n",
        "\n",
        "\n",
        "- Ajouter 'seq_length' dans les x_data\n",
        "- Se mettre Ã  utiliser des LSTM ou GRU Ã  un moment puis bi LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0iBFE3ykw5F",
        "colab_type": "text"
      },
      "source": [
        "# 1. Some imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sne6MmP98n44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import nltk \n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import rcParams\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiEmokyq3rG4",
        "colab_type": "code",
        "outputId": "0dfb01de-920b-47b4-f27d-bd864dd1ff8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "sys.setrecursionlimit(10000)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CPC90lB9OR9",
        "colab_type": "code",
        "outputId": "802dd2d6-574a-46b2-9336-c79b005349ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardx\n",
        "!pip install simpletransformers"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx) (46.1.3)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.6/dist-packages (0.28.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: transformers>=2.9.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.9.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.0.12)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (0.1.90)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.0->simpletransformers) (0.7)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.0->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1sEXR9b0lTo",
        "colab_type": "code",
        "outputId": "e1170fb4-8eab-4489-d51b-8f620fb28d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!git clone https://github.com/NVIDIA/apex\n",
        "#%cd apex\n",
        "#!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 6771 (delta 10), reused 5 (delta 2), pack-reused 6745\u001b[K\n",
            "Receiving objects: 100% (6771/6771), 13.75 MiB | 7.73 MiB/s, done.\n",
            "Resolving deltas: 100% (4518/4518), done.\n",
            "/content/apex/apex/apex/apex/apex/apex/apex\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-_yqktn7k\n",
            "Created temporary directory: /tmp/pip-req-tracker-8vck3yyg\n",
            "Created requirements tracker '/tmp/pip-req-tracker-8vck3yyg'\n",
            "Created temporary directory: /tmp/pip-install-8n6ytl59\n",
            "Processing /content/apex/apex/apex/apex/apex/apex/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-zim02kie\n",
            "  Added file:///content/apex/apex/apex/apex/apex/apex/apex to build tracker '/tmp/pip-req-tracker-8vck3yyg'\n",
            "    Running setup.py (path:/tmp/pip-req-build-zim02kie/setup.py) egg_info for package from file:///content/apex/apex/apex/apex/apex/apex/apex\n",
            "    Running command python setup.py egg_info\n",
            "    torch.__version__  =  1.5.0+cu101\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-zim02kie/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-zim02kie/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-zim02kie/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-zim02kie/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-zim02kie/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-zim02kie/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-zim02kie/setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-zim02kie has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex/apex/apex/apex/apex/apex/apex\n",
            "  Removed apex==0.1 from file:///content/apex/apex/apex/apex/apex/apex/apex from build tracker '/tmp/pip-req-tracker-8vck3yyg'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /tmp/pip-uninstall-5aqewxp8\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex-0.1-py3.6.egg-info\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex/\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "      Successfully uninstalled apex-0.1\n",
            "  Created temporary directory: /tmp/pip-record-dgul4dqn\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-zim02kie/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-zim02kie/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-dgul4dqn/install-record.txt --single-version-externally-managed --compile\n",
            "    torch.__version__  =  1.5.0+cu101\n",
            "    /tmp/pip-req-build-zim02kie/setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function â€˜at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()â€™:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)â€™:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function â€˜at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)â€™:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function â€˜std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)â€™:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of â€˜reserved_sizeâ€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of â€˜reserved_sizeâ€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function â€˜std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)â€™:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-dgul4dqn/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-zim02kie\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-8vck3yyg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdX050178bk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget http://theo.delemazure.fr/perso/toxic_comments.csv\n",
        "#!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "#!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmsWFYMdaGzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "from simpletransformers.classification import MultiLabelClassificationModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0IEeKudvQAd",
        "colab_type": "text"
      },
      "source": [
        "## 1.a. Playing with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMXqM37ajlZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "98e034f5-d7b4-46b8-c6c0-afaf0100219f"
      },
      "source": [
        "sns.set()\n",
        "data = pd.read_csv('toxic_comments.csv')\n",
        "for index, row in data.iterrows():\n",
        "  if row.clean and (row.toxic == 1 or row.identity_hate == 1 or row.insult == 1 or row.obscene == 1 or row.severe_toxic == 1 or row.threat == 1):\n",
        "    print(row)\n",
        "#print(data[data.clean == True and data.toxic == 1])"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-224-4b487177ea08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'toxic_comments.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoxic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity_hate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobscene\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msevere_toxic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File toxic_comments.csv does not exist: 'toxic_comments.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyZ0GZIejzRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wRoWYhp9m1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lcom = []\n",
        "for com in data[\"comment_text\"]:\n",
        "  lcom.append((len(com.split(\" \")), com))\n",
        "\n",
        "lcom = sorted(lcom)[::-1]\n",
        "\n",
        "for i in range(50):\n",
        "  print(lcom[i][0], lcom[i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIN8-K0LOEU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = data[data.split == \"train\"].copy()\n",
        "train['length'] = train['comment_text'].apply(len)\n",
        "toxic_data = train[train['toxic'] == 1]\n",
        "sevtoxic_data = train[train['severe_toxic'] == 1]\n",
        "nontoxic=train[train.iloc[:,2:7].sum(axis=1)==0]\n",
        "\n",
        "count=pd.Series(' '.join(toxic_data['comment_text']).split()).value_counts()\n",
        "count=count.sort_values(ascending=False)\n",
        "count_head=count.head(10)\n",
        "count_head.plot(kind = 'bar', figsize=(10, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj3DZ2X4Objn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "bins = 1000\n",
        "plt.hist(toxic_data['length'], alpha = 0.4, bins=bins, label='toxic',density=True)\n",
        "plt.hist(sevtoxic_data['length'], alpha = 0.4, bins=bins, label='severe_toxic',density=True)\n",
        "plt.hist(nontoxic['length'], alpha = 0.4, bins=bins, label='non-toxic',density=True)\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('number_ratio')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(0,200)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SrbmovvO_nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x=train.iloc[:,2:7].sum()\n",
        "nb_occ = train[[\"clean\", \"toxic\", \"insult\", \"obscene\", \"severe_toxic\", \"identity_hate\", \"threat\"]].sum()\n",
        "#plot\n",
        "plt.figure(figsize=(8,4))\n",
        "print(nb_occ)\n",
        "ax= sns.barplot(nb_occ.index, nb_occ.values, alpha=0.8)\n",
        "plt.title(\"WIKI comment # per class\")\n",
        "plt.ylabel('# of Occurrences', fontsize=12)\n",
        "plt.xlabel('Type ', fontsize=12)\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = nb_occ.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n",
        "print('ratio of wiki toxic comment', nb_occ[[\"toxic\", \"insult\", \"obscene\", \"severe_toxic\", \"identity_hate\", \"threat\"]].sum()/len(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edxq25t88wO_",
        "colab_type": "text"
      },
      "source": [
        "# 2. Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWh_mOkARQ_",
        "colab_type": "text"
      },
      "source": [
        "La classe \"Vocabulary\" crÃ©Ã© un dictionnaire qui associe Ã  chaque mot du vocabulaire un entier (son index). Elle contient des fonctions pour aller du mot vers son index et inversement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TinpzN8zqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token \n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "        \n",
        "    def to_serializable(self):\n",
        "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "            \n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\n",
        "        \n",
        "        Args:\n",
        "            tokens (list): a list of string tokens\n",
        "        Returns:\n",
        "            indices (list): a list of indices corresponding to the tokens\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "        \n",
        "        Args: \n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "            KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qFvUhwAcFF",
        "colab_type": "text"
      },
      "source": [
        "La classe \"SequenceVocabulary\" hÃ©rite de la classe vocabulary et permets de dÃ©finir des token pour les dÃ©buts de phrase, fin de phrase, mot inconnu et \"mask\" pour aprÃ¨s la fin de la phrase (si les phrase d'un minibatch sont de longueur diffÃ©rente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcoJ-BZU_8Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = self.add_token(self._unk_token)\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "          or the UNK index if token isn't present.\n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
        "              for the UNK functionality \n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaL5cI_k_j8V",
        "colab_type": "text"
      },
      "source": [
        "# 3. The Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9hA0fgcB4p_",
        "colab_type": "text"
      },
      "source": [
        "Le vectorizer permet de transformer un commentaire en un suite de token (leur index  + les tokens spÃ©ciaux)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-4LQypFvhB4",
        "colab_type": "text"
      },
      "source": [
        "## 3.a. Word level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb-AY7JC_Wwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CommentVectorizer(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
        "    def __init__(self, comment_vocab, bigrams=False):\n",
        "        self.comment_vocab = comment_vocab\n",
        "        self.bigrams = bigrams\n",
        "    def vectorize(self, comment, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            comment (str): the string of words separated by a space\n",
        "            vector_length (int): an argument for forcing the length of index vector\n",
        "        Returns:\n",
        "            the vetorized comment (numpy.array)\n",
        "        \"\"\"\n",
        "\n",
        "        # Added bigrams\n",
        "        indices = [self.comment_vocab.begin_seq_index]\n",
        "        tokens = comment.split(\" \")\n",
        "        if args.bigrams:\n",
        "            tokens = generate_bigrams(tokens)\n",
        "        indices.extend(self.comment_vocab.lookup_token(token) \n",
        "                       for token in tokens)\n",
        "        indices.append(self.comment_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices[:vector_length]\n",
        "        out_vector[len(indices):] = self.comment_vocab.mask_index\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, comments_df, cutoff=5, bigrams=False):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            comments_df (pandas.DataFrame): the target dataset\n",
        "            cutoff (int): frequency threshold for including in Vocabulary \n",
        "        Returns:\n",
        "            an instance of the NewsVectorizer\n",
        "        \"\"\"\n",
        "        \n",
        "        # Bigrams\n",
        "        word_counts = Counter()\n",
        "        for comment in comments_df.comment_text:\n",
        "            tokens = comment.split(\" \")\n",
        "            if args.bigrams:\n",
        "              tokens = generate_bigrams(tokens)\n",
        "            for token in tokens:\n",
        "                if token not in string.punctuation:\n",
        "                    word_counts[token] += 1\n",
        "        \n",
        "        comment_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                comment_vocab.add_token(word)\n",
        "        \n",
        "        return cls(comment_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        comment_vocab = \\\n",
        "            SequenceVocabulary.from_serializable(contents['comment_vocab'])\n",
        "\n",
        "        return cls(comment_vocab=comment_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'comment_vocab': self.comment_vocab.to_serializable()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANB6chGdvkxc",
        "colab_type": "text"
      },
      "source": [
        "## 3.b. char level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg0snxG-t00U",
        "colab_type": "text"
      },
      "source": [
        "Voici une deuxiÃ¨me classe de vectorizer, pour sÃ©parer plus finement, au niveau des caractÃ¨res"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX4VXqSEt6UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CommentVectorizerChar(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
        "    def __init__(self, comment_vocab, max_comment_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            surname_vocab (Vocabulary): maps characters to integers\n",
        "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
        "            max_surname_length (int): the length of the longest surname\n",
        "        \"\"\"\n",
        "        self.comment_vocab = comment_vocab\n",
        "        self._max_comment_length = max_comment_length\n",
        "\n",
        "    def vectorize(self, comment, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            comment (str): the comment\n",
        "        Returns:\n",
        "            one_hot_matrix (np.ndarray): a matrix of one-hot vectors\n",
        "        \"\"\"\n",
        "        indices = [self.comment_vocab.begin_seq_index]\n",
        "        indices.extend(self.comment_vocab.lookup_token(token) \n",
        "                       for token in comment)\n",
        "        indices.append(self.comment_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices[:vector_length]\n",
        "        out_vector[len(indices):] = self.comment_vocab.mask_index\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, comments_df,cutoff=5):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            comments_df (pandas.DataFrame): the surnames dataset\n",
        "        Returns:\n",
        "            an instance of the SurnameVectorizer\n",
        "        \"\"\"\n",
        "        max_comment_length = 0\n",
        "\n",
        "        char_count = Counter()\n",
        "        for comment in comments_df.comment_text:\n",
        "          max_comment_length = max(max_comment_length,len(comment))\n",
        "          for token in comment:\n",
        "            # Modified this\n",
        "            if token not in string.punctuation and token not in stopword:\n",
        "              char_count[token] += 1\n",
        "        \n",
        "        comment_vocab = SequenceVocabulary()\n",
        "        for char, char_c in char_count.items():\n",
        "            if char_c >= cutoff:\n",
        "                comment_vocab.add_token(char)\n",
        "\n",
        "        return cls(comment_vocab,  max_comment_length)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        comment_vocab = Vocabulary.from_serializable(contents['comment_vocab'])\n",
        "        return cls(comment_vocab=comment_vocab,  \n",
        "                   max_comment_length=contents['max_comment_length'])\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'comment_vocab': self.comment_vocab.to_serializable(),\n",
        "                'max_comment_length': self._max_comment_length}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f34L0xEuBd28",
        "colab_type": "text"
      },
      "source": [
        "# 4. The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z60cgo9hSoVd"
      },
      "source": [
        "Cette classe permet d'accÃ©der facilement aux commentaires du dataset\n",
        "\n",
        "J'ai rajoutÃ© un paramÃ¨tre \"balanced\". Ce paramÃ¨tre permets de choisir si on veut un dataset avec 50% Good et 50% Bad (True) ou 90% Good et 10% Bad (False). Les deux ont autant de commentaires labelisÃ© \"Good\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKjbIZRqSoVe",
        "colab": {}
      },
      "source": [
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, comments_df, vectorizer,\n",
        "                 balanced=False,binary=True,fine_grained=False,\n",
        "                 max_length=-1,length_out=False, preprocess=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            comments_df (pandas.DataFrame): the dataset\n",
        "            vectorizer (CommentsVectorizer): vectorizer instatiated from dataset\n",
        "        \"\"\"\n",
        "        self.comments_df = comments_df.copy()   # Je le modifie plus tard. Juste pour eviter un effet de bord.\n",
        "        if preprocess:\n",
        "          self.comments_df['comment_text'] = self.comments_df['comment_text'].apply(lambda txt: clean_text(txt, tokenize=False)) \n",
        "        self._vectorizer = vectorizer\n",
        "        self._binary = binary\n",
        "        self._length_out = length_out\n",
        "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
        "        if binary:\n",
        "          self._n_classes = 2\n",
        "        else:\n",
        "          self._n_classes = 7  # 6 or 7 ?\n",
        "        if fine_grained:\n",
        "          self.measure_len = lambda context: len(context)\n",
        "        else:\n",
        "          self.measure_len = lambda context: len(context.split(\" \"))\n",
        "\n",
        "        if max_length == -1:\n",
        "          self._max_seq_length = max(map(self.measure_len, comments_df.comment_text)) + 2\n",
        "        else:\n",
        "          self._max_seq_length = max_length +2\n",
        "        \n",
        "\n",
        "        self.train_df = self.comments_df[self.comments_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        if balanced:\n",
        "          self.samples_weights = np.zeros(self.train_size) \n",
        "          if binary:\n",
        "            self.samples_weights[self.train_df.clean == True] = 1. / 9. # 1. / self.train_df.loc[self.train_df.clean == True].shape[0] pour 50-50\n",
        "            self.samples_weights[self.train_df.clean == False] = 1. # 1. / self.train_df.loc[self.train_df.clean == False].shape[0] pour 50-50\n",
        "          else:\n",
        "            # This one is tricky. How to balance the multi-class case ?\n",
        "            self.samples_weights[self.train_df.clean == True] = 1. / 9. \n",
        "            self.samples_weights[self.train_df.clean == False] = 1. \n",
        "\n",
        "        \n",
        "        self.val_df = self.comments_df[self.comments_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.comments_df[self.comments_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.validation_size),\n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "        \"\"\"\n",
        "        class_counts = comments_df.clean.value_counts().to_dict()\n",
        "\n",
        "        def sort_key(item):\n",
        "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
        "        \n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, comments_csv,balanced=False,\n",
        "                                         binary=True,fine_grained=False,\n",
        "                                         max_length=-1,length_out=False, preprocess=False):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        comments_df = pd.read_csv(comments_csv)\n",
        "        train_comments_df = comments_df[comments_df.split=='train']\n",
        "        if fine_grained:\n",
        "          comment_vectorizer = CommentVectorizerChar.from_dataframe(train_comments_df)\n",
        "        else:\n",
        "          comment_vectorizer = CommentVectorizer.from_dataframe(train_comments_df)\n",
        "        return cls(comments_df,comment_vectorizer ,\n",
        "                   balanced,binary,fine_grained,\n",
        "                   max_length, length_out, preprocess)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \n",
        "        Used in the case in the vectorizer has been cached for re-use\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        comments_df = pd.read_csv(comments_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(comments_csv, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"a static method for loading the vectorizer from file\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameVectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return NameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"saves the vectorizer to disk using json\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        vect_length = min(self.measure_len(row.comment_text)+1,self._max_seq_length-1)\n",
        "\n",
        "        comments_vector = \\\n",
        "            self._vectorizer.vectorize(row.comment_text, self._max_seq_length)\n",
        "\n",
        "        if self._length_out:\n",
        "          comments_vector = np.concatenate([[vect_length], comments_vector])\n",
        "        \n",
        "        if row.clean:\n",
        "          category_index = 0.0\n",
        "        else:\n",
        "          category_index = 1.0\n",
        "\n",
        "        if self._binary:\n",
        "          target = category_index\n",
        "        else:\n",
        "          target = torch.tensor([category_index,\n",
        "                                 row.identity_hate,\n",
        "                                 row.insult,\n",
        "                                 row.obscene,\n",
        "                                 row.severe_toxic,\n",
        "                                 row.threat,\n",
        "                                 row.toxic])\n",
        "        return {'x_data': comments_vector,\n",
        "                'y_target': target}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cufyRV3uSoVi"
      },
      "source": [
        "## 4.a. batch generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cx4tkoOOSoVi"
      },
      "source": [
        "Ci-dessous une fonction pour gÃ©nerer automatiquement des batchs lors de la phase d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LeVnPI5_SoVj",
        "colab": {}
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\", balanced=False): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    if balanced:\n",
        "      sampler = WeightedRandomSampler(\n",
        "      weights=dataset.samples_weights,\n",
        "      num_samples=dataset.train_size,\n",
        "      replacement=True)\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=sampler,\n",
        "                            drop_last=drop_last)\n",
        "    else:\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TvCkvAKsSoVl",
        "colab": {}
      },
      "source": [
        "def generate_batches_sorted(dataset, batch_size, shuffle=True, \n",
        "                            drop_last=True, device=\"cpu\"):\n",
        "    \"\"\"A generator function which wraps the PyTorch DataLoader.  The NMT Version \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        lengths = data_dict['x_data'][:,0].numpy()\n",
        "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
        "        \n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwaJYEdbksUN",
        "colab_type": "text"
      },
      "source": [
        "# 5. Helping functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q_AXzYj-C3E",
        "colab_type": "text"
      },
      "source": [
        "Ces fonctions permettent de mettre Ã  jour l'Ã©tat d'entrainement qui sauvegarde les loss et accuracy Ã  chaque epoch pour le train et le val dataset (entre autre)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q37k5mV6kuZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"Handle the training state updates.\n",
        "\n",
        "    Components:\n",
        "     - Early Stopping: Prevent overfitting.\n",
        "     - Model Checkpoint: Model is saved if the model is better\n",
        "\n",
        "    :param args: main arguments\n",
        "    :param model: model to train\n",
        "    :param train_state: a dictionary representing the training state values\n",
        "    :returns:\n",
        "        a new train_state\n",
        "    \"\"\"\n",
        "\n",
        "    # Save one model at least\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "\n",
        "    # Save model if performance improved\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "\n",
        "        # If loss worsened\n",
        "        if loss_t >= train_state['early_stopping_best_val']:\n",
        "            # Update step\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # Loss decreased\n",
        "        else:\n",
        "            # Save the best model\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "\n",
        "            # Reset early stopping step\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # Stop early ?\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxacMTR0Uo5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "def clean_text(text, tokenize=True, lemmatize=False):\n",
        "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
        "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
        "    if tokenize:\n",
        "      tokens = re.split('\\W+', text_rc)    # tokenization\n",
        "      if lemmatize:\n",
        "        text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
        "      else:\n",
        "        text = [word for word in tokens if word not in stopword] # remove stopwords\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy39Z3SKlAm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSvo7qFp_Tbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_embedding_matrix(glove_filepath, words):\n",
        "    \"\"\"\n",
        "    Create embedding matrix for a specific set of words.\n",
        "    \n",
        "    Args:\n",
        "        glove_filepath (str): file path to the glove embeddigns\n",
        "        words (list): list of words in the dataset\n",
        "    \"\"\"\n",
        "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
        "    embedding_size = glove_embeddings.shape[1]\n",
        "    \n",
        "    final_embeddings = np.zeros((len(words), embedding_size))\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word in word_to_idx:\n",
        "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
        "        else:\n",
        "            embedding_i = torch.ones(1, embedding_size)\n",
        "            torch.nn.init.xavier_uniform_(embedding_i)\n",
        "            final_embeddings[i, :] = embedding_i\n",
        "\n",
        "    return final_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l6I-AlU_w3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove_from_file(glove_filepath):\n",
        "    \"\"\"\n",
        "    Load the GloVe embeddings \n",
        "    \n",
        "    Args:\n",
        "        glove_filepath (str): path to the glove embeddings file \n",
        "    Returns:\n",
        "        word_to_index (dict), embeddings (numpy.ndarary)\n",
        "    \"\"\"\n",
        "\n",
        "    word_to_index = {}\n",
        "    embeddings = []\n",
        "    with open(glove_filepath, \"r\") as fp:\n",
        "        for index, line in enumerate(fp):\n",
        "            line = line.split(\" \") # each line: word num1 num2 ...\n",
        "            word_to_index[line[0]] = index # word = line[0] \n",
        "            embedding_i = np.array([float(val) for val in line[1:]])\n",
        "            embeddings.append(embedding_i)\n",
        "    return word_to_index, np.stack(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hs-GrFg-NPz",
        "colab_type": "text"
      },
      "source": [
        "La fonction ci-dessous permets de calculer une accuracy, dans le cas ou il y a une seule output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCS9rGJ1DP0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(y_pred, y_target):\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w95r0Z5YvY4Y",
        "colab_type": "text"
      },
      "source": [
        "Fonction pour la generation des bi-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymqah48svfSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_bigrams(x):\n",
        "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "    for n_gram in n_grams:\n",
        "        x.append(' '.join(n_gram))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8qL5KJ1kc88",
        "colab_type": "text"
      },
      "source": [
        "# 6. Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctay4c5Rv0dl",
        "colab_type": "text"
      },
      "source": [
        "## 6.a. Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7REBTHJW-ULu",
        "colab_type": "text"
      },
      "source": [
        "Ci-dessous un premier modÃ¨le (MultiLayerPerceptron)\n",
        "\n",
        "+ 10 epochs, emb 50,hid 50, bs128,do0.15,lr-3 : 87%\n",
        "+ idem + glove : 89%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRc1mNW7kcRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None,length_out=False,\n",
        "                 padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(MLPClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "            \n",
        "        \n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self._length_out = length_out\n",
        "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, num_features)\n",
        "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,)\n",
        "        \"\"\"\n",
        "        if self._length_out:\n",
        "          x_in = x_in[:,1:]\n",
        "          \n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        features = x_embedded.sum(dim=1)\n",
        "        #features = F.dropout(features, p=self._dropout_p)\n",
        "        \n",
        "        # mlp classifier\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
        "        prediction = self.fc2(intermediate_vector).squeeze()\n",
        "\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVCfeomv6Dv",
        "colab_type": "text"
      },
      "source": [
        "## 6.b. Conv 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3tEPOdqxYO",
        "colab_type": "text"
      },
      "source": [
        "Le classifieur CNN suivant Ã  l'air d'overfittÃ© au fil des Ã©poques, cependant, il atteint 95% d'accuracy mais surtout 81% sur les commentaires \"mÃ©chants\"\n",
        "\n",
        "+ 20 epochs, emb 50,hid 50, 128chqnm bs128,do0.15,lr-3 : 90%\n",
        "+ idem + glove : 92%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOd26_gMMTV5",
        "colab": {}
      },
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 num_channels, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None,length_out=False,\n",
        "                 padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(CNNClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=embedding_size, \n",
        "                   out_channels=num_channels, kernel_size=3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self._length_out = length_out\n",
        "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        if self._length_out:\n",
        "          x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
        "        \n",
        "        features = self.convnet(x_embedded)\n",
        "\n",
        "        # average and remove the extra dimension\n",
        "        remaining_size = features.size(dim=2)\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
        "        features = F.dropout(features, p=self._dropout_p)\n",
        "        \n",
        "        # mlp classifier\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
        "        prediction = self.fc2(intermediate_vector).squeeze()\n",
        "\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dexgbb0evEqP",
        "colab_type": "text"
      },
      "source": [
        "## 6.c. Fast Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGe93CAyrSWM",
        "colab_type": "text"
      },
      "source": [
        "ModÃ¨le inspirÃ© du papier https://arxiv.org/abs/1607.01759 \n",
        "En gros, c'est juste un MLP avec une phase de pre-processing du texte oÃ¹ on ajoute Ã  la fin de chaque phrase les n-grams qui y apparaissent (bigrams ici)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjgjpnbEvKxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastText(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_size, hidden_dim, out_dim, padding_idx, length_out=False, pretrained_embeddings=None, dropout_p=0.0):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self._length_out = length_out\n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "        \n",
        "    def forward(self, x, apply_sigmoid=False):\n",
        "        \n",
        "        x_in = x.t()\n",
        "        if self._length_out:\n",
        "          x_in = x_in[1:, :]\n",
        "          \n",
        "        embedded = self.emb(x_in)                \n",
        "        \n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "                \n",
        "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "        prediction = F.relu(F.dropout(self.fc1(pooled), p=self._dropout_p))\n",
        "        prediction = self.fc2(prediction).squeeze()\n",
        "\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPgyv5a3v-fI",
        "colab_type": "text"
      },
      "source": [
        "## 6.d. GRU + Conv1D sur les variables d'Ã©tat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hfF8BYB9AL8",
        "colab_type": "text"
      },
      "source": [
        "90% Pour le classifieur suivant avec une courbe de loss bizarre mais 90% sur les mauvais commentaires !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yztvZBDWM2Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size,num_channels, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, length_out=False,\n",
        "                 padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(GRUClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=rnn_hidden_size*2, \n",
        "                   out_channels=num_channels, kernel_size=3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=kernel_size),\n",
        "            nn.ELU()\n",
        "        )\n",
        "        self._dropout_p = dropout_p\n",
        "        self._length_out = length_out\n",
        "        self.fc1 = nn.Linear(num_channels, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        if self._length_out:\n",
        "          x_in = x_in[:,1:]\n",
        "          \n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "        y_out = y_out.permute(0, 2, 1)\n",
        "        features = self.convnet(y_out)\n",
        "        # average and remove the extra dimension\n",
        "        remaining_size = features.size(dim=2)\n",
        "\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
        "        features = F.dropout(features, p=self._dropout_p)\n",
        "\n",
        "        prediction = self.fc1(F.dropout(features, p=self._dropout_p)).squeeze()\n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsxSmb53wC_A",
        "colab_type": "text"
      },
      "source": [
        "## 6.e. GRU + MLP sur les variable d'Ã©tat finales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQu1nh1iMjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUClassifier2(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(GRUClassifier2, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=False)\n",
        "        \n",
        "        self._dropout_p = dropout_p\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:, 0]\n",
        "\n",
        "        x_in = x_in[:, 1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        features = []\n",
        "        for i in range(batchsize):\n",
        "          features.append(y_out[i,x_length[i],:])\n",
        "\n",
        "        features = torch.stack(features)\n",
        "\n",
        "        prediction = self.fc1(F.dropout(features, p=self._dropout_p)).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s0gK7u2XxDLM"
      },
      "source": [
        "## 6.f. LSTM + MLP sur les variable d'Ã©tat finales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P-euScCnxDLO",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, padding_idx=0, \n",
        "                 kernel_size=3, dropout_rec=0.1, bidirectional=True, n_layers=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.LSTM(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          num_layers=n_layers, \n",
        "                          dropout=dropout_rec,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=bidirectional)\n",
        "        \n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "        if bidirectional:\n",
        "          self.fc1 = nn.Linear(2 * rnn_hidden_size, hidden_dim)\n",
        "        else:\n",
        "          self.fc1 = nn.Linear(rnn_hidden_size, hidden_dim)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:,0]\n",
        "\n",
        "        x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        features = []\n",
        "        for i in range(batchsize):\n",
        "          features.append(y_out[i,x_length[i],:])\n",
        "\n",
        "        features = torch.stack(features)\n",
        "\n",
        "        intermediate_vector = self.fc1(F.dropout(features, p=self._dropout_p))\n",
        "        prediction = self.fc2(F.dropout(intermediate_vector, p=self._dropout_p)).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9KPM-qi6OuD",
        "colab_type": "text"
      },
      "source": [
        "##6.g. LSTM with Attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkkA4bQL6Trg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMAttentionClassifier(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, num_embeddings, out_dim, rnn_hidden_size, pretrained_embeddings=None, padding_idx=0, dropout_rec=0.1, bidirectional=False, n_layers=1):\n",
        "    super(LSTMAttentionClassifier, self).__init__()\n",
        "    if pretrained_embeddings is None:\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "    else:\n",
        "      pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\t\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size=embedding_size, \n",
        "                        hidden_size=rnn_hidden_size,\n",
        "                        num_layers=n_layers, \n",
        "                        dropout=dropout_rec,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=bidirectional)\n",
        "  \n",
        "\n",
        "    if bidirectional:\n",
        "        self.fc = nn.Linear(2 * rnn_hidden_size, out_dim)\n",
        "    else:\n",
        "        self.fc = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\t\t\n",
        "  def attention_net(self, lstm_output, final_state):\n",
        "    \"\"\" \n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\t  \n",
        "\t\t\"\"\"\n",
        "\n",
        "\n",
        "    hidden = final_state.squeeze(0)\n",
        "    attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "    soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "    new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "    return new_hidden_state\n",
        "\n",
        "  def forward(self, input_sentences, apply_sigmoid=False):\n",
        "    \"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "    batchsize, seq_length = input_sentences.size()\n",
        "    x_length = input_sentences[:,0]\n",
        "\n",
        "    input_sentences = input_sentences[:,1:]\n",
        "    input_sentences = input_sentences.t()\n",
        "    x_embedded = self.emb(input_sentences)\n",
        "    x_embedded = x_embedded.permute(1, 0, 2)\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(x_embedded)  \n",
        "    attn_output = self.attention_net(output, final_hidden_state)\n",
        "    prediction = self.fc(attn_output).squeeze()\n",
        "\n",
        "    if apply_sigmoid:\n",
        "      prediction = F.sigmoid(prediction)\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwngiqd8kocK",
        "colab_type": "text"
      },
      "source": [
        "##6.h RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_73YV_pbktRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RCNNClassifier(nn.Module):\n",
        "  def __init__(self, embedding_size, num_embeddings, out_dim, rnn_hidden_size, pretrained_embeddings=None, padding_idx=0, dropout_rec=0.1, bidirectional=False, n_layers=1, dropout_p=0.5):\n",
        "    super(RCNNClassifier, self).__init__()\n",
        "    if pretrained_embeddings is None:\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                              num_embeddings=num_embeddings,\n",
        "                              padding_idx=padding_idx)        \n",
        "    else:\n",
        "      pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "      self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "\n",
        "    self._dropout_p = dropout_p\n",
        "    self.lstm = nn.LSTM(input_size=embedding_size, \n",
        "                        hidden_size=rnn_hidden_size,\n",
        "                        num_layers=n_layers, \n",
        "                        dropout=dropout_rec,\n",
        "                        batch_first=False, \n",
        "                        bidirectional=bidirectional)\n",
        "  \n",
        "    if bidirectional:\n",
        "      self.fc1 = nn.Linear(2 * rnn_hidden_size + embedding_size, rnn_hidden_size) \n",
        "    else:\n",
        "      self.fc1 = nn.Linear(rnn_hidden_size + embedding_size, rnn_hidden_size) \n",
        "    self.fc2 = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\t\t\n",
        "  def forward(self, input_sentence, apply_sigmoid=False):\n",
        "    \"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\t\n",
        "\t\t\n",
        "\t\tThe idea of the paper \"Recurrent Convolutional Neural Networks for Text Classification\" is that we pass the embedding vector\n",
        "\t\tof the text sequences through a bidirectional LSTM and then for each sequence, our final embedding vector is the concatenation of \n",
        "\t\tits own GloVe embedding and the left and right contextual embedding which in bidirectional LSTM is same as the corresponding hidden\n",
        "\t\tstate. This final embedding is passed through a linear layer which maps this long concatenated encoding vector back to the hidden_size\n",
        "\t\tvector. After this step, we use a max pooling layer across all sequences of texts. This converts any varying length text into a fixed\n",
        "\t\tdimension tensor of size (batch_size, hidden_size) and finally we map this to the output layer.\n",
        "\t\t\"\"\"\n",
        "    \n",
        "    x_embedded = self.emb(input_sentence) \n",
        "    x_embedded = x_embedded.permute(1, 0, 2) \n",
        "\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(x_embedded)\n",
        "    final_encoding = torch.cat((output, x_embedded), 2).permute(1, 0, 2)\n",
        "    y = self.fc1(final_encoding) \n",
        "    y = y.permute(0, 2, 1) \n",
        "    y = F.max_pool1d(y, y.size()[2]) \n",
        "    y = y.squeeze(2)\n",
        "    prediction = self.fc2(y).squeeze()\n",
        "    if apply_sigmoid:\n",
        "      prediction = F.sigmoid(prediction)\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQJv4-62_Ixn",
        "colab_type": "text"
      },
      "source": [
        "## 6.i. Encoder + Generator (Rationales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxHnE4fE_NpE",
        "colab_type": "text"
      },
      "source": [
        "In the paper Rationalizing Neural Predictions (https://arxiv.org/pdf/1606.04155.pdf), the authors use the encoder described in cell 6.d. of this notebook and a generator to generate rationales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJS-7zbQuMtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myEncoder(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, dropout_p, out_dim,\n",
        "                 pretrained_embeddings=None, padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(myEncoder, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=False)\n",
        "        \n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x_in, z=None,apply_sigmoid=False,train=True):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:,0]\n",
        "\n",
        "        x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "        \n",
        "        if (z is not None):\n",
        "          x_in = x_in * z\n",
        "        \n",
        "\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        features = []\n",
        "        for i in range(batchsize):\n",
        "          features.append(y_out[i,x_length[i],:])\n",
        "\n",
        "        features = torch.stack(features)\n",
        "        if train:\n",
        "          features = F.dropout(features, p=self._dropout_p)\n",
        "        prediction = self.fc1(features).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            prediction = F.sigmoid(prediction)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfGqgMEp_NL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myGenerator(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, \n",
        "                 rnn_hidden_size, hidden_dim, num_layers,\n",
        "                 size_s, pretrained_embeddings=None, \n",
        "                 padding_idx=0,kernel_size=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        \n",
        "        super(myGenerator, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "        self.rnn = nn.LSTM(input_size=embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.s_size = size_s\n",
        "        self.rnn_cond = nn.LSTM(2 * rnn_hidden_size + 1, size_s, 1)\n",
        "        self.fc1 = nn.Linear(2 * rnn_hidden_size+ size_s, 1)\n",
        "\n",
        "    def logProb(self, x_in, z, init_hidden, use_cuda=True):\n",
        "        lstm_i2h_h0, lstm_i2h_c0, lstm_h2s_h0, lstm_h2s_c0 = init_hidden\n",
        "\n",
        "\n",
        "        x_length = x_in[:,0] - 1\n",
        "        x_in = x_in[:,1:]\n",
        "        batch_size, seq_len = x_in.size()\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        x_embedded = torch.transpose(x_embedded, 1, 0) # (seqlength,batchsize,embsize)\n",
        "\n",
        "        hidden_features, _ = self.rnn(x_embedded, (lstm_i2h_h0, lstm_i2h_c0))\n",
        "\n",
        "\n",
        "        z_transformed = torch.transpose(z, 1, 0)\n",
        "        z_transformed_unsqueezed = torch.unsqueeze(z_transformed, 2)  \n",
        " \n",
        "        s_h2s, (_, _) = self.rnn_cond(torch.cat((hidden_features, z_transformed_unsqueezed), dim=2), (lstm_h2s_h0, lstm_h2s_c0))\n",
        "\n",
        "        if use_cuda:\n",
        "          log_p_z = Variable(torch.zeros((batch_size)).cuda())\n",
        "        else:\n",
        "          log_p_z = Variable(torch.zeros((batch_size)))\n",
        "\n",
        "        for i in range(seq_len):\n",
        "          if (i == 0):\n",
        "            cur_p_z = self.fc1(torch.cat((hidden_i2h[i], lstm_h2s_h0[0]), dim=1))\n",
        "          else:\n",
        "            cur_p_z = self.fc1(torch.cat((hidden_i2h[i], s_h2s[i - 1]), dim=1))\n",
        "\n",
        "          cur_p_z = F.sigmoid(torch.squeeze(cur_p_z, 1))\n",
        "          cur_p_z = z_transformed[i] * cur_p_z + (1 - z_transformed[i]) * (1 - cur_p_z)\n",
        "          cur_log_p_z = torch.log(cur_p_z)\n",
        "\n",
        "          log_p_z = log_p_z + cur_log_p_z\n",
        "\n",
        "        return log_p_z\n",
        "\n",
        "\n",
        "    def sample(self, x_in, init_hidden, use_cuda=True):\n",
        "        lstm_i2h_h0, lstm_i2h_c0, lstm_h2s_h0, lstm_h2s_c0 = init_hidden\n",
        "\n",
        "        x_length = x_in[:,0] - 1\n",
        "        x_in = x_in[:,1:]\n",
        "        batch_size, seq_len = x_in.size()\n",
        "        x = self.emb(x_in)\n",
        "        #x = torch.transpose(self.emb(x_in), 1, 0)\n",
        "\n",
        "        hidden_i2h , (_, _) = self.rnn(x, (lstm_i2h_h0, lstm_i2h_c0))\n",
        "\n",
        "        hidden_i2h = torch.transpose(hidden_i2h, 0,1)  \n",
        "        if use_cuda:\n",
        "          z = torch.zeros((seq_len, batch_size)).cuda()\n",
        "        else:\n",
        "          z = torch.zeros((seq_len, batch_size))\n",
        "\n",
        "        z = Variable(z)\n",
        "\n",
        "        s_h2s_h = lstm_h2s_h0\n",
        "        s_h2s_c = lstm_h2s_c0\n",
        "\n",
        "        for i in range(seq_len):\n",
        "          cur_p_z = self.fc1(torch.cat((hidden_i2h[i], s_h2s_h[0]), dim=1))\n",
        "\n",
        "          cur_p_z = F.sigmoid(torch.squeeze(cur_p_z, 1))\n",
        "          m = Bernoulli(cur_p_z)\n",
        "          z[i] = m.sample()\n",
        "\n",
        "\n",
        "          cat_hidden_z = torch.unsqueeze(torch.cat((hidden_i2h[i], torch.unsqueeze(z[i], 1)), dim=1), 0)\n",
        "          _, (s_h2s_h, s_h2s_c) = self.rnn_cond(cat_hidden_z, (s_h2s_h, s_h2s_c))\n",
        "\n",
        "\n",
        "        return torch.transpose(z, 1, 0)\n",
        "    \"\"\"\n",
        "     def forward(self, x_in):\n",
        "        The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_surname (torch.Tensor): an input data tensor. \n",
        "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "       \n",
        "        batchsize,seq_length = x_in.size()\n",
        "        x_length = x_in[:,0] - 1\n",
        "        x_in = x_in[:,1:]\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        y_out = y_out[:,1:] # On enlÃ¨ve le charactere de dÃ©but\n",
        "\n",
        "        features = pack_padded_sequence(y_out,x_length.detach().cpu().numpy(),batch_first=True)\n",
        "        #Utiliser le packpading\n",
        "\n",
        "        intermediate_vector = self.fc1(features.data).squeeze()\n",
        "        intermediate_vector = F.sigmoid(intermediate_vector)\n",
        "        features = PackedSequence(intermediate_vector, features.batch_sizes, \n",
        "                 features.sorted_indices, features.unsorted_indices)\n",
        "        prediction, _ = pad_packed_sequence(features, batch_first=True)\n",
        "\n",
        "\n",
        "        return prediction\n",
        "\"\"\"\n",
        "    def initHidden(self, batch_size, use_cuda=True):\n",
        "      if use_cuda:\n",
        "        lstm_i2h_h0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size).cuda())\n",
        "        lstm_i2h_c0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size).cuda())\n",
        "\n",
        "        lstm_h2s_h0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size).cuda())\n",
        "        lstm_h2s_c0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size).cuda())\n",
        "\n",
        "      else:\n",
        "        lstm_i2h_h0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size))\n",
        "        lstm_i2h_c0 = Variable(torch.zeros(self.num_layers * 2, batch_size, self.rnn_hidden_size))\n",
        "\n",
        "        lstm_h2s_h0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size))\n",
        "        lstm_h2s_c0 = Variable(torch.zeros(1 * 1, batch_size, self.s_size))\n",
        "\n",
        "      return lstm_i2h_h0, lstm_i2h_c0, lstm_h2s_h0, lstm_h2s_c0\n",
        "\n",
        "    def loss(self, z):\n",
        "\n",
        "      length_cost = torch.sum(z, dim=1)\n",
        "      l_padded_mask =  torch.cat([z[:,0].unsqueeze(1), z] , dim=1)\n",
        "      r_padded_mask =  torch.cat([z, z[:,-1].unsqueeze(1)] , dim=1)\n",
        "      continuity_cost = torch.sum(torch.abs(r_padded_mask - l_padded_mask), dim=1)\n",
        "      return length_cost, continuity_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIT36j5NqHSv",
        "colab_type": "text"
      },
      "source": [
        "# 7. PrÃ©paration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE9fsZC5-fTR",
        "colab_type": "text"
      },
      "source": [
        "les diffÃ©rents paramÃ¨tres du modÃ¨le, du dataset, et d'entrainements Ã  un seul endroit !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbjBLz6k98G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    frequency_cutoff=5,\n",
        "    model_state_file='model.pth',\n",
        "    comments_csv='toxic_comments.csv',\n",
        "    save_dir='model_storage',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    glove_filepath='glove.twitter.27B.50d.txt',\n",
        "    balanced=True,\n",
        "    binary=True, #Est-ce que l'output est binaire (good/bad) ou multilabel ?\n",
        "    fine_grained = False,\n",
        "    max_length=800,\n",
        "    length_out=True,\n",
        "    preprocess=True,\n",
        "    # Model hyper parameters\n",
        "    model=\"MLP1\",\n",
        "    bigrams=False, # A mettre a True seulement pour FastText\n",
        "    use_glove=False,\n",
        "    embedding_size=128, \n",
        "    hidden_dim=100,\n",
        "    num_channels=128,\n",
        "    rnn_hidden_size=100,\n",
        "    kernel_size=3,\n",
        "    out_dim=1, #Nombre de labels = 1 ou 7 (6 particulier + 1 general)\n",
        "    # For the generator\n",
        "    num_samples = 20,\n",
        "    gen_num_layers = 1,\n",
        "    gen_s_size = 30,\n",
        "    # Training hyper parameters\n",
        "    batch_size=64,\n",
        "    early_stopping_criteria=3,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=5,\n",
        "    seed=1337, \n",
        "    dropout_p=0.3, \n",
        "    dropout_rec=0.1,  # Droupout LSTM\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.binary:\n",
        "  assert(args.out_dim == 1)\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4K1pWvH-qUO",
        "colab_type": "text"
      },
      "source": [
        "Cette cellulle crÃ©Ã© tout ce qu'on a dÃ©fini au dessus !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvjCGCAclife",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Loading dataset and creating vectorizer\")\n",
        "# create dataset and vectorizer\n",
        "dataset = CommentDataset.load_dataset_and_make_vectorizer(args.comments_csv,\n",
        "                                                          args.balanced,\n",
        "                                                          args.binary,\n",
        "                                                          args.fine_grained,\n",
        "                                                          args.max_length,\n",
        "                                                          args.length_out, \n",
        "                                                          args.preprocess)\n",
        "dataset.save_vectorizer(args.vectorizer_file)    \n",
        "vectorizer = dataset.get_vectorizer()\n",
        "# Use GloVe or randomly initialized embeddings\n",
        "if args.use_glove:\n",
        "    words = vectorizer.comment_vocab._token_to_idx.keys()\n",
        "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
        "                                       words=words)\n",
        "    print(\"Using pre-trained embeddings\")\n",
        "else:\n",
        "    print(\"Not using pre-trained embeddings\")\n",
        "    embeddings = None\n",
        "\n",
        "if args.model == \"MLP1\":\n",
        "  classifier = MLPClassifier(embedding_size=args.embedding_size, \n",
        "                              num_embeddings=len(vectorizer.comment_vocab),\n",
        "                              hidden_dim=args.hidden_dim, \n",
        "                              dropout_p=args.dropout_p,\n",
        "                              out_dim =args.out_dim,\n",
        "                              pretrained_embeddings=embeddings,\n",
        "                              length_out=args.length_out,\n",
        "                              padding_idx=0)\n",
        "  \n",
        "elif args.model == \"FastText\":\n",
        "  classifier = FastText(embedding_size=args.embedding_size, \n",
        "                        num_embeddings=len(vectorizer.comment_vocab),\n",
        "                        hidden_dim=args.hidden_dim, \n",
        "                        dropout_p=args.dropout_p,\n",
        "                        out_dim =args.out_dim,\n",
        "                        length_out=args.length_out,\n",
        "                        padding_idx=0)\n",
        "\n",
        "elif args.model == \"CNN1\":\n",
        "\n",
        "  classifier = CNNClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            num_channels=args.num_channels, \n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            length_out=args.length_out,\n",
        "                            padding_idx=0)\n",
        "\n",
        "\n",
        "elif args.model == \"GRU1\":\n",
        "\n",
        "  classifier = GRUClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            num_channels=args.num_channels,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            length_out=args.length_out,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"GRU2\":\n",
        "  assert(args.length_out)\n",
        "  classifier = GRUClassifier2(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"LSTM\":\n",
        "\n",
        "  classifier = LSTMClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"LSTMAttention\":\n",
        "  classifier = LSTMAttentionClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            dropout_rec=args.dropout_rec,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "  \n",
        "elif args.model == \"RCNN\":\n",
        "  classifier = RCNNClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            dropout_rec=args.dropout_rec,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "\n",
        "elif args.model == \"RAT\":\n",
        "  assert(args.length_out)\n",
        "  classifier = myEncoder(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            dropout_p=args.dropout_p,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            out_dim =args.out_dim,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)\n",
        "  \n",
        "  generator = myGenerator(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.comment_vocab),\n",
        "                            rnn_hidden_size=args.rnn_hidden_size,\n",
        "                            hidden_dim=args.hidden_dim,\n",
        "                            num_layers = args.gen_num_layers,\n",
        "                            size_s = args.gen_s_size,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            kernel_size=args.kernel_size,\n",
        "                            padding_idx=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myQKG6pQvEKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model): \n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
        "\n",
        "count_parameters(classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCvk7zeWJdDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMgQOdc1cKIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.set_split('train')\n",
        "batch_generator = generate_batches_sorted(dataset, \n",
        "                                  batch_size=args.batch_size, \n",
        "                                  device=args.device)\n",
        "batch_index, batch_dict = next(enumerate(batch_generator))\n",
        "\n",
        "batch_dict['x_data'].size()\n",
        "\n",
        "print(batch_dict['x_data'][-5:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO1rzhQkoVUO",
        "colab_type": "text"
      },
      "source": [
        "# 8. Simple Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-t4Y9C2m-5D",
        "colab_type": "text"
      },
      "source": [
        "## 8.a. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMN0KZgzoYT3",
        "colab_type": "text"
      },
      "source": [
        "Un peu de pre-processing pour les methodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHjoGF9olz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = dataset.train_df['comment_text']\n",
        "X_val = dataset.val_df['comment_text']\n",
        "X_test = dataset.test_df['comment_text']\n",
        "\n",
        "if args.binary:\n",
        "  y_train = dataset.train_df['clean']\n",
        "  y_val = dataset.val_df['clean']\n",
        "  y_test = dataset.test_df['clean']\n",
        "else:\n",
        "  # TODO: Complete the multi-class case\n",
        "  y_train = dataset.train_df['clean', 'identity_hate', 'insult', 'obscene', 'severe_toxic', 'severe_toxic', 'toxic']\n",
        "\n",
        "print(\"Number of clean comments: \", len(y_train[y_train == True]))\n",
        "print(\"Number of toxic comments: \", len(y_train[y_train == False]))\n",
        "\n",
        "print(\"Before cleaning:\", X_train[0])\n",
        "print(\"After training:\", clean_text(X_train[0]))\n",
        "\n",
        "if args.binary:\n",
        "  tmp_y_train = y_train[:]\n",
        "  tmp_y_train[y_train == False] = 1.0\n",
        "  tmp_y_train[y_train == True] = 0.0\n",
        "  y_train = tmp_y_train[:]\n",
        "\n",
        "  tmp_y_val = y_val[:]\n",
        "  tmp_y_val[y_val == False] = 1.0\n",
        "  tmp_y_val[y_val == True] = 0.0\n",
        "  y_val = tmp_y_val[:]\n",
        "\n",
        "  tmp_y_test = y_test[:]\n",
        "  tmp_y_test[y_test == False] = 1.0\n",
        "  tmp_y_test[y_test == True] = 0.0\n",
        "  y_test = tmp_y_test[:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY7yN7--eao6",
        "colab_type": "text"
      },
      "source": [
        "## 8.b. Naive Bayes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cwGM1fbe860",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes en utilisant TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tnObz3aeebT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB()),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"Test ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbwyYlS7JEAq",
        "colab_type": "text"
      },
      "source": [
        "## 8.c. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnMdoSU2syuP",
        "colab_type": "text"
      },
      "source": [
        "Regression logistique avec TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdALaAnTJGyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression()),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6PELxXfoKJv",
        "colab_type": "text"
      },
      "source": [
        "## 8.d. Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHrTl_nHs2dk",
        "colab_type": "text"
      },
      "source": [
        "Gradient Boosting avec TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUtM9UnWoNpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', GradientBoostingClassifier(n_estimators=200)),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hHIrRn7v_Jq",
        "colab_type": "text"
      },
      "source": [
        "## 8.e. SVM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dy6g968wFtN",
        "colab_type": "text"
      },
      "source": [
        "SVM avec TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMfDWQfQwHa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LinearSVC()),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9-1sadLxALq",
        "colab_type": "text"
      },
      "source": [
        "## 8.e. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmWRDNpnxJZ1",
        "colab_type": "text"
      },
      "source": [
        "Random Forest avec TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9IX-04NxDw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer=clean_text)),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', RandomForestClassifier(n_estimators=100)),\n",
        "                     ])\n",
        "text_clf.fit(X_train, y_train)\n",
        "y_pred_val = text_clf.predict(X_val)\n",
        "y_pred_test = text_clf.predict(X_test)\n",
        "y_score_test = text_clf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_score_test)\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "print(\"Val accuracy: \", metrics.accuracy_score(y_val, y_pred_val))\n",
        "print(\"Test accuracy: \", metrics.accuracy_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IPi6U8B9q0w",
        "colab_type": "text"
      },
      "source": [
        "# 9. Transformers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSvTj1hMnRiq",
        "colab_type": "text"
      },
      "source": [
        "La bibliothÃ¨que Simple Transformers rend l'utilisation des transformers ridiculement simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2MiYJRI9slg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = dataset.train_df.copy()\n",
        "test_df = dataset.test_df.copy()\n",
        "\n",
        "if args.binary:\n",
        "  train_df['labels'] = y_train\n",
        "  test_df['labels'] = y_test\n",
        "\n",
        "else:\n",
        "  train_df['labels'] = list(zip(train_df.toxic.tolist(), train_df.severe_toxic.tolist(), train_df.obscene.tolist(), train_df.threat.tolist(), train_df.insult.tolist(), train_df.identity_hate.tolist()))\n",
        "\n",
        "train_df['text'] = train_df['comment_text'].apply(lambda x: x.replace('\\n', ' '))\n",
        "test_df['text'] = test_df['comment_text'].apply(lambda x: x.replace('\\n', ' '))\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "if args.binary:\n",
        "  model = ClassificationModel('roberta', 'roberta-base', args={'train_batch_size':64, 'gradient_accumulation_steps':16, 'learning_rate': 1e-4, 'num_train_epochs': 2, 'max_seq_length': 200})\n",
        "else:\n",
        "  model = MultiLabelClassificationModel('roberta', 'roberta-base', num_labels=2, args={'train_batch_size':64, 'gradient_accumulation_steps':16, 'learning_rate': 1e-4, 'num_train_epochs': 2, 'max_seq_length': 200})\n",
        "\n",
        "model.train_model(train_df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gye4RJtTIO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqC86jI1XxlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7WUUi6MqLJa",
        "colab_type": "text"
      },
      "source": [
        "# 10. Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiDI0qGJ-wc8",
        "colab_type": "text"
      },
      "source": [
        "On initialize tout Ã§a et on crÃ©Ã© des jolies barre de progression :D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FQSjlSemqoc",
        "colab_type": "code",
        "outputId": "aadd1a14-6126-4ca3-c9cc-fa4e220acb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "a676acd7046a4e138eda9d98a230204b",
            "b375e88eeb5e4812b38cecf24f36230c",
            "8321bf0a5586488fb9f7126c2b4bae86",
            "b310f3243fed4ef0b211300d653943f7",
            "00a45aac0432410ea397b6179b677b4b",
            "5858fc69b7684bca9fec3e3dbf186729",
            "184b344193e143218ef7c0214e0cb074",
            "6a4695ec8a6c41bbb0fa0fc8d587ad8a",
            "f168f02133bc4ee5a3368739c6579b2e",
            "670891fa974c453b851bc8a4b7067479",
            "8f879507f24c4c2d959e9ae4963e2d38",
            "bb1db400dbb74a78b6cc9d7109bf4753",
            "17500a66ee5548bebfe05a3d1c03b174",
            "76f0541702e64deeafbab200f643e010",
            "d295d83662c64ddb80cca60bcae837f4",
            "0d0e9c08cd47421284942417cfa3b1e6",
            "5759e55b569642bf8d70c25fa09490f9",
            "70a4543637704a8693f9aefe17a19032",
            "68813df9d4864dd295cf544af63838f8",
            "ac4e1a8d092c4ea493602c0764186100",
            "c85f848957f948ac84ae21fbb31ef73e",
            "a7cabd5cc96a41d2aca431380def2059",
            "933b7b5ceae0405f9316e6f5d037b23f",
            "b04c7ed55516405fa0746298c8dd3b9b"
          ]
        }
      },
      "source": [
        "classifier = classifier.to(args.device)\n",
        "    \n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=5)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a676acd7046a4e138eda9d98a230204b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='training routine', max=5.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f168f02133bc4ee5a3368739c6579b2e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=train', max=1745.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5759e55b569642bf8d70c25fa09490f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=val', max=498.0, style=ProgressStyle(description_wiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJbE7oU-1Gu",
        "colab_type": "text"
      },
      "source": [
        "Et c'est partit pour l'entrainement !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3TDWBJSq5S8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34d4b090-6975-40fe-cad1-6efe6c272ffb"
      },
      "source": [
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device, \n",
        "                                           balanced=args.balanced)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "            y_target_cpu = batch_dict['y_target'].cpu().numpy()\n",
        "            # --------------------------------------\n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = classifier(batch_dict['x_data'])\n",
        "            y_target = batch_dict['y_target']\n",
        "            if not(args.binary):\n",
        "              y_pred = y_pred.view(-1)\n",
        "              y_target = y_target.view(-1)\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, y_target.float())\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, y_target)\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "              # compute the output\n",
        "              y_pred = classifier(batch_dict['x_data'])\n",
        "              y_target = batch_dict['y_target']\n",
        "              if not(args.binary):\n",
        "                y_pred = y_pred.view(-1)\n",
        "                y_target = y_target.view(-1)\n",
        "\n",
        "              # step 3. compute the loss\n",
        "              loss = loss_func(y_pred, y_target.float())\n",
        "              loss_t = loss.item()\n",
        "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "              # compute the accuracy\n",
        "              acc_t = compute_accuracy(y_pred, y_target)\n",
        "              running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "              val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                              epoch=epoch_index)\n",
        "              val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier,\n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exiting loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_7cGsBW6tFz",
        "colab_type": "text"
      },
      "source": [
        "# 11. Evolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBMXySg1-4cq",
        "colab_type": "text"
      },
      "source": [
        "Une petite Ã©tude de l'Ã©volution de la loss et de l'accuracy au cours de epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KrvmFPt6uKf",
        "colab_type": "code",
        "outputId": "0b8ad899-1ec8-4b8a-ba68-f31757788d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        }
      },
      "source": [
        "train_loss = train_state['train_loss']\n",
        "train_acc = train_state['train_acc']\n",
        "val_loss = train_state['val_loss']\n",
        "val_acc = train_state['val_acc']\n",
        "\n",
        "plt.plot(train_loss,label=\"train\")\n",
        "plt.plot(val_loss,label=\"val\")\n",
        "plt.legend()\n",
        "plt.title(\"Evolution of the loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_acc,label=\"train\")\n",
        "plt.plot(val_acc,label=\"val\")\n",
        "plt.legend()\n",
        "plt.title(\"Evolution of the accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL9aM0HQo99",
        "colab_type": "text"
      },
      "source": [
        "On calcule l'AUC. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZ22QabQoQA",
        "colab_type": "code",
        "outputId": "ffefa871-4bfc-49e7-a734-cbf22b189d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "classifier.eval().cuda()\n",
        "n_chunks = 8\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                  batch_size=int(dataset.test_size / n_chunks), \n",
        "                                  device=args.device)\n",
        "y_score = np.array([])\n",
        "y_test = np.array([])\n",
        "\n",
        "for batch_dict in batch_generator:\n",
        "  y_score = np.concatenate((y_score, torch.sigmoid(classifier(batch_dict['x_data'])).cpu().detach().numpy()))\n",
        "  y_test = np.concatenate((y_test, batch_dict['y_target'].cpu().detach().numpy()))\n",
        "\n",
        "#y_test = batch_dict['y_target'].cpu().detach().numpy()\n",
        "print(y_score[:20])\n",
        "y_pred_test = np.zeros(len(y_score))\n",
        "y_pred_test[y_score > 0.5] = 1.0\n",
        "print(metrics.classification_report(y_test, y_pred_test))\n",
        "auc = roc_auc_score(y_test, y_score)\n",
        "print(\"Test F1 score: \", f1_score(y_test, y_pred_test))\n",
        "print(\"ROC-AUC: \", auc)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.35351788e-02 1.28325656e-01 8.09599806e-06 6.34794578e-11\n",
            " 2.23863081e-11 3.48404408e-01 4.81776055e-07 1.69001498e-06\n",
            " 1.68622353e-06 9.11895768e-04 7.77621839e-32 5.06078312e-03\n",
            " 5.51021476e-05 2.22553699e-05 5.58860407e-08 3.66115943e-03\n",
            " 2.93572526e-02 3.69382072e-08 4.57030302e-03 1.06502775e-04]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.95      0.96     14330\n",
            "         1.0       0.64      0.83      0.73      1622\n",
            "\n",
            "    accuracy                           0.94     15952\n",
            "   macro avg       0.81      0.89      0.85     15952\n",
            "weighted avg       0.95      0.94      0.94     15952\n",
            "\n",
            "Test F1 score:  0.7269798657718121\n",
            "ROC-AUC:  0.955839198115927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE4e89dN3atO",
        "colab_type": "text"
      },
      "source": [
        "## 12. Inference (qualitative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhJ3izE-8Mx",
        "colab_type": "text"
      },
      "source": [
        "Regardons quelques exemples de prÃ©dictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87DTPFy54X-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzf7zNxSrPI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_category(comment, classifier, vectorizer, max_length,i=-1):\n",
        "    \"\"\"Predict a News category for a new title\n",
        "    \n",
        "    Args:\n",
        "        title (str): a raw title string\n",
        "        classifier (NewsClassifier): an instance of the trained classifier\n",
        "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
        "        max_length (int): the max sequence length\n",
        "            Note: CNNs are sensitive to the input data tensor size. \n",
        "                  This ensures to keep it the same size as the training data\n",
        "    \"\"\"\n",
        "    comment = preprocess_text(comment)\n",
        "    vectorized_comment = \\\n",
        "        torch.tensor(vectorizer.vectorize(comment, vector_length=max_length))\n",
        "    result = classifier(vectorized_comment.unsqueeze(0), apply_sigmoid=True).squeeze()\n",
        "    \n",
        "    if i == -1:\n",
        "      out = result.item()\n",
        "    else:\n",
        "      out = result[i].item()\n",
        "    return {'probability': out}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuqzbZph4ixt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_samples_binary(max_s=5):\n",
        "    samples = {}\n",
        "    for cat in dataset.val_df.clean.unique():\n",
        "        samples[cat] = dataset.val_df.comment_text[dataset.val_df.clean==cat].sample(max_s).tolist()\n",
        "    return samples\n",
        "\n",
        "def get_samples_multi(max_s=5):\n",
        "    labels = []\n",
        "    samples = {}\n",
        "    samples[\"clean\"] = dataset.val_df.comment_text[dataset.val_df.clean==True].sample(max_s).tolist()\n",
        "    samples[\"identity_hate\"] = dataset.val_df.comment_text[dataset.val_df.identity_hate==1].sample(max_s).tolist()\n",
        "    samples[\"insult\"] = dataset.val_df.comment_text[dataset.val_df.insult==1].sample(max_s).tolist()\n",
        "    samples[\"obscene\"] = dataset.val_df.comment_text[dataset.val_df.obscene==1].sample(max_s).tolist()\n",
        "    samples[\"severe_toxic\"] = dataset.val_df.comment_text[dataset.val_df.severe_toxic==1].sample(max_s).tolist()\n",
        "    samples[\"threat\"] = dataset.val_df.comment_text[dataset.val_df.threat==1].sample(max_s).tolist()\n",
        "    samples[\"toxic\"] = dataset.val_df.comment_text[dataset.val_df.toxic==1].sample(max_s).tolist()\n",
        "    return samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAr7Tsp94yXQ",
        "colab_type": "code",
        "outputId": "b98d3b2a-f118-430e-dc9c-acc9826b4df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "if args.binary:\n",
        "  val_samples = get_samples_binary(5)\n",
        "  classifier = classifier.to(\"cpu\")\n",
        "\n",
        "  for truth, sample_group in val_samples.items():\n",
        "      print(f\"True Category: {truth}\")\n",
        "      print(\"=\"*30)\n",
        "      for sample in sample_group:\n",
        "          prediction = predict_category(sample, classifier, \n",
        "                                        vectorizer, dataset._max_seq_length + 1)\n",
        "          print(\"Prediction:(p={:0.2f})\".format( prediction['probability']))\n",
        "          print(\"\\t + Sample: {}\".format(sample))\n",
        "      print(\"-\"*30 + \"\\n\")\n",
        "else:\n",
        "  val_samples = get_samples_multi(5)\n",
        "  classifier = classifier.to(\"cpu\")\n",
        "  i = 0\n",
        "  for truth, sample_group in val_samples.items():\n",
        "      print(f\"True Category: {truth}\")\n",
        "      print(\"=\"*30)\n",
        "      for sample in sample_group:\n",
        "          prediction = predict_category(sample, classifier, \n",
        "                                        vectorizer, dataset._max_seq_length + 1,i)\n",
        "          print(\"Prediction:(p={:0.2f})\".format( prediction['probability']))\n",
        "          print(\"\\t + Sample: {}\".format(sample))\n",
        "      i += 1\n",
        "      print(\"-\"*30 + \"\\n\")"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Category: False\n",
            "==============================\n",
            "Prediction:(p=0.12)\n",
            "\t + Sample: red flag baloney baloney is what your mother pleasured herself with after your father left the best part of you on her thigh \n",
            "Prediction:(p=0.76)\n",
            "\t + Sample: quite frankly nothing is helping due to his arrogance and truculence he keeps getting called a dick as he is acting like one , over and over in my view that makes him one thank you for posting my comment over to the ib incidentally , what makes you think a block will cure anything ? \n",
            "Prediction:(p=0.62)\n",
            "\t + Sample: uh oh , mr big shot registered user is crying like a little girl because i called him out on his bs \n",
            "Prediction:(p=0.98)\n",
            "\t + Sample: i 'm not gonna say it again ! stop fuckin' wit me goddamn it ! ! ! ! ! ! \n",
            "Prediction:(p=0.99)\n",
            "\t + Sample: yeah , why beat around the bush why disguise your racist views ? everyone can see where you 're coming from just blank the page again with 'fuck israel , as brief and clear as that '\n",
            "------------------------------\n",
            "\n",
            "True Category: True\n",
            "==============================\n",
            "Prediction:(p=0.24)\n",
            "\t + Sample: i take you up on your offer of dyk pointer ! \n",
            "Prediction:(p=0.12)\n",
            "\t + Sample: nallebombning an international incident when the swedish were bombing latvia and some russians with teddy brears on the teddy bears were slogans about human rights the russians were pissed nallebombning ca n't find anythihg on any wiki about this \n",
            "Prediction:(p=0.00)\n",
            "\t + Sample: i 'm assuming that greenberg 's classification scheme has a significant historical impact \n",
            "Prediction:(p=0.08)\n",
            "\t + Sample: nothing recent though , some ip hopping i 'll keep an eye out and semiprotect block , if things escalate '''' talk contribs\n",
            "Prediction:(p=0.00)\n",
            "\t + Sample:  we have been over the prophethood bit long ago , please see the archives of talkpages here , at the ahmadiyyah article and at the prophethood ( ahmadiyya ) article there is a long standing consensus on that matter which cannot be changed by your views so i will not even start to discuss it with you unless you have something new to add which has not been covered by previous discussions as for your view that muslims do not consider him to be the founder of their faith just do a quick google search amina adil calls him founder , so do these guys v 1 , 2 , 3 just look through this google search , there are a lot of muslim writers so i am not sure where this started from now as i have given you sources as per wp burden it is your duty to provide reliable sources which say that he is not considered the founder by muslims , then we can compare them ty for the opinion regards\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlB_qUKHL9U",
        "colab_type": "text"
      },
      "source": [
        "Below is a biais detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Iu47ULFKrj",
        "colab_type": "code",
        "outputId": "e89550e4-0436-4053-cf11-5bb0b7726aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "comment_list = [\"white people\",\"black people\",\"I am heterosexual\",\"I am homosexual\",\"I am gay\",\"I am a man\",\"I am a woman\"]\n",
        "classifier = classifier.to(\"cpu\")\n",
        "\n",
        "if args.binary:\n",
        "      print(\"=\"*30)\n",
        "      for comment in comment_list:\n",
        "        prediction = predict_category(comment, classifier, \n",
        "                                      vectorizer, dataset._max_seq_length + 1)\n",
        "        print(\"Sample : {}\".format(comment))\n",
        "        print(\"Toxicity:(p={:0.2f})\".format( prediction['probability']))\n",
        "else:\n",
        "      print(\"=\"*30)\n",
        "      for comment in comment_list:\n",
        "        print(\"Sample : {}\".format(comment))\n",
        "        for i in range(7):\n",
        "          prediction = predict_category(comment, classifier, \n",
        "                                            vectorizer, dataset._max_seq_length + 1,i)\n",
        "          print(prediction['probability'])"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================\n",
            "Sample : white people\n",
            "Toxicity:(p=0.14)\n",
            "Sample : black people\n",
            "Toxicity:(p=0.63)\n",
            "Sample : I am heterosexual\n",
            "Toxicity:(p=0.69)\n",
            "Sample : I am homosexual\n",
            "Toxicity:(p=0.99)\n",
            "Sample : I am gay\n",
            "Toxicity:(p=0.98)\n",
            "Sample : I am a man\n",
            "Toxicity:(p=0.74)\n",
            "Sample : I am a woman\n",
            "Toxicity:(p=0.62)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TP7D8on-6oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comment = input()\n",
        "classifier = classifier.to(\"cpu\")\n",
        "\n",
        "if args.binary:\n",
        "      print(\"=\"*30)\n",
        "      prediction = predict_category(comment, classifier, \n",
        "                                    vectorizer, dataset._max_seq_length + 1)\n",
        "      print(\"Prediction:(p={:0.2f})\".format( prediction['probability']))\n",
        "else:\n",
        "      print(\"=\"*30)\n",
        "      for i in range(7):\n",
        "        prediction = predict_category(comment, classifier, \n",
        "                                          vectorizer, dataset._max_seq_length + 1,i)\n",
        "        print(prediction['probability'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwz7x-lu5heB",
        "colab_type": "text"
      },
      "source": [
        "# 12. Confusion (quantitative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ujuk9g_EAc",
        "colab_type": "text"
      },
      "source": [
        "L'accuracy n'est pas une mesure suffisante dans notre cas, on veut regarder les cas commentaires toxique/non toxique sÃ©paremment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SteHWTAY44KU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_multi(y_pred, y_target):\n",
        "    out_dim = y_pred.size(-1)\n",
        "    M = torch.zeros((out_dim,2,2))\n",
        "    l = len(y_target)\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu()\n",
        "    for j in range(out_dim):\n",
        "      for i in range(l):\n",
        "        M[j,int(y_target[i][j]),int(y_pred_indices[i][j])] += 1\n",
        "    return M\n",
        "\n",
        "\n",
        "def confusion_binary(y_pred, y_target):\n",
        "    M = torch.zeros((1,2,2))\n",
        "    l = len(y_target)\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu()\n",
        "    for i in range(l):\n",
        "      M[0,int(y_target[i]),int(y_pred_indices[i])] += 1\n",
        "    return M\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YU33pPG7pvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.eval().cuda()\n",
        "dataset.set_split('val')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                  batch_size=args.batch_size, \n",
        "                                  device=args.device)\n",
        "\n",
        "M = torch.zeros((args.out_dim,2,2))\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "  y_pred =  classifier(batch_dict['x_data'])\n",
        "  if args.binary:\n",
        "    M_t = confusion_binary(y_pred, batch_dict['y_target'])\n",
        "  else:\n",
        "    M_t = confusion_multi(y_pred, batch_dict['y_target'])\n",
        "  M += M_t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEbtase3BmuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(args.out_dim):\n",
        "  print(M[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG_nVXiYV-W8",
        "colab_type": "text"
      },
      "source": [
        "# 13. Analyse Rationales "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_uPQVHpXAlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_rationale(comment, generator, vectorizer, max_length,i=-1):\n",
        "    \"\"\"Predict a News category for a new title\n",
        "    \n",
        "    Args:\n",
        "        title (str): a raw title string\n",
        "        classifier (NewsClassifier): an instance of the trained classifier\n",
        "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
        "        max_length (int): the max sequence length\n",
        "            Note: CNNs are sensitive to the input data tensor size. \n",
        "                  This ensures to keep it the same size as the training data\n",
        "    \"\"\"\n",
        "    comment = preprocess_text(comment)\n",
        "    vectorized_comment = \\\n",
        "        torch.tensor(vectorizer.vectorize(comment, vector_length=max_length))\n",
        "\n",
        "    init_hidden = generator.initHidden(1)\n",
        "    z = generator.sample(vectorized_comment.unsqueeze(0),init_hidden)\n",
        "    print(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs40eHjCV9d_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if args.binary:\n",
        "  val_samples = get_samples_binary(5)\n",
        "  classifier = classifier.to(\"cpu\")\n",
        "  generator = generator.to(\"cpu\")\n",
        "\n",
        "  for truth, sample_group in val_samples.items():\n",
        "      print(f\"True Category: {truth}\")\n",
        "      print(\"=\"*30)\n",
        "      for sample in sample_group:\n",
        "          #prediction = predict_category(sample, classifier, \n",
        "                                        #vectorizer, dataset._max_seq_length + 1)\n",
        "          prediction = generate_rationale(sample, generator, \n",
        "                                        vectorizer, dataset._max_seq_length + 1)\n",
        "          print(\"\\t + Sample: {}\".format(sample))\n",
        "      print(\"-\"*30 + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSRLJ6ygahqc",
        "colab_type": "text"
      },
      "source": [
        "# 14. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvl62POrrIFz",
        "colab_type": "text"
      },
      "source": [
        "Binary output :\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNYYodjLaj8n",
        "colab_type": "text"
      },
      "source": [
        "Binary output :\n",
        "\n",
        "id | Model used | Embeddings | epochs | dropout | batch size | learning rate | hidden_size | num_channels| kernel size | max length | level | accuracy | loss\n",
        "--- | --- | --- | ---| --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
        "1 | MLP1 | 50 | 20 | 0.15 | 128 | 0.001 |  50 | - | - | 800 | words | 87 % | 0.42 \n",
        "2 | MLP1 | G50 | 20 | 0.25 | 128 | 0.002 |  50 | - | - | 800 | words | 89 % | 0.3\n",
        "3 | CNN1 | 50 | 2 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 91% | 0.254 \n",
        "4 | CNN1 | G50 | 5 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 91% | 0.28\n",
        "5 | GRU1 | 50 | 4 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 92.4% | 0.204\n",
        "6 | GRU1 | G50 | 4 | 0.25 | 128 | 0.002 |  50 | 64 | 3 | 800 | words | 93% | 0.19\n",
        "7 | GRU2 | 50 | 5 | 0.25 | 128 | 0.002 |  50 | - | - | 800 | words | 90 % | 0.28\n",
        "8 | GRU2 | G50 | 5 | 0.25 | 128 | 0.002 |  50 | - | - | 800 | words | 92 % | 0.23\n",
        "9 | GRU2 | 25 | 8 | 0.25 | 128 | 0.0005 |  50 | - | - | 2000 | char | 90 % | 0.31\n",
        "\n",
        "\n",
        "le CNN overfit Ã  balle\n",
        "\n",
        "On note Ã©galement que les embeddings entrainÃ© sur ce dataset sont TRES biaisÃ© (black/white, gay, etc...) alors que Glove bcp moins !\n",
        "\n",
        "Multilabel :\n",
        "\n",
        "\n",
        "id | epochs | clean | unclean | hate | insult | obscene | severe toxic | threat  |  toxic\n",
        "--- | --- | --- | --- | --- | --- | --- | --- | --- | --- \n",
        "1 | 20 | 84 % |  86 % | 1.3 % | 60 % |  65 % | 12 % |0  % | 83%\n",
        "2 | 20 | 83 % | 87% | 0.6 % | 53 % | 55 % | 5% | 0% | 85 %\n",
        "3 | 2 | 89 % | 83 % | 0% | 67% | 78% | 8% | 0% | 81% \n",
        "3 | 4 | 86 % | 91 % | 0% | 67% | 77% | 10% | 0% | 90% \n",
        "4 | 5 | 89 % | 94 % | 0% | 73% | 81% | 7 % | 0% | 94 % \n",
        "5 | 5 | 88 % | 93 % | 0% | 58% | 71% | 9 % | 0% | 89 % \n",
        "5 | 6 | 91 % | 89 % | 0% | 72% | 73% | 19 % | 0% | 88 % \n",
        "6 | 5 | 90 % | 95 % | 0% | 75% | 87% | 50 % | 0% | 86 % \n",
        "7 | 5 | 90 % | 90 % | 1.7% | 67% | 77% | 20 % | 1% | 89 % \n",
        "8 | 4 | 92 % | 94 % | 10% | 77% | 84% | 28 % | 0% | 92 % \n"
      ]
    }
  ]
}